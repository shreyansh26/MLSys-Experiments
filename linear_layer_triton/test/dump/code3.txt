[2024-01-05 21:01:31,349] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)



def forward(self, input_ids : torch.Tensor, labels : torch.Tensor):
    size = input_ids.size()
    getitem = size[-1]
    view = input_ids.view(-1, getitem);  input_ids = getitem = None
    shared = self.shared(view);  view = None
    getitem_1 = size[0]
    getitem_2 = size[1];  size = None
    getattr_1 = shared.device
    ones = torch.ones(getitem_1, getitem_2, device = getattr_1);  getitem_1 = getitem_2 = getattr_1 = None
    dim = ones.dim()
    eq = dim == 2;  dim = None
    dim_1 = ones.dim()
    eq_1 = dim_1 == 3;  dim_1 = None
    dim_2 = ones.dim()
    eq_2 = dim_2 == 2;  dim_2 = None
    getitem_3 = ones[(slice(None, None, None), None, None, slice(None, None, None))];  ones = None
    to = getitem_3.to(dtype = torch.float16);  getitem_3 = None
    sub = 1.0 - to;  to = None
    mul = sub * -65504.0;  sub = None
    encoder_dropout = self.encoder.dropout(shared);  shared = None
    to_1 = encoder_dropout.to(torch.float32)
    pow_1 = to_1.pow(2);  to_1 = None
    mean = pow_1.mean(-1, keepdim = True);  pow_1 = None
    add = mean + 1e-06;  mean = None
    rsqrt = torch.rsqrt(add);  add = None
    mul_1 = encoder_dropout * rsqrt;  rsqrt = None
    encoder_block_0_layer_0_layer_norm_weight = getattr(getattr(self.encoder.block, "0").layer, "0").layer_norm.weight
    getattr_2 = encoder_block_0_layer_0_layer_norm_weight.dtype
    eq_3 = getattr_2 == torch.float16;  getattr_2 = None
    getattr_3 = encoder_block_0_layer_0_layer_norm_weight.dtype
    to_2 = mul_1.to(getattr_3);  mul_1 = getattr_3 = None
    mul_2 = encoder_block_0_layer_0_layer_norm_weight * to_2;  encoder_block_0_layer_0_layer_norm_weight = to_2 = None
    size_1 = mul_2.size()
    getitem_4 = size_1[slice(None, 2, None)];  size_1 = None
    getitem_5 = getitem_4[0]
    getitem_6 = getitem_4[1];  getitem_4 = None
    encoder_block_0_layer_0_self_attention_q = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.q(mul_2)
    view_1 = encoder_block_0_layer_0_self_attention_q.view(getitem_5, -1, 12, 64);  encoder_block_0_layer_0_self_attention_q = None
    transpose = view_1.transpose(1, 2);  view_1 = None
    encoder_block_0_layer_0_self_attention_k = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.k(mul_2)
    view_2 = encoder_block_0_layer_0_self_attention_k.view(getitem_5, -1, 12, 64);  encoder_block_0_layer_0_self_attention_k = None
    transpose_1 = view_2.transpose(1, 2);  view_2 = None
    encoder_block_0_layer_0_self_attention_v = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.v(mul_2);  mul_2 = None
    view_3 = encoder_block_0_layer_0_self_attention_v.view(getitem_5, -1, 12, 64);  encoder_block_0_layer_0_self_attention_v = None
    transpose_2 = view_3.transpose(1, 2);  view_3 = None
    transpose_3 = transpose_1.transpose(3, 2);  transpose_1 = None
    matmul = torch.matmul(transpose, transpose_3);  transpose = transpose_3 = None
    getattr_4 = matmul.device
    arange = torch.arange(getitem_6, dtype = torch.int64, device = getattr_4)
    getitem_7 = arange[(slice(None, None, None), None)];  arange = None
    arange_1 = torch.arange(getitem_6, dtype = torch.int64, device = getattr_4);  getitem_6 = getattr_4 = None
    getitem_8 = arange_1[(None, slice(None, None, None))];  arange_1 = None
    sub_1 = getitem_8 - getitem_7;  getitem_8 = getitem_7 = None
    gt = sub_1 > 0
    to_3 = gt.to(torch.int64);  gt = None
    mul_3 = to_3 * 16;  to_3 = None
    add_1 = 0 + mul_3;  mul_3 = None
    abs_1 = torch.abs(sub_1);  sub_1 = None
    lt = abs_1 < 8
    float_1 = abs_1.float()
    truediv = float_1 / 8;  float_1 = None
    log = torch.log(truediv);  truediv = None
    truediv_1 = log / 2.772588722239781;  log = None
    mul_4 = truediv_1 * 8;  truediv_1 = None
    to_4 = mul_4.to(torch.int64);  mul_4 = None
    add_2 = 8 + to_4;  to_4 = None
    full_like = torch.full_like(add_2, 15)
    min_1 = torch.min(add_2, full_like);  add_2 = full_like = None
    where = torch.where(lt, abs_1, min_1);  lt = abs_1 = min_1 = None
    add_3 = add_1 + where;  add_1 = where = None
    encoder_block_0_layer_0_self_attention_relative_attention_bias = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.relative_attention_bias(add_3);  add_3 = None
    permute = encoder_block_0_layer_0_self_attention_relative_attention_bias.permute([2, 0, 1]);  encoder_block_0_layer_0_self_attention_relative_attention_bias = None
    unsqueeze = permute.unsqueeze(0);  permute = None
    add_4 = unsqueeze + mul;  unsqueeze = mul = None
    add_5 = matmul + add_4;  matmul = None
    float_2 = add_5.float()
    softmax = torch.nn.functional.softmax(float_2, dim = -1, _stacklevel = 3, dtype = None);  float_2 = None
    type_as = softmax.type_as(add_5);  softmax = add_5 = None
    dropout = torch.nn.functional.dropout(type_as, p = 0.1, training = False, inplace = False);  type_as = None
    matmul_1 = torch.matmul(dropout, transpose_2);  dropout = transpose_2 = None
    transpose_4 = matmul_1.transpose(1, 2);  matmul_1 = None
    contiguous = transpose_4.contiguous();  transpose_4 = None
    view_4 = contiguous.view(getitem_5, -1, 768);  contiguous = getitem_5 = None
    encoder_block_0_layer_0_self_attention_o = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o(view_4);  view_4 = None
    encoder_block_0_layer_0_dropout = getattr(getattr(self.encoder.block, "0").layer, "0").dropout(encoder_block_0_layer_0_self_attention_o);  encoder_block_0_layer_0_self_attention_o = None
    add_6 = encoder_dropout + encoder_block_0_layer_0_dropout;  encoder_dropout = encoder_block_0_layer_0_dropout = None
    getattr_5 = add_6.dtype
    eq_4 = getattr_5 == torch.float16;  getattr_5 = None
    to_5 = add_6.to(torch.float32)
    pow_2 = to_5.pow(2);  to_5 = None
    mean_1 = pow_2.mean(-1, keepdim = True);  pow_2 = None
    add_7 = mean_1 + 1e-06;  mean_1 = None
    rsqrt_1 = torch.rsqrt(add_7);  add_7 = None
    mul_5 = add_6 * rsqrt_1;  rsqrt_1 = None
    encoder_block_0_layer_1_layer_norm_weight = getattr(getattr(self.encoder.block, "0").layer, "1").layer_norm.weight
    getattr_6 = encoder_block_0_layer_1_layer_norm_weight.dtype
    eq_5 = getattr_6 == torch.float16;  getattr_6 = None
    getattr_7 = encoder_block_0_layer_1_layer_norm_weight.dtype
    to_6 = mul_5.to(getattr_7);  mul_5 = getattr_7 = None
    mul_6 = encoder_block_0_layer_1_layer_norm_weight * to_6;  encoder_block_0_layer_1_layer_norm_weight = to_6 = None
    encoder_block_0_layer_1_dense_relu_dense_wi_0 = getattr(getattr(self.encoder.block, "0").layer, "1").DenseReluDense.wi_0(mul_6)
    mul_7 = 0.5 * encoder_block_0_layer_1_dense_relu_dense_wi_0
    pow_3 = torch.pow(encoder_block_0_layer_1_dense_relu_dense_wi_0, 3.0)
    mul_8 = 0.044715 * pow_3;  pow_3 = None
    add_8 = encoder_block_0_layer_1_dense_relu_dense_wi_0 + mul_8;  encoder_block_0_layer_1_dense_relu_dense_wi_0 = mul_8 = None
    mul_9 = 0.7978845608028654 * add_8;  add_8 = None
    tanh = torch.tanh(mul_9);  mul_9 = None
    add_9 = 1.0 + tanh;  tanh = None
    mul_10 = mul_7 * add_9;  mul_7 = add_9 = None
    encoder_block_0_layer_1_dense_relu_dense_wi_1 = getattr(getattr(self.encoder.block, "0").layer, "1").DenseReluDense.wi_1(mul_6);  mul_6 = None
    mul_11 = mul_10 * encoder_block_0_layer_1_dense_relu_dense_wi_1;  mul_10 = encoder_block_0_layer_1_dense_relu_dense_wi_1 = None
    encoder_block_0_layer_1_dense_relu_dense_dropout = getattr(getattr(self.encoder.block, "0").layer, "1").DenseReluDense.dropout(mul_11);  mul_11 = None
    encoder_block_0_layer_1_dense_relu_dense_wo_weight = getattr(getattr(self.encoder.block, "0").layer, "1").DenseReluDense.wo.weight
    encoder_block_0_layer_1_dense_relu_dense_wo = getattr(getattr(self.encoder.block, "0").layer, "1").DenseReluDense.wo(encoder_block_0_layer_1_dense_relu_dense_dropout);  encoder_block_0_layer_1_dense_relu_dense_dropout = None
    encoder_block_0_layer_1_dropout = getattr(getattr(self.encoder.block, "0").layer, "1").dropout(encoder_block_0_layer_1_dense_relu_dense_wo);  encoder_block_0_layer_1_dense_relu_dense_wo = None
    add_10 = add_6 + encoder_block_0_layer_1_dropout;  add_6 = encoder_block_0_layer_1_dropout = None
    getattr_8 = add_10.dtype
    eq_6 = getattr_8 == torch.float16;  getattr_8 = None
    to_7 = add_10.to(torch.float32)
    pow_4 = to_7.pow(2);  to_7 = None
    mean_2 = pow_4.mean(-1, keepdim = True);  pow_4 = None
    add_11 = mean_2 + 1e-06;  mean_2 = None
    rsqrt_2 = torch.rsqrt(add_11);  add_11 = None
    mul_12 = add_10 * rsqrt_2;  rsqrt_2 = None
    encoder_block_1_layer_0_layer_norm_weight = getattr(getattr(self.encoder.block, "1").layer, "0").layer_norm.weight
    getattr_9 = encoder_block_1_layer_0_layer_norm_weight.dtype
    eq_7 = getattr_9 == torch.float16;  getattr_9 = None
    getattr_10 = encoder_block_1_layer_0_layer_norm_weight.dtype
    to_8 = mul_12.to(getattr_10);  mul_12 = getattr_10 = None
    mul_13 = encoder_block_1_layer_0_layer_norm_weight * to_8;  encoder_block_1_layer_0_layer_norm_weight = to_8 = None
    size_2 = mul_13.size()
    getitem_9 = size_2[slice(None, 2, None)];  size_2 = None
    getitem_10 = getitem_9[0]
    getitem_11 = getitem_9[1];  getitem_9 = None
    encoder_block_1_layer_0_self_attention_q = getattr(getattr(self.encoder.block, "1").layer, "0").SelfAttention.q(mul_13)
    view_5 = encoder_block_1_layer_0_self_attention_q.view(getitem_10, -1, 12, 64);  encoder_block_1_layer_0_self_attention_q = None
    transpose_5 = view_5.transpose(1, 2);  view_5 = None
    encoder_block_1_layer_0_self_attention_k = getattr(getattr(self.encoder.block, "1").layer, "0").SelfAttention.k(mul_13)
    view_6 = encoder_block_1_layer_0_self_attention_k.view(getitem_10, -1, 12, 64);  encoder_block_1_layer_0_self_attention_k = None
    transpose_6 = view_6.transpose(1, 2);  view_6 = None
    encoder_block_1_layer_0_self_attention_v = getattr(getattr(self.encoder.block, "1").layer, "0").SelfAttention.v(mul_13);  mul_13 = None
    view_7 = encoder_block_1_layer_0_self_attention_v.view(getitem_10, -1, 12, 64);  encoder_block_1_layer_0_self_attention_v = None
    transpose_7 = view_7.transpose(1, 2);  view_7 = None
    transpose_8 = transpose_6.transpose(3, 2);  transpose_6 = None
    matmul_2 = torch.matmul(transpose_5, transpose_8);  transpose_5 = transpose_8 = None
    add_12 = matmul_2 + add_4;  matmul_2 = None
    float_3 = add_12.float()
    softmax_1 = torch.nn.functional.softmax(float_3, dim = -1, _stacklevel = 3, dtype = None);  float_3 = None
    type_as_1 = softmax_1.type_as(add_12);  softmax_1 = add_12 = None
    dropout_1 = torch.nn.functional.dropout(type_as_1, p = 0.1, training = False, inplace = False);  type_as_1 = None
    matmul_3 = torch.matmul(dropout_1, transpose_7);  dropout_1 = transpose_7 = None
    transpose_9 = matmul_3.transpose(1, 2);  matmul_3 = None
    contiguous_1 = transpose_9.contiguous();  transpose_9 = None
    view_8 = contiguous_1.view(getitem_10, -1, 768);  contiguous_1 = getitem_10 = None
    encoder_block_1_layer_0_self_attention_o = getattr(getattr(self.encoder.block, "1").layer, "0").SelfAttention.o(view_8);  view_8 = None
    encoder_block_1_layer_0_dropout = getattr(getattr(self.encoder.block, "1").layer, "0").dropout(encoder_block_1_layer_0_self_attention_o);  encoder_block_1_layer_0_self_attention_o = None
    add_13 = add_10 + encoder_block_1_layer_0_dropout;  add_10 = encoder_block_1_layer_0_dropout = None
    getattr_11 = add_13.dtype
    eq_8 = getattr_11 == torch.float16;  getattr_11 = None
    to_9 = add_13.to(torch.float32)
    pow_5 = to_9.pow(2);  to_9 = None
    mean_3 = pow_5.mean(-1, keepdim = True);  pow_5 = None
    add_14 = mean_3 + 1e-06;  mean_3 = None
    rsqrt_3 = torch.rsqrt(add_14);  add_14 = None
    mul_14 = add_13 * rsqrt_3;  rsqrt_3 = None
    encoder_block_1_layer_1_layer_norm_weight = getattr(getattr(self.encoder.block, "1").layer, "1").layer_norm.weight
    getattr_12 = encoder_block_1_layer_1_layer_norm_weight.dtype
    eq_9 = getattr_12 == torch.float16;  getattr_12 = None
    getattr_13 = encoder_block_1_layer_1_layer_norm_weight.dtype
    to_10 = mul_14.to(getattr_13);  mul_14 = getattr_13 = None
    mul_15 = encoder_block_1_layer_1_layer_norm_weight * to_10;  encoder_block_1_layer_1_layer_norm_weight = to_10 = None
    encoder_block_1_layer_1_dense_relu_dense_wi_0 = getattr(getattr(self.encoder.block, "1").layer, "1").DenseReluDense.wi_0(mul_15)
    mul_16 = 0.5 * encoder_block_1_layer_1_dense_relu_dense_wi_0
    pow_6 = torch.pow(encoder_block_1_layer_1_dense_relu_dense_wi_0, 3.0)
    mul_17 = 0.044715 * pow_6;  pow_6 = None
    add_15 = encoder_block_1_layer_1_dense_relu_dense_wi_0 + mul_17;  encoder_block_1_layer_1_dense_relu_dense_wi_0 = mul_17 = None
    mul_18 = 0.7978845608028654 * add_15;  add_15 = None
    tanh_1 = torch.tanh(mul_18);  mul_18 = None
    add_16 = 1.0 + tanh_1;  tanh_1 = None
    mul_19 = mul_16 * add_16;  mul_16 = add_16 = None
    encoder_block_1_layer_1_dense_relu_dense_wi_1 = getattr(getattr(self.encoder.block, "1").layer, "1").DenseReluDense.wi_1(mul_15);  mul_15 = None
    mul_20 = mul_19 * encoder_block_1_layer_1_dense_relu_dense_wi_1;  mul_19 = encoder_block_1_layer_1_dense_relu_dense_wi_1 = None
    encoder_block_1_layer_1_dense_relu_dense_dropout = getattr(getattr(self.encoder.block, "1").layer, "1").DenseReluDense.dropout(mul_20);  mul_20 = None
    encoder_block_1_layer_1_dense_relu_dense_wo_weight = getattr(getattr(self.encoder.block, "1").layer, "1").DenseReluDense.wo.weight
    encoder_block_1_layer_1_dense_relu_dense_wo = getattr(getattr(self.encoder.block, "1").layer, "1").DenseReluDense.wo(encoder_block_1_layer_1_dense_relu_dense_dropout);  encoder_block_1_layer_1_dense_relu_dense_dropout = None
    encoder_block_1_layer_1_dropout = getattr(getattr(self.encoder.block, "1").layer, "1").dropout(encoder_block_1_layer_1_dense_relu_dense_wo);  encoder_block_1_layer_1_dense_relu_dense_wo = None
    add_17 = add_13 + encoder_block_1_layer_1_dropout;  add_13 = encoder_block_1_layer_1_dropout = None
    getattr_14 = add_17.dtype
    eq_10 = getattr_14 == torch.float16;  getattr_14 = None
    to_11 = add_17.to(torch.float32)
    pow_7 = to_11.pow(2);  to_11 = None
    mean_4 = pow_7.mean(-1, keepdim = True);  pow_7 = None
    add_18 = mean_4 + 1e-06;  mean_4 = None
    rsqrt_4 = torch.rsqrt(add_18);  add_18 = None
    mul_21 = add_17 * rsqrt_4;  rsqrt_4 = None
    encoder_block_2_layer_0_layer_norm_weight = getattr(getattr(self.encoder.block, "2").layer, "0").layer_norm.weight
    getattr_15 = encoder_block_2_layer_0_layer_norm_weight.dtype
    eq_11 = getattr_15 == torch.float16;  getattr_15 = None
    getattr_16 = encoder_block_2_layer_0_layer_norm_weight.dtype
    to_12 = mul_21.to(getattr_16);  mul_21 = getattr_16 = None
    mul_22 = encoder_block_2_layer_0_layer_norm_weight * to_12;  encoder_block_2_layer_0_layer_norm_weight = to_12 = None
    size_3 = mul_22.size()
    getitem_12 = size_3[slice(None, 2, None)];  size_3 = None
    getitem_13 = getitem_12[0]
    getitem_14 = getitem_12[1];  getitem_12 = None
    encoder_block_2_layer_0_self_attention_q = getattr(getattr(self.encoder.block, "2").layer, "0").SelfAttention.q(mul_22)
    view_9 = encoder_block_2_layer_0_self_attention_q.view(getitem_13, -1, 12, 64);  encoder_block_2_layer_0_self_attention_q = None
    transpose_10 = view_9.transpose(1, 2);  view_9 = None
    encoder_block_2_layer_0_self_attention_k = getattr(getattr(self.encoder.block, "2").layer, "0").SelfAttention.k(mul_22)
    view_10 = encoder_block_2_layer_0_self_attention_k.view(getitem_13, -1, 12, 64);  encoder_block_2_layer_0_self_attention_k = None
    transpose_11 = view_10.transpose(1, 2);  view_10 = None
    encoder_block_2_layer_0_self_attention_v = getattr(getattr(self.encoder.block, "2").layer, "0").SelfAttention.v(mul_22);  mul_22 = None
    view_11 = encoder_block_2_layer_0_self_attention_v.view(getitem_13, -1, 12, 64);  encoder_block_2_layer_0_self_attention_v = None
    transpose_12 = view_11.transpose(1, 2);  view_11 = None
    transpose_13 = transpose_11.transpose(3, 2);  transpose_11 = None
    matmul_4 = torch.matmul(transpose_10, transpose_13);  transpose_10 = transpose_13 = None
    add_19 = matmul_4 + add_4;  matmul_4 = None
    float_4 = add_19.float()
    softmax_2 = torch.nn.functional.softmax(float_4, dim = -1, _stacklevel = 3, dtype = None);  float_4 = None
    type_as_2 = softmax_2.type_as(add_19);  softmax_2 = add_19 = None
    dropout_2 = torch.nn.functional.dropout(type_as_2, p = 0.1, training = False, inplace = False);  type_as_2 = None
    matmul_5 = torch.matmul(dropout_2, transpose_12);  dropout_2 = transpose_12 = None
    transpose_14 = matmul_5.transpose(1, 2);  matmul_5 = None
    contiguous_2 = transpose_14.contiguous();  transpose_14 = None
    view_12 = contiguous_2.view(getitem_13, -1, 768);  contiguous_2 = getitem_13 = None
    encoder_block_2_layer_0_self_attention_o = getattr(getattr(self.encoder.block, "2").layer, "0").SelfAttention.o(view_12);  view_12 = None
    encoder_block_2_layer_0_dropout = getattr(getattr(self.encoder.block, "2").layer, "0").dropout(encoder_block_2_layer_0_self_attention_o);  encoder_block_2_layer_0_self_attention_o = None
    add_20 = add_17 + encoder_block_2_layer_0_dropout;  add_17 = encoder_block_2_layer_0_dropout = None
    getattr_17 = add_20.dtype
    eq_12 = getattr_17 == torch.float16;  getattr_17 = None
    to_13 = add_20.to(torch.float32)
    pow_8 = to_13.pow(2);  to_13 = None
    mean_5 = pow_8.mean(-1, keepdim = True);  pow_8 = None
    add_21 = mean_5 + 1e-06;  mean_5 = None
    rsqrt_5 = torch.rsqrt(add_21);  add_21 = None
    mul_23 = add_20 * rsqrt_5;  rsqrt_5 = None
    encoder_block_2_layer_1_layer_norm_weight = getattr(getattr(self.encoder.block, "2").layer, "1").layer_norm.weight
    getattr_18 = encoder_block_2_layer_1_layer_norm_weight.dtype
    eq_13 = getattr_18 == torch.float16;  getattr_18 = None
    getattr_19 = encoder_block_2_layer_1_layer_norm_weight.dtype
    to_14 = mul_23.to(getattr_19);  mul_23 = getattr_19 = None
    mul_24 = encoder_block_2_layer_1_layer_norm_weight * to_14;  encoder_block_2_layer_1_layer_norm_weight = to_14 = None
    encoder_block_2_layer_1_dense_relu_dense_wi_0 = getattr(getattr(self.encoder.block, "2").layer, "1").DenseReluDense.wi_0(mul_24)
    mul_25 = 0.5 * encoder_block_2_layer_1_dense_relu_dense_wi_0
    pow_9 = torch.pow(encoder_block_2_layer_1_dense_relu_dense_wi_0, 3.0)
    mul_26 = 0.044715 * pow_9;  pow_9 = None
    add_22 = encoder_block_2_layer_1_dense_relu_dense_wi_0 + mul_26;  encoder_block_2_layer_1_dense_relu_dense_wi_0 = mul_26 = None
    mul_27 = 0.7978845608028654 * add_22;  add_22 = None
    tanh_2 = torch.tanh(mul_27);  mul_27 = None
    add_23 = 1.0 + tanh_2;  tanh_2 = None
    mul_28 = mul_25 * add_23;  mul_25 = add_23 = None
    encoder_block_2_layer_1_dense_relu_dense_wi_1 = getattr(getattr(self.encoder.block, "2").layer, "1").DenseReluDense.wi_1(mul_24);  mul_24 = None
    mul_29 = mul_28 * encoder_block_2_layer_1_dense_relu_dense_wi_1;  mul_28 = encoder_block_2_layer_1_dense_relu_dense_wi_1 = None
    encoder_block_2_layer_1_dense_relu_dense_dropout = getattr(getattr(self.encoder.block, "2").layer, "1").DenseReluDense.dropout(mul_29);  mul_29 = None
    encoder_block_2_layer_1_dense_relu_dense_wo_weight = getattr(getattr(self.encoder.block, "2").layer, "1").DenseReluDense.wo.weight
    encoder_block_2_layer_1_dense_relu_dense_wo = getattr(getattr(self.encoder.block, "2").layer, "1").DenseReluDense.wo(encoder_block_2_layer_1_dense_relu_dense_dropout);  encoder_block_2_layer_1_dense_relu_dense_dropout = None
    encoder_block_2_layer_1_dropout = getattr(getattr(self.encoder.block, "2").layer, "1").dropout(encoder_block_2_layer_1_dense_relu_dense_wo);  encoder_block_2_layer_1_dense_relu_dense_wo = None
    add_24 = add_20 + encoder_block_2_layer_1_dropout;  add_20 = encoder_block_2_layer_1_dropout = None
    getattr_20 = add_24.dtype
    eq_14 = getattr_20 == torch.float16;  getattr_20 = None
    to_15 = add_24.to(torch.float32)
    pow_10 = to_15.pow(2);  to_15 = None
    mean_6 = pow_10.mean(-1, keepdim = True);  pow_10 = None
    add_25 = mean_6 + 1e-06;  mean_6 = None
    rsqrt_6 = torch.rsqrt(add_25);  add_25 = None
    mul_30 = add_24 * rsqrt_6;  rsqrt_6 = None
    encoder_block_3_layer_0_layer_norm_weight = getattr(getattr(self.encoder.block, "3").layer, "0").layer_norm.weight
    getattr_21 = encoder_block_3_layer_0_layer_norm_weight.dtype
    eq_15 = getattr_21 == torch.float16;  getattr_21 = None
    getattr_22 = encoder_block_3_layer_0_layer_norm_weight.dtype
    to_16 = mul_30.to(getattr_22);  mul_30 = getattr_22 = None
    mul_31 = encoder_block_3_layer_0_layer_norm_weight * to_16;  encoder_block_3_layer_0_layer_norm_weight = to_16 = None
    size_4 = mul_31.size()
    getitem_15 = size_4[slice(None, 2, None)];  size_4 = None
    getitem_16 = getitem_15[0]
    getitem_17 = getitem_15[1];  getitem_15 = None
    encoder_block_3_layer_0_self_attention_q = getattr(getattr(self.encoder.block, "3").layer, "0").SelfAttention.q(mul_31)
    view_13 = encoder_block_3_layer_0_self_attention_q.view(getitem_16, -1, 12, 64);  encoder_block_3_layer_0_self_attention_q = None
    transpose_15 = view_13.transpose(1, 2);  view_13 = None
    encoder_block_3_layer_0_self_attention_k = getattr(getattr(self.encoder.block, "3").layer, "0").SelfAttention.k(mul_31)
    view_14 = encoder_block_3_layer_0_self_attention_k.view(getitem_16, -1, 12, 64);  encoder_block_3_layer_0_self_attention_k = None
    transpose_16 = view_14.transpose(1, 2);  view_14 = None
    encoder_block_3_layer_0_self_attention_v = getattr(getattr(self.encoder.block, "3").layer, "0").SelfAttention.v(mul_31);  mul_31 = None
    view_15 = encoder_block_3_layer_0_self_attention_v.view(getitem_16, -1, 12, 64);  encoder_block_3_layer_0_self_attention_v = None
    transpose_17 = view_15.transpose(1, 2);  view_15 = None
    transpose_18 = transpose_16.transpose(3, 2);  transpose_16 = None
    matmul_6 = torch.matmul(transpose_15, transpose_18);  transpose_15 = transpose_18 = None
    add_26 = matmul_6 + add_4;  matmul_6 = None
    float_5 = add_26.float()
    softmax_3 = torch.nn.functional.softmax(float_5, dim = -1, _stacklevel = 3, dtype = None);  float_5 = None
    type_as_3 = softmax_3.type_as(add_26);  softmax_3 = add_26 = None
    dropout_3 = torch.nn.functional.dropout(type_as_3, p = 0.1, training = False, inplace = False);  type_as_3 = None
    matmul_7 = torch.matmul(dropout_3, transpose_17);  dropout_3 = transpose_17 = None
    transpose_19 = matmul_7.transpose(1, 2);  matmul_7 = None
    contiguous_3 = transpose_19.contiguous();  transpose_19 = None
    view_16 = contiguous_3.view(getitem_16, -1, 768);  contiguous_3 = getitem_16 = None
    encoder_block_3_layer_0_self_attention_o = getattr(getattr(self.encoder.block, "3").layer, "0").SelfAttention.o(view_16);  view_16 = None
    encoder_block_3_layer_0_dropout = getattr(getattr(self.encoder.block, "3").layer, "0").dropout(encoder_block_3_layer_0_self_attention_o);  encoder_block_3_layer_0_self_attention_o = None
    add_27 = add_24 + encoder_block_3_layer_0_dropout;  add_24 = encoder_block_3_layer_0_dropout = None
    getattr_23 = add_27.dtype
    eq_16 = getattr_23 == torch.float16;  getattr_23 = None
    to_17 = add_27.to(torch.float32)
    pow_11 = to_17.pow(2);  to_17 = None
    mean_7 = pow_11.mean(-1, keepdim = True);  pow_11 = None
    add_28 = mean_7 + 1e-06;  mean_7 = None
    rsqrt_7 = torch.rsqrt(add_28);  add_28 = None
    mul_32 = add_27 * rsqrt_7;  rsqrt_7 = None
    encoder_block_3_layer_1_layer_norm_weight = getattr(getattr(self.encoder.block, "3").layer, "1").layer_norm.weight
    getattr_24 = encoder_block_3_layer_1_layer_norm_weight.dtype
    eq_17 = getattr_24 == torch.float16;  getattr_24 = None
    getattr_25 = encoder_block_3_layer_1_layer_norm_weight.dtype
    to_18 = mul_32.to(getattr_25);  mul_32 = getattr_25 = None
    mul_33 = encoder_block_3_layer_1_layer_norm_weight * to_18;  encoder_block_3_layer_1_layer_norm_weight = to_18 = None
    encoder_block_3_layer_1_dense_relu_dense_wi_0 = getattr(getattr(self.encoder.block, "3").layer, "1").DenseReluDense.wi_0(mul_33)
    mul_34 = 0.5 * encoder_block_3_layer_1_dense_relu_dense_wi_0
    pow_12 = torch.pow(encoder_block_3_layer_1_dense_relu_dense_wi_0, 3.0)
    mul_35 = 0.044715 * pow_12;  pow_12 = None
    add_29 = encoder_block_3_layer_1_dense_relu_dense_wi_0 + mul_35;  encoder_block_3_layer_1_dense_relu_dense_wi_0 = mul_35 = None
    mul_36 = 0.7978845608028654 * add_29;  add_29 = None
    tanh_3 = torch.tanh(mul_36);  mul_36 = None
    add_30 = 1.0 + tanh_3;  tanh_3 = None
    mul_37 = mul_34 * add_30;  mul_34 = add_30 = None
    encoder_block_3_layer_1_dense_relu_dense_wi_1 = getattr(getattr(self.encoder.block, "3").layer, "1").DenseReluDense.wi_1(mul_33);  mul_33 = None
    mul_38 = mul_37 * encoder_block_3_layer_1_dense_relu_dense_wi_1;  mul_37 = encoder_block_3_layer_1_dense_relu_dense_wi_1 = None
    encoder_block_3_layer_1_dense_relu_dense_dropout = getattr(getattr(self.encoder.block, "3").layer, "1").DenseReluDense.dropout(mul_38);  mul_38 = None
    encoder_block_3_layer_1_dense_relu_dense_wo_weight = getattr(getattr(self.encoder.block, "3").layer, "1").DenseReluDense.wo.weight
    encoder_block_3_layer_1_dense_relu_dense_wo = getattr(getattr(self.encoder.block, "3").layer, "1").DenseReluDense.wo(encoder_block_3_layer_1_dense_relu_dense_dropout);  encoder_block_3_layer_1_dense_relu_dense_dropout = None
    encoder_block_3_layer_1_dropout = getattr(getattr(self.encoder.block, "3").layer, "1").dropout(encoder_block_3_layer_1_dense_relu_dense_wo);  encoder_block_3_layer_1_dense_relu_dense_wo = None
    add_31 = add_27 + encoder_block_3_layer_1_dropout;  add_27 = encoder_block_3_layer_1_dropout = None
    getattr_26 = add_31.dtype
    eq_18 = getattr_26 == torch.float16;  getattr_26 = None
    to_19 = add_31.to(torch.float32)
    pow_13 = to_19.pow(2);  to_19 = None
    mean_8 = pow_13.mean(-1, keepdim = True);  pow_13 = None
    add_32 = mean_8 + 1e-06;  mean_8 = None
    rsqrt_8 = torch.rsqrt(add_32);  add_32 = None
    mul_39 = add_31 * rsqrt_8;  rsqrt_8 = None
    encoder_block_4_layer_0_layer_norm_weight = getattr(getattr(self.encoder.block, "4").layer, "0").layer_norm.weight
    getattr_27 = encoder_block_4_layer_0_layer_norm_weight.dtype
    eq_19 = getattr_27 == torch.float16;  getattr_27 = None
    getattr_28 = encoder_block_4_layer_0_layer_norm_weight.dtype
    to_20 = mul_39.to(getattr_28);  mul_39 = getattr_28 = None
    mul_40 = encoder_block_4_layer_0_layer_norm_weight * to_20;  encoder_block_4_layer_0_layer_norm_weight = to_20 = None
    size_5 = mul_40.size()
    getitem_18 = size_5[slice(None, 2, None)];  size_5 = None
    getitem_19 = getitem_18[0]
    getitem_20 = getitem_18[1];  getitem_18 = None
    encoder_block_4_layer_0_self_attention_q = getattr(getattr(self.encoder.block, "4").layer, "0").SelfAttention.q(mul_40)
    view_17 = encoder_block_4_layer_0_self_attention_q.view(getitem_19, -1, 12, 64);  encoder_block_4_layer_0_self_attention_q = None
    transpose_20 = view_17.transpose(1, 2);  view_17 = None
    encoder_block_4_layer_0_self_attention_k = getattr(getattr(self.encoder.block, "4").layer, "0").SelfAttention.k(mul_40)
    view_18 = encoder_block_4_layer_0_self_attention_k.view(getitem_19, -1, 12, 64);  encoder_block_4_layer_0_self_attention_k = None
    transpose_21 = view_18.transpose(1, 2);  view_18 = None
    encoder_block_4_layer_0_self_attention_v = getattr(getattr(self.encoder.block, "4").layer, "0").SelfAttention.v(mul_40);  mul_40 = None
    view_19 = encoder_block_4_layer_0_self_attention_v.view(getitem_19, -1, 12, 64);  encoder_block_4_layer_0_self_attention_v = None
    transpose_22 = view_19.transpose(1, 2);  view_19 = None
    transpose_23 = transpose_21.transpose(3, 2);  transpose_21 = None
    matmul_8 = torch.matmul(transpose_20, transpose_23);  transpose_20 = transpose_23 = None
    add_33 = matmul_8 + add_4;  matmul_8 = None
    float_6 = add_33.float()
    softmax_4 = torch.nn.functional.softmax(float_6, dim = -1, _stacklevel = 3, dtype = None);  float_6 = None
    type_as_4 = softmax_4.type_as(add_33);  softmax_4 = add_33 = None
    dropout_4 = torch.nn.functional.dropout(type_as_4, p = 0.1, training = False, inplace = False);  type_as_4 = None
    matmul_9 = torch.matmul(dropout_4, transpose_22);  dropout_4 = transpose_22 = None
    transpose_24 = matmul_9.transpose(1, 2);  matmul_9 = None
    contiguous_4 = transpose_24.contiguous();  transpose_24 = None
    view_20 = contiguous_4.view(getitem_19, -1, 768);  contiguous_4 = getitem_19 = None
    encoder_block_4_layer_0_self_attention_o = getattr(getattr(self.encoder.block, "4").layer, "0").SelfAttention.o(view_20);  view_20 = None
    encoder_block_4_layer_0_dropout = getattr(getattr(self.encoder.block, "4").layer, "0").dropout(encoder_block_4_layer_0_self_attention_o);  encoder_block_4_layer_0_self_attention_o = None
    add_34 = add_31 + encoder_block_4_layer_0_dropout;  add_31 = encoder_block_4_layer_0_dropout = None
    getattr_29 = add_34.dtype
    eq_20 = getattr_29 == torch.float16;  getattr_29 = None
    to_21 = add_34.to(torch.float32)
    pow_14 = to_21.pow(2);  to_21 = None
    mean_9 = pow_14.mean(-1, keepdim = True);  pow_14 = None
    add_35 = mean_9 + 1e-06;  mean_9 = None
    rsqrt_9 = torch.rsqrt(add_35);  add_35 = None
    mul_41 = add_34 * rsqrt_9;  rsqrt_9 = None
    encoder_block_4_layer_1_layer_norm_weight = getattr(getattr(self.encoder.block, "4").layer, "1").layer_norm.weight
    getattr_30 = encoder_block_4_layer_1_layer_norm_weight.dtype
    eq_21 = getattr_30 == torch.float16;  getattr_30 = None
    getattr_31 = encoder_block_4_layer_1_layer_norm_weight.dtype
    to_22 = mul_41.to(getattr_31);  mul_41 = getattr_31 = None
    mul_42 = encoder_block_4_layer_1_layer_norm_weight * to_22;  encoder_block_4_layer_1_layer_norm_weight = to_22 = None
    encoder_block_4_layer_1_dense_relu_dense_wi_0 = getattr(getattr(self.encoder.block, "4").layer, "1").DenseReluDense.wi_0(mul_42)
    mul_43 = 0.5 * encoder_block_4_layer_1_dense_relu_dense_wi_0
    pow_15 = torch.pow(encoder_block_4_layer_1_dense_relu_dense_wi_0, 3.0)
    mul_44 = 0.044715 * pow_15;  pow_15 = None
    add_36 = encoder_block_4_layer_1_dense_relu_dense_wi_0 + mul_44;  encoder_block_4_layer_1_dense_relu_dense_wi_0 = mul_44 = None
    mul_45 = 0.7978845608028654 * add_36;  add_36 = None
    tanh_4 = torch.tanh(mul_45);  mul_45 = None
    add_37 = 1.0 + tanh_4;  tanh_4 = None
    mul_46 = mul_43 * add_37;  mul_43 = add_37 = None
    encoder_block_4_layer_1_dense_relu_dense_wi_1 = getattr(getattr(self.encoder.block, "4").layer, "1").DenseReluDense.wi_1(mul_42);  mul_42 = None
    mul_47 = mul_46 * encoder_block_4_layer_1_dense_relu_dense_wi_1;  mul_46 = encoder_block_4_layer_1_dense_relu_dense_wi_1 = None
    encoder_block_4_layer_1_dense_relu_dense_dropout = getattr(getattr(self.encoder.block, "4").layer, "1").DenseReluDense.dropout(mul_47);  mul_47 = None
    encoder_block_4_layer_1_dense_relu_dense_wo_weight = getattr(getattr(self.encoder.block, "4").layer, "1").DenseReluDense.wo.weight
    encoder_block_4_layer_1_dense_relu_dense_wo = getattr(getattr(self.encoder.block, "4").layer, "1").DenseReluDense.wo(encoder_block_4_layer_1_dense_relu_dense_dropout);  encoder_block_4_layer_1_dense_relu_dense_dropout = None
    encoder_block_4_layer_1_dropout = getattr(getattr(self.encoder.block, "4").layer, "1").dropout(encoder_block_4_layer_1_dense_relu_dense_wo);  encoder_block_4_layer_1_dense_relu_dense_wo = None
    add_38 = add_34 + encoder_block_4_layer_1_dropout;  add_34 = encoder_block_4_layer_1_dropout = None
    getattr_32 = add_38.dtype
    eq_22 = getattr_32 == torch.float16;  getattr_32 = None
    to_23 = add_38.to(torch.float32)
    pow_16 = to_23.pow(2);  to_23 = None
    mean_10 = pow_16.mean(-1, keepdim = True);  pow_16 = None
    add_39 = mean_10 + 1e-06;  mean_10 = None
    rsqrt_10 = torch.rsqrt(add_39);  add_39 = None
    mul_48 = add_38 * rsqrt_10;  rsqrt_10 = None
    encoder_block_5_layer_0_layer_norm_weight = getattr(getattr(self.encoder.block, "5").layer, "0").layer_norm.weight
    getattr_33 = encoder_block_5_layer_0_layer_norm_weight.dtype
    eq_23 = getattr_33 == torch.float16;  getattr_33 = None
    getattr_34 = encoder_block_5_layer_0_layer_norm_weight.dtype
    to_24 = mul_48.to(getattr_34);  mul_48 = getattr_34 = None
    mul_49 = encoder_block_5_layer_0_layer_norm_weight * to_24;  encoder_block_5_layer_0_layer_norm_weight = to_24 = None
    size_6 = mul_49.size()
    getitem_21 = size_6[slice(None, 2, None)];  size_6 = None
    getitem_22 = getitem_21[0]
    getitem_23 = getitem_21[1];  getitem_21 = None
    encoder_block_5_layer_0_self_attention_q = getattr(getattr(self.encoder.block, "5").layer, "0").SelfAttention.q(mul_49)
    view_21 = encoder_block_5_layer_0_self_attention_q.view(getitem_22, -1, 12, 64);  encoder_block_5_layer_0_self_attention_q = None
    transpose_25 = view_21.transpose(1, 2);  view_21 = None
    encoder_block_5_layer_0_self_attention_k = getattr(getattr(self.encoder.block, "5").layer, "0").SelfAttention.k(mul_49)
    view_22 = encoder_block_5_layer_0_self_attention_k.view(getitem_22, -1, 12, 64);  encoder_block_5_layer_0_self_attention_k = None
    transpose_26 = view_22.transpose(1, 2);  view_22 = None
    encoder_block_5_layer_0_self_attention_v = getattr(getattr(self.encoder.block, "5").layer, "0").SelfAttention.v(mul_49);  mul_49 = None
    view_23 = encoder_block_5_layer_0_self_attention_v.view(getitem_22, -1, 12, 64);  encoder_block_5_layer_0_self_attention_v = None
    transpose_27 = view_23.transpose(1, 2);  view_23 = None
    transpose_28 = transpose_26.transpose(3, 2);  transpose_26 = None
    matmul_10 = torch.matmul(transpose_25, transpose_28);  transpose_25 = transpose_28 = None
    add_40 = matmul_10 + add_4;  matmul_10 = None
    float_7 = add_40.float()
    softmax_5 = torch.nn.functional.softmax(float_7, dim = -1, _stacklevel = 3, dtype = None);  float_7 = None
    type_as_5 = softmax_5.type_as(add_40);  softmax_5 = add_40 = None
    dropout_5 = torch.nn.functional.dropout(type_as_5, p = 0.1, training = False, inplace = False);  type_as_5 = None
    matmul_11 = torch.matmul(dropout_5, transpose_27);  dropout_5 = transpose_27 = None
    transpose_29 = matmul_11.transpose(1, 2);  matmul_11 = None
    contiguous_5 = transpose_29.contiguous();  transpose_29 = None
    view_24 = contiguous_5.view(getitem_22, -1, 768);  contiguous_5 = getitem_22 = None
    encoder_block_5_layer_0_self_attention_o = getattr(getattr(self.encoder.block, "5").layer, "0").SelfAttention.o(view_24);  view_24 = None
    encoder_block_5_layer_0_dropout = getattr(getattr(self.encoder.block, "5").layer, "0").dropout(encoder_block_5_layer_0_self_attention_o);  encoder_block_5_layer_0_self_attention_o = None
    add_41 = add_38 + encoder_block_5_layer_0_dropout;  add_38 = encoder_block_5_layer_0_dropout = None
    getattr_35 = add_41.dtype
    eq_24 = getattr_35 == torch.float16;  getattr_35 = None
    to_25 = add_41.to(torch.float32)
    pow_17 = to_25.pow(2);  to_25 = None
    mean_11 = pow_17.mean(-1, keepdim = True);  pow_17 = None
    add_42 = mean_11 + 1e-06;  mean_11 = None
    rsqrt_11 = torch.rsqrt(add_42);  add_42 = None
    mul_50 = add_41 * rsqrt_11;  rsqrt_11 = None
    encoder_block_5_layer_1_layer_norm_weight = getattr(getattr(self.encoder.block, "5").layer, "1").layer_norm.weight
    getattr_36 = encoder_block_5_layer_1_layer_norm_weight.dtype
    eq_25 = getattr_36 == torch.float16;  getattr_36 = None
    getattr_37 = encoder_block_5_layer_1_layer_norm_weight.dtype
    to_26 = mul_50.to(getattr_37);  mul_50 = getattr_37 = None
    mul_51 = encoder_block_5_layer_1_layer_norm_weight * to_26;  encoder_block_5_layer_1_layer_norm_weight = to_26 = None
    encoder_block_5_layer_1_dense_relu_dense_wi_0 = getattr(getattr(self.encoder.block, "5").layer, "1").DenseReluDense.wi_0(mul_51)
    mul_52 = 0.5 * encoder_block_5_layer_1_dense_relu_dense_wi_0
    pow_18 = torch.pow(encoder_block_5_layer_1_dense_relu_dense_wi_0, 3.0)
    mul_53 = 0.044715 * pow_18;  pow_18 = None
    add_43 = encoder_block_5_layer_1_dense_relu_dense_wi_0 + mul_53;  encoder_block_5_layer_1_dense_relu_dense_wi_0 = mul_53 = None
    mul_54 = 0.7978845608028654 * add_43;  add_43 = None
    tanh_5 = torch.tanh(mul_54);  mul_54 = None
    add_44 = 1.0 + tanh_5;  tanh_5 = None
    mul_55 = mul_52 * add_44;  mul_52 = add_44 = None
    encoder_block_5_layer_1_dense_relu_dense_wi_1 = getattr(getattr(self.encoder.block, "5").layer, "1").DenseReluDense.wi_1(mul_51);  mul_51 = None
    mul_56 = mul_55 * encoder_block_5_layer_1_dense_relu_dense_wi_1;  mul_55 = encoder_block_5_layer_1_dense_relu_dense_wi_1 = None
    encoder_block_5_layer_1_dense_relu_dense_dropout = getattr(getattr(self.encoder.block, "5").layer, "1").DenseReluDense.dropout(mul_56);  mul_56 = None
    encoder_block_5_layer_1_dense_relu_dense_wo_weight = getattr(getattr(self.encoder.block, "5").layer, "1").DenseReluDense.wo.weight
    encoder_block_5_layer_1_dense_relu_dense_wo = getattr(getattr(self.encoder.block, "5").layer, "1").DenseReluDense.wo(encoder_block_5_layer_1_dense_relu_dense_dropout);  encoder_block_5_layer_1_dense_relu_dense_dropout = None
    encoder_block_5_layer_1_dropout = getattr(getattr(self.encoder.block, "5").layer, "1").dropout(encoder_block_5_layer_1_dense_relu_dense_wo);  encoder_block_5_layer_1_dense_relu_dense_wo = None
    add_45 = add_41 + encoder_block_5_layer_1_dropout;  add_41 = encoder_block_5_layer_1_dropout = None
    getattr_38 = add_45.dtype
    eq_26 = getattr_38 == torch.float16;  getattr_38 = None
    to_27 = add_45.to(torch.float32)
    pow_19 = to_27.pow(2);  to_27 = None
    mean_12 = pow_19.mean(-1, keepdim = True);  pow_19 = None
    add_46 = mean_12 + 1e-06;  mean_12 = None
    rsqrt_12 = torch.rsqrt(add_46);  add_46 = None
    mul_57 = add_45 * rsqrt_12;  rsqrt_12 = None
    encoder_block_6_layer_0_layer_norm_weight = getattr(getattr(self.encoder.block, "6").layer, "0").layer_norm.weight
    getattr_39 = encoder_block_6_layer_0_layer_norm_weight.dtype
    eq_27 = getattr_39 == torch.float16;  getattr_39 = None
    getattr_40 = encoder_block_6_layer_0_layer_norm_weight.dtype
    to_28 = mul_57.to(getattr_40);  mul_57 = getattr_40 = None
    mul_58 = encoder_block_6_layer_0_layer_norm_weight * to_28;  encoder_block_6_layer_0_layer_norm_weight = to_28 = None
    size_7 = mul_58.size()
    getitem_24 = size_7[slice(None, 2, None)];  size_7 = None
    getitem_25 = getitem_24[0]
    getitem_26 = getitem_24[1];  getitem_24 = None
    encoder_block_6_layer_0_self_attention_q = getattr(getattr(self.encoder.block, "6").layer, "0").SelfAttention.q(mul_58)
    view_25 = encoder_block_6_layer_0_self_attention_q.view(getitem_25, -1, 12, 64);  encoder_block_6_layer_0_self_attention_q = None
    transpose_30 = view_25.transpose(1, 2);  view_25 = None
    encoder_block_6_layer_0_self_attention_k = getattr(getattr(self.encoder.block, "6").layer, "0").SelfAttention.k(mul_58)
    view_26 = encoder_block_6_layer_0_self_attention_k.view(getitem_25, -1, 12, 64);  encoder_block_6_layer_0_self_attention_k = None
    transpose_31 = view_26.transpose(1, 2);  view_26 = None
    encoder_block_6_layer_0_self_attention_v = getattr(getattr(self.encoder.block, "6").layer, "0").SelfAttention.v(mul_58);  mul_58 = None
    view_27 = encoder_block_6_layer_0_self_attention_v.view(getitem_25, -1, 12, 64);  encoder_block_6_layer_0_self_attention_v = None
    transpose_32 = view_27.transpose(1, 2);  view_27 = None
    transpose_33 = transpose_31.transpose(3, 2);  transpose_31 = None
    matmul_12 = torch.matmul(transpose_30, transpose_33);  transpose_30 = transpose_33 = None
    add_47 = matmul_12 + add_4;  matmul_12 = None
    float_8 = add_47.float()
    softmax_6 = torch.nn.functional.softmax(float_8, dim = -1, _stacklevel = 3, dtype = None);  float_8 = None
    type_as_6 = softmax_6.type_as(add_47);  softmax_6 = add_47 = None
    dropout_6 = torch.nn.functional.dropout(type_as_6, p = 0.1, training = False, inplace = False);  type_as_6 = None
    matmul_13 = torch.matmul(dropout_6, transpose_32);  dropout_6 = transpose_32 = None
    transpose_34 = matmul_13.transpose(1, 2);  matmul_13 = None
    contiguous_6 = transpose_34.contiguous();  transpose_34 = None
    view_28 = contiguous_6.view(getitem_25, -1, 768);  contiguous_6 = getitem_25 = None
    encoder_block_6_layer_0_self_attention_o = getattr(getattr(self.encoder.block, "6").layer, "0").SelfAttention.o(view_28);  view_28 = None
    encoder_block_6_layer_0_dropout = getattr(getattr(self.encoder.block, "6").layer, "0").dropout(encoder_block_6_layer_0_self_attention_o);  encoder_block_6_layer_0_self_attention_o = None
    add_48 = add_45 + encoder_block_6_layer_0_dropout;  add_45 = encoder_block_6_layer_0_dropout = None
    getattr_41 = add_48.dtype
    eq_28 = getattr_41 == torch.float16;  getattr_41 = None
    to_29 = add_48.to(torch.float32)
    pow_20 = to_29.pow(2);  to_29 = None
    mean_13 = pow_20.mean(-1, keepdim = True);  pow_20 = None
    add_49 = mean_13 + 1e-06;  mean_13 = None
    rsqrt_13 = torch.rsqrt(add_49);  add_49 = None
    mul_59 = add_48 * rsqrt_13;  rsqrt_13 = None
    encoder_block_6_layer_1_layer_norm_weight = getattr(getattr(self.encoder.block, "6").layer, "1").layer_norm.weight
    getattr_42 = encoder_block_6_layer_1_layer_norm_weight.dtype
    eq_29 = getattr_42 == torch.float16;  getattr_42 = None
    getattr_43 = encoder_block_6_layer_1_layer_norm_weight.dtype
    to_30 = mul_59.to(getattr_43);  mul_59 = getattr_43 = None
    mul_60 = encoder_block_6_layer_1_layer_norm_weight * to_30;  encoder_block_6_layer_1_layer_norm_weight = to_30 = None
    encoder_block_6_layer_1_dense_relu_dense_wi_0 = getattr(getattr(self.encoder.block, "6").layer, "1").DenseReluDense.wi_0(mul_60)
    mul_61 = 0.5 * encoder_block_6_layer_1_dense_relu_dense_wi_0
    pow_21 = torch.pow(encoder_block_6_layer_1_dense_relu_dense_wi_0, 3.0)
    mul_62 = 0.044715 * pow_21;  pow_21 = None
    add_50 = encoder_block_6_layer_1_dense_relu_dense_wi_0 + mul_62;  encoder_block_6_layer_1_dense_relu_dense_wi_0 = mul_62 = None
    mul_63 = 0.7978845608028654 * add_50;  add_50 = None
    tanh_6 = torch.tanh(mul_63);  mul_63 = None
    add_51 = 1.0 + tanh_6;  tanh_6 = None
    mul_64 = mul_61 * add_51;  mul_61 = add_51 = None
    encoder_block_6_layer_1_dense_relu_dense_wi_1 = getattr(getattr(self.encoder.block, "6").layer, "1").DenseReluDense.wi_1(mul_60);  mul_60 = None
    mul_65 = mul_64 * encoder_block_6_layer_1_dense_relu_dense_wi_1;  mul_64 = encoder_block_6_layer_1_dense_relu_dense_wi_1 = None
    encoder_block_6_layer_1_dense_relu_dense_dropout = getattr(getattr(self.encoder.block, "6").layer, "1").DenseReluDense.dropout(mul_65);  mul_65 = None
    encoder_block_6_layer_1_dense_relu_dense_wo_weight = getattr(getattr(self.encoder.block, "6").layer, "1").DenseReluDense.wo.weight
    encoder_block_6_layer_1_dense_relu_dense_wo = getattr(getattr(self.encoder.block, "6").layer, "1").DenseReluDense.wo(encoder_block_6_layer_1_dense_relu_dense_dropout);  encoder_block_6_layer_1_dense_relu_dense_dropout = None
    encoder_block_6_layer_1_dropout = getattr(getattr(self.encoder.block, "6").layer, "1").dropout(encoder_block_6_layer_1_dense_relu_dense_wo);  encoder_block_6_layer_1_dense_relu_dense_wo = None
    add_52 = add_48 + encoder_block_6_layer_1_dropout;  add_48 = encoder_block_6_layer_1_dropout = None
    getattr_44 = add_52.dtype
    eq_30 = getattr_44 == torch.float16;  getattr_44 = None
    to_31 = add_52.to(torch.float32)
    pow_22 = to_31.pow(2);  to_31 = None
    mean_14 = pow_22.mean(-1, keepdim = True);  pow_22 = None
    add_53 = mean_14 + 1e-06;  mean_14 = None
    rsqrt_14 = torch.rsqrt(add_53);  add_53 = None
    mul_66 = add_52 * rsqrt_14;  rsqrt_14 = None
    encoder_block_7_layer_0_layer_norm_weight = getattr(getattr(self.encoder.block, "7").layer, "0").layer_norm.weight
    getattr_45 = encoder_block_7_layer_0_layer_norm_weight.dtype
    eq_31 = getattr_45 == torch.float16;  getattr_45 = None
    getattr_46 = encoder_block_7_layer_0_layer_norm_weight.dtype
    to_32 = mul_66.to(getattr_46);  mul_66 = getattr_46 = None
    mul_67 = encoder_block_7_layer_0_layer_norm_weight * to_32;  encoder_block_7_layer_0_layer_norm_weight = to_32 = None
    size_8 = mul_67.size()
    getitem_27 = size_8[slice(None, 2, None)];  size_8 = None
    getitem_28 = getitem_27[0]
    getitem_29 = getitem_27[1];  getitem_27 = None
    encoder_block_7_layer_0_self_attention_q = getattr(getattr(self.encoder.block, "7").layer, "0").SelfAttention.q(mul_67)
    view_29 = encoder_block_7_layer_0_self_attention_q.view(getitem_28, -1, 12, 64);  encoder_block_7_layer_0_self_attention_q = None
    transpose_35 = view_29.transpose(1, 2);  view_29 = None
    encoder_block_7_layer_0_self_attention_k = getattr(getattr(self.encoder.block, "7").layer, "0").SelfAttention.k(mul_67)
    view_30 = encoder_block_7_layer_0_self_attention_k.view(getitem_28, -1, 12, 64);  encoder_block_7_layer_0_self_attention_k = None
    transpose_36 = view_30.transpose(1, 2);  view_30 = None
    encoder_block_7_layer_0_self_attention_v = getattr(getattr(self.encoder.block, "7").layer, "0").SelfAttention.v(mul_67);  mul_67 = None
    view_31 = encoder_block_7_layer_0_self_attention_v.view(getitem_28, -1, 12, 64);  encoder_block_7_layer_0_self_attention_v = None
    transpose_37 = view_31.transpose(1, 2);  view_31 = None
    transpose_38 = transpose_36.transpose(3, 2);  transpose_36 = None
    matmul_14 = torch.matmul(transpose_35, transpose_38);  transpose_35 = transpose_38 = None
    add_54 = matmul_14 + add_4;  matmul_14 = None
    float_9 = add_54.float()
    softmax_7 = torch.nn.functional.softmax(float_9, dim = -1, _stacklevel = 3, dtype = None);  float_9 = None
    type_as_7 = softmax_7.type_as(add_54);  softmax_7 = add_54 = None
    dropout_7 = torch.nn.functional.dropout(type_as_7, p = 0.1, training = False, inplace = False);  type_as_7 = None
    matmul_15 = torch.matmul(dropout_7, transpose_37);  dropout_7 = transpose_37 = None
    transpose_39 = matmul_15.transpose(1, 2);  matmul_15 = None
    contiguous_7 = transpose_39.contiguous();  transpose_39 = None
    view_32 = contiguous_7.view(getitem_28, -1, 768);  contiguous_7 = getitem_28 = None
    encoder_block_7_layer_0_self_attention_o = getattr(getattr(self.encoder.block, "7").layer, "0").SelfAttention.o(view_32);  view_32 = None
    encoder_block_7_layer_0_dropout = getattr(getattr(self.encoder.block, "7").layer, "0").dropout(encoder_block_7_layer_0_self_attention_o);  encoder_block_7_layer_0_self_attention_o = None
    add_55 = add_52 + encoder_block_7_layer_0_dropout;  add_52 = encoder_block_7_layer_0_dropout = None
    getattr_47 = add_55.dtype
    eq_32 = getattr_47 == torch.float16;  getattr_47 = None
    to_33 = add_55.to(torch.float32)
    pow_23 = to_33.pow(2);  to_33 = None
    mean_15 = pow_23.mean(-1, keepdim = True);  pow_23 = None
    add_56 = mean_15 + 1e-06;  mean_15 = None
    rsqrt_15 = torch.rsqrt(add_56);  add_56 = None
    mul_68 = add_55 * rsqrt_15;  rsqrt_15 = None
    encoder_block_7_layer_1_layer_norm_weight = getattr(getattr(self.encoder.block, "7").layer, "1").layer_norm.weight
    getattr_48 = encoder_block_7_layer_1_layer_norm_weight.dtype
    eq_33 = getattr_48 == torch.float16;  getattr_48 = None
    getattr_49 = encoder_block_7_layer_1_layer_norm_weight.dtype
    to_34 = mul_68.to(getattr_49);  mul_68 = getattr_49 = None
    mul_69 = encoder_block_7_layer_1_layer_norm_weight * to_34;  encoder_block_7_layer_1_layer_norm_weight = to_34 = None
    encoder_block_7_layer_1_dense_relu_dense_wi_0 = getattr(getattr(self.encoder.block, "7").layer, "1").DenseReluDense.wi_0(mul_69)
    mul_70 = 0.5 * encoder_block_7_layer_1_dense_relu_dense_wi_0
    pow_24 = torch.pow(encoder_block_7_layer_1_dense_relu_dense_wi_0, 3.0)
    mul_71 = 0.044715 * pow_24;  pow_24 = None
    add_57 = encoder_block_7_layer_1_dense_relu_dense_wi_0 + mul_71;  encoder_block_7_layer_1_dense_relu_dense_wi_0 = mul_71 = None
    mul_72 = 0.7978845608028654 * add_57;  add_57 = None
    tanh_7 = torch.tanh(mul_72);  mul_72 = None
    add_58 = 1.0 + tanh_7;  tanh_7 = None
    mul_73 = mul_70 * add_58;  mul_70 = add_58 = None
    encoder_block_7_layer_1_dense_relu_dense_wi_1 = getattr(getattr(self.encoder.block, "7").layer, "1").DenseReluDense.wi_1(mul_69);  mul_69 = None
    mul_74 = mul_73 * encoder_block_7_layer_1_dense_relu_dense_wi_1;  mul_73 = encoder_block_7_layer_1_dense_relu_dense_wi_1 = None
    encoder_block_7_layer_1_dense_relu_dense_dropout = getattr(getattr(self.encoder.block, "7").layer, "1").DenseReluDense.dropout(mul_74);  mul_74 = None
    encoder_block_7_layer_1_dense_relu_dense_wo_weight = getattr(getattr(self.encoder.block, "7").layer, "1").DenseReluDense.wo.weight
    encoder_block_7_layer_1_dense_relu_dense_wo = getattr(getattr(self.encoder.block, "7").layer, "1").DenseReluDense.wo(encoder_block_7_layer_1_dense_relu_dense_dropout);  encoder_block_7_layer_1_dense_relu_dense_dropout = None
    encoder_block_7_layer_1_dropout = getattr(getattr(self.encoder.block, "7").layer, "1").dropout(encoder_block_7_layer_1_dense_relu_dense_wo);  encoder_block_7_layer_1_dense_relu_dense_wo = None
    add_59 = add_55 + encoder_block_7_layer_1_dropout;  add_55 = encoder_block_7_layer_1_dropout = None
    getattr_50 = add_59.dtype
    eq_34 = getattr_50 == torch.float16;  getattr_50 = None
    to_35 = add_59.to(torch.float32)
    pow_25 = to_35.pow(2);  to_35 = None
    mean_16 = pow_25.mean(-1, keepdim = True);  pow_25 = None
    add_60 = mean_16 + 1e-06;  mean_16 = None
    rsqrt_16 = torch.rsqrt(add_60);  add_60 = None
    mul_75 = add_59 * rsqrt_16;  rsqrt_16 = None
    encoder_block_8_layer_0_layer_norm_weight = getattr(getattr(self.encoder.block, "8").layer, "0").layer_norm.weight
    getattr_51 = encoder_block_8_layer_0_layer_norm_weight.dtype
    eq_35 = getattr_51 == torch.float16;  getattr_51 = None
    getattr_52 = encoder_block_8_layer_0_layer_norm_weight.dtype
    to_36 = mul_75.to(getattr_52);  mul_75 = getattr_52 = None
    mul_76 = encoder_block_8_layer_0_layer_norm_weight * to_36;  encoder_block_8_layer_0_layer_norm_weight = to_36 = None
    size_9 = mul_76.size()
    getitem_30 = size_9[slice(None, 2, None)];  size_9 = None
    getitem_31 = getitem_30[0]
    getitem_32 = getitem_30[1];  getitem_30 = None
    encoder_block_8_layer_0_self_attention_q = getattr(getattr(self.encoder.block, "8").layer, "0").SelfAttention.q(mul_76)
    view_33 = encoder_block_8_layer_0_self_attention_q.view(getitem_31, -1, 12, 64);  encoder_block_8_layer_0_self_attention_q = None
    transpose_40 = view_33.transpose(1, 2);  view_33 = None
    encoder_block_8_layer_0_self_attention_k = getattr(getattr(self.encoder.block, "8").layer, "0").SelfAttention.k(mul_76)
    view_34 = encoder_block_8_layer_0_self_attention_k.view(getitem_31, -1, 12, 64);  encoder_block_8_layer_0_self_attention_k = None
    transpose_41 = view_34.transpose(1, 2);  view_34 = None
    encoder_block_8_layer_0_self_attention_v = getattr(getattr(self.encoder.block, "8").layer, "0").SelfAttention.v(mul_76);  mul_76 = None
    view_35 = encoder_block_8_layer_0_self_attention_v.view(getitem_31, -1, 12, 64);  encoder_block_8_layer_0_self_attention_v = None
    transpose_42 = view_35.transpose(1, 2);  view_35 = None
    transpose_43 = transpose_41.transpose(3, 2);  transpose_41 = None
    matmul_16 = torch.matmul(transpose_40, transpose_43);  transpose_40 = transpose_43 = None
    add_61 = matmul_16 + add_4;  matmul_16 = None
    float_10 = add_61.float()
    softmax_8 = torch.nn.functional.softmax(float_10, dim = -1, _stacklevel = 3, dtype = None);  float_10 = None
    type_as_8 = softmax_8.type_as(add_61);  softmax_8 = add_61 = None
    dropout_8 = torch.nn.functional.dropout(type_as_8, p = 0.1, training = False, inplace = False);  type_as_8 = None
    matmul_17 = torch.matmul(dropout_8, transpose_42);  dropout_8 = transpose_42 = None
    transpose_44 = matmul_17.transpose(1, 2);  matmul_17 = None
    contiguous_8 = transpose_44.contiguous();  transpose_44 = None
    view_36 = contiguous_8.view(getitem_31, -1, 768);  contiguous_8 = getitem_31 = None
    encoder_block_8_layer_0_self_attention_o = getattr(getattr(self.encoder.block, "8").layer, "0").SelfAttention.o(view_36);  view_36 = None
    encoder_block_8_layer_0_dropout = getattr(getattr(self.encoder.block, "8").layer, "0").dropout(encoder_block_8_layer_0_self_attention_o);  encoder_block_8_layer_0_self_attention_o = None
    add_62 = add_59 + encoder_block_8_layer_0_dropout;  add_59 = encoder_block_8_layer_0_dropout = None
    getattr_53 = add_62.dtype
    eq_36 = getattr_53 == torch.float16;  getattr_53 = None
    to_37 = add_62.to(torch.float32)
    pow_26 = to_37.pow(2);  to_37 = None
    mean_17 = pow_26.mean(-1, keepdim = True);  pow_26 = None
    add_63 = mean_17 + 1e-06;  mean_17 = None
    rsqrt_17 = torch.rsqrt(add_63);  add_63 = None
    mul_77 = add_62 * rsqrt_17;  rsqrt_17 = None
    encoder_block_8_layer_1_layer_norm_weight = getattr(getattr(self.encoder.block, "8").layer, "1").layer_norm.weight
    getattr_54 = encoder_block_8_layer_1_layer_norm_weight.dtype
    eq_37 = getattr_54 == torch.float16;  getattr_54 = None
    getattr_55 = encoder_block_8_layer_1_layer_norm_weight.dtype
    to_38 = mul_77.to(getattr_55);  mul_77 = getattr_55 = None
    mul_78 = encoder_block_8_layer_1_layer_norm_weight * to_38;  encoder_block_8_layer_1_layer_norm_weight = to_38 = None
    encoder_block_8_layer_1_dense_relu_dense_wi_0 = getattr(getattr(self.encoder.block, "8").layer, "1").DenseReluDense.wi_0(mul_78)
    mul_79 = 0.5 * encoder_block_8_layer_1_dense_relu_dense_wi_0
    pow_27 = torch.pow(encoder_block_8_layer_1_dense_relu_dense_wi_0, 3.0)
    mul_80 = 0.044715 * pow_27;  pow_27 = None
    add_64 = encoder_block_8_layer_1_dense_relu_dense_wi_0 + mul_80;  encoder_block_8_layer_1_dense_relu_dense_wi_0 = mul_80 = None
    mul_81 = 0.7978845608028654 * add_64;  add_64 = None
    tanh_8 = torch.tanh(mul_81);  mul_81 = None
    add_65 = 1.0 + tanh_8;  tanh_8 = None
    mul_82 = mul_79 * add_65;  mul_79 = add_65 = None
    encoder_block_8_layer_1_dense_relu_dense_wi_1 = getattr(getattr(self.encoder.block, "8").layer, "1").DenseReluDense.wi_1(mul_78);  mul_78 = None
    mul_83 = mul_82 * encoder_block_8_layer_1_dense_relu_dense_wi_1;  mul_82 = encoder_block_8_layer_1_dense_relu_dense_wi_1 = None
    encoder_block_8_layer_1_dense_relu_dense_dropout = getattr(getattr(self.encoder.block, "8").layer, "1").DenseReluDense.dropout(mul_83);  mul_83 = None
    encoder_block_8_layer_1_dense_relu_dense_wo_weight = getattr(getattr(self.encoder.block, "8").layer, "1").DenseReluDense.wo.weight
    encoder_block_8_layer_1_dense_relu_dense_wo = getattr(getattr(self.encoder.block, "8").layer, "1").DenseReluDense.wo(encoder_block_8_layer_1_dense_relu_dense_dropout);  encoder_block_8_layer_1_dense_relu_dense_dropout = None
    encoder_block_8_layer_1_dropout = getattr(getattr(self.encoder.block, "8").layer, "1").dropout(encoder_block_8_layer_1_dense_relu_dense_wo);  encoder_block_8_layer_1_dense_relu_dense_wo = None
    add_66 = add_62 + encoder_block_8_layer_1_dropout;  add_62 = encoder_block_8_layer_1_dropout = None
    getattr_56 = add_66.dtype
    eq_38 = getattr_56 == torch.float16;  getattr_56 = None
    to_39 = add_66.to(torch.float32)
    pow_28 = to_39.pow(2);  to_39 = None
    mean_18 = pow_28.mean(-1, keepdim = True);  pow_28 = None
    add_67 = mean_18 + 1e-06;  mean_18 = None
    rsqrt_18 = torch.rsqrt(add_67);  add_67 = None
    mul_84 = add_66 * rsqrt_18;  rsqrt_18 = None
    encoder_block_9_layer_0_layer_norm_weight = getattr(getattr(self.encoder.block, "9").layer, "0").layer_norm.weight
    getattr_57 = encoder_block_9_layer_0_layer_norm_weight.dtype
    eq_39 = getattr_57 == torch.float16;  getattr_57 = None
    getattr_58 = encoder_block_9_layer_0_layer_norm_weight.dtype
    to_40 = mul_84.to(getattr_58);  mul_84 = getattr_58 = None
    mul_85 = encoder_block_9_layer_0_layer_norm_weight * to_40;  encoder_block_9_layer_0_layer_norm_weight = to_40 = None
    size_10 = mul_85.size()
    getitem_33 = size_10[slice(None, 2, None)];  size_10 = None
    getitem_34 = getitem_33[0]
    getitem_35 = getitem_33[1];  getitem_33 = None
    encoder_block_9_layer_0_self_attention_q = getattr(getattr(self.encoder.block, "9").layer, "0").SelfAttention.q(mul_85)
    view_37 = encoder_block_9_layer_0_self_attention_q.view(getitem_34, -1, 12, 64);  encoder_block_9_layer_0_self_attention_q = None
    transpose_45 = view_37.transpose(1, 2);  view_37 = None
    encoder_block_9_layer_0_self_attention_k = getattr(getattr(self.encoder.block, "9").layer, "0").SelfAttention.k(mul_85)
    view_38 = encoder_block_9_layer_0_self_attention_k.view(getitem_34, -1, 12, 64);  encoder_block_9_layer_0_self_attention_k = None
    transpose_46 = view_38.transpose(1, 2);  view_38 = None
    encoder_block_9_layer_0_self_attention_v = getattr(getattr(self.encoder.block, "9").layer, "0").SelfAttention.v(mul_85);  mul_85 = None
    view_39 = encoder_block_9_layer_0_self_attention_v.view(getitem_34, -1, 12, 64);  encoder_block_9_layer_0_self_attention_v = None
    transpose_47 = view_39.transpose(1, 2);  view_39 = None
    transpose_48 = transpose_46.transpose(3, 2);  transpose_46 = None
    matmul_18 = torch.matmul(transpose_45, transpose_48);  transpose_45 = transpose_48 = None
    add_68 = matmul_18 + add_4;  matmul_18 = None
    float_11 = add_68.float()
    softmax_9 = torch.nn.functional.softmax(float_11, dim = -1, _stacklevel = 3, dtype = None);  float_11 = None
    type_as_9 = softmax_9.type_as(add_68);  softmax_9 = add_68 = None
    dropout_9 = torch.nn.functional.dropout(type_as_9, p = 0.1, training = False, inplace = False);  type_as_9 = None
    matmul_19 = torch.matmul(dropout_9, transpose_47);  dropout_9 = transpose_47 = None
    transpose_49 = matmul_19.transpose(1, 2);  matmul_19 = None
    contiguous_9 = transpose_49.contiguous();  transpose_49 = None
    view_40 = contiguous_9.view(getitem_34, -1, 768);  contiguous_9 = getitem_34 = None
    encoder_block_9_layer_0_self_attention_o = getattr(getattr(self.encoder.block, "9").layer, "0").SelfAttention.o(view_40);  view_40 = None
    encoder_block_9_layer_0_dropout = getattr(getattr(self.encoder.block, "9").layer, "0").dropout(encoder_block_9_layer_0_self_attention_o);  encoder_block_9_layer_0_self_attention_o = None
    add_69 = add_66 + encoder_block_9_layer_0_dropout;  add_66 = encoder_block_9_layer_0_dropout = None
    getattr_59 = add_69.dtype
    eq_40 = getattr_59 == torch.float16;  getattr_59 = None
    to_41 = add_69.to(torch.float32)
    pow_29 = to_41.pow(2);  to_41 = None
    mean_19 = pow_29.mean(-1, keepdim = True);  pow_29 = None
    add_70 = mean_19 + 1e-06;  mean_19 = None
    rsqrt_19 = torch.rsqrt(add_70);  add_70 = None
    mul_86 = add_69 * rsqrt_19;  rsqrt_19 = None
    encoder_block_9_layer_1_layer_norm_weight = getattr(getattr(self.encoder.block, "9").layer, "1").layer_norm.weight
    getattr_60 = encoder_block_9_layer_1_layer_norm_weight.dtype
    eq_41 = getattr_60 == torch.float16;  getattr_60 = None
    getattr_61 = encoder_block_9_layer_1_layer_norm_weight.dtype
    to_42 = mul_86.to(getattr_61);  mul_86 = getattr_61 = None
    mul_87 = encoder_block_9_layer_1_layer_norm_weight * to_42;  encoder_block_9_layer_1_layer_norm_weight = to_42 = None
    encoder_block_9_layer_1_dense_relu_dense_wi_0 = getattr(getattr(self.encoder.block, "9").layer, "1").DenseReluDense.wi_0(mul_87)
    mul_88 = 0.5 * encoder_block_9_layer_1_dense_relu_dense_wi_0
    pow_30 = torch.pow(encoder_block_9_layer_1_dense_relu_dense_wi_0, 3.0)
    mul_89 = 0.044715 * pow_30;  pow_30 = None
    add_71 = encoder_block_9_layer_1_dense_relu_dense_wi_0 + mul_89;  encoder_block_9_layer_1_dense_relu_dense_wi_0 = mul_89 = None
    mul_90 = 0.7978845608028654 * add_71;  add_71 = None
    tanh_9 = torch.tanh(mul_90);  mul_90 = None
    add_72 = 1.0 + tanh_9;  tanh_9 = None
    mul_91 = mul_88 * add_72;  mul_88 = add_72 = None
    encoder_block_9_layer_1_dense_relu_dense_wi_1 = getattr(getattr(self.encoder.block, "9").layer, "1").DenseReluDense.wi_1(mul_87);  mul_87 = None
    mul_92 = mul_91 * encoder_block_9_layer_1_dense_relu_dense_wi_1;  mul_91 = encoder_block_9_layer_1_dense_relu_dense_wi_1 = None
    encoder_block_9_layer_1_dense_relu_dense_dropout = getattr(getattr(self.encoder.block, "9").layer, "1").DenseReluDense.dropout(mul_92);  mul_92 = None
    encoder_block_9_layer_1_dense_relu_dense_wo_weight = getattr(getattr(self.encoder.block, "9").layer, "1").DenseReluDense.wo.weight
    encoder_block_9_layer_1_dense_relu_dense_wo = getattr(getattr(self.encoder.block, "9").layer, "1").DenseReluDense.wo(encoder_block_9_layer_1_dense_relu_dense_dropout);  encoder_block_9_layer_1_dense_relu_dense_dropout = None
    encoder_block_9_layer_1_dropout = getattr(getattr(self.encoder.block, "9").layer, "1").dropout(encoder_block_9_layer_1_dense_relu_dense_wo);  encoder_block_9_layer_1_dense_relu_dense_wo = None
    add_73 = add_69 + encoder_block_9_layer_1_dropout;  add_69 = encoder_block_9_layer_1_dropout = None
    getattr_62 = add_73.dtype
    eq_42 = getattr_62 == torch.float16;  getattr_62 = None
    to_43 = add_73.to(torch.float32)
    pow_31 = to_43.pow(2);  to_43 = None
    mean_20 = pow_31.mean(-1, keepdim = True);  pow_31 = None
    add_74 = mean_20 + 1e-06;  mean_20 = None
    rsqrt_20 = torch.rsqrt(add_74);  add_74 = None
    mul_93 = add_73 * rsqrt_20;  rsqrt_20 = None
    encoder_block_10_layer_0_layer_norm_weight = getattr(getattr(self.encoder.block, "10").layer, "0").layer_norm.weight
    getattr_63 = encoder_block_10_layer_0_layer_norm_weight.dtype
    eq_43 = getattr_63 == torch.float16;  getattr_63 = None
    getattr_64 = encoder_block_10_layer_0_layer_norm_weight.dtype
    to_44 = mul_93.to(getattr_64);  mul_93 = getattr_64 = None
    mul_94 = encoder_block_10_layer_0_layer_norm_weight * to_44;  encoder_block_10_layer_0_layer_norm_weight = to_44 = None
    size_11 = mul_94.size()
    getitem_36 = size_11[slice(None, 2, None)];  size_11 = None
    getitem_37 = getitem_36[0]
    getitem_38 = getitem_36[1];  getitem_36 = None
    encoder_block_10_layer_0_self_attention_q = getattr(getattr(self.encoder.block, "10").layer, "0").SelfAttention.q(mul_94)
    view_41 = encoder_block_10_layer_0_self_attention_q.view(getitem_37, -1, 12, 64);  encoder_block_10_layer_0_self_attention_q = None
    transpose_50 = view_41.transpose(1, 2);  view_41 = None
    encoder_block_10_layer_0_self_attention_k = getattr(getattr(self.encoder.block, "10").layer, "0").SelfAttention.k(mul_94)
    view_42 = encoder_block_10_layer_0_self_attention_k.view(getitem_37, -1, 12, 64);  encoder_block_10_layer_0_self_attention_k = None
    transpose_51 = view_42.transpose(1, 2);  view_42 = None
    encoder_block_10_layer_0_self_attention_v = getattr(getattr(self.encoder.block, "10").layer, "0").SelfAttention.v(mul_94);  mul_94 = None
    view_43 = encoder_block_10_layer_0_self_attention_v.view(getitem_37, -1, 12, 64);  encoder_block_10_layer_0_self_attention_v = None
    transpose_52 = view_43.transpose(1, 2);  view_43 = None
    transpose_53 = transpose_51.transpose(3, 2);  transpose_51 = None
    matmul_20 = torch.matmul(transpose_50, transpose_53);  transpose_50 = transpose_53 = None
    add_75 = matmul_20 + add_4;  matmul_20 = None
    float_12 = add_75.float()
    softmax_10 = torch.nn.functional.softmax(float_12, dim = -1, _stacklevel = 3, dtype = None);  float_12 = None
    type_as_10 = softmax_10.type_as(add_75);  softmax_10 = add_75 = None
    dropout_10 = torch.nn.functional.dropout(type_as_10, p = 0.1, training = False, inplace = False);  type_as_10 = None
    matmul_21 = torch.matmul(dropout_10, transpose_52);  dropout_10 = transpose_52 = None
    transpose_54 = matmul_21.transpose(1, 2);  matmul_21 = None
    contiguous_10 = transpose_54.contiguous();  transpose_54 = None
    view_44 = contiguous_10.view(getitem_37, -1, 768);  contiguous_10 = getitem_37 = None
    encoder_block_10_layer_0_self_attention_o = getattr(getattr(self.encoder.block, "10").layer, "0").SelfAttention.o(view_44);  view_44 = None
    encoder_block_10_layer_0_dropout = getattr(getattr(self.encoder.block, "10").layer, "0").dropout(encoder_block_10_layer_0_self_attention_o);  encoder_block_10_layer_0_self_attention_o = None
    add_76 = add_73 + encoder_block_10_layer_0_dropout;  add_73 = encoder_block_10_layer_0_dropout = None
    getattr_65 = add_76.dtype
    eq_44 = getattr_65 == torch.float16;  getattr_65 = None
    to_45 = add_76.to(torch.float32)
    pow_32 = to_45.pow(2);  to_45 = None
    mean_21 = pow_32.mean(-1, keepdim = True);  pow_32 = None
    add_77 = mean_21 + 1e-06;  mean_21 = None
    rsqrt_21 = torch.rsqrt(add_77);  add_77 = None
    mul_95 = add_76 * rsqrt_21;  rsqrt_21 = None
    encoder_block_10_layer_1_layer_norm_weight = getattr(getattr(self.encoder.block, "10").layer, "1").layer_norm.weight
    getattr_66 = encoder_block_10_layer_1_layer_norm_weight.dtype
    eq_45 = getattr_66 == torch.float16;  getattr_66 = None
    getattr_67 = encoder_block_10_layer_1_layer_norm_weight.dtype
    to_46 = mul_95.to(getattr_67);  mul_95 = getattr_67 = None
    mul_96 = encoder_block_10_layer_1_layer_norm_weight * to_46;  encoder_block_10_layer_1_layer_norm_weight = to_46 = None
    encoder_block_10_layer_1_dense_relu_dense_wi_0 = getattr(getattr(self.encoder.block, "10").layer, "1").DenseReluDense.wi_0(mul_96)
    mul_97 = 0.5 * encoder_block_10_layer_1_dense_relu_dense_wi_0
    pow_33 = torch.pow(encoder_block_10_layer_1_dense_relu_dense_wi_0, 3.0)
    mul_98 = 0.044715 * pow_33;  pow_33 = None
    add_78 = encoder_block_10_layer_1_dense_relu_dense_wi_0 + mul_98;  encoder_block_10_layer_1_dense_relu_dense_wi_0 = mul_98 = None
    mul_99 = 0.7978845608028654 * add_78;  add_78 = None
    tanh_10 = torch.tanh(mul_99);  mul_99 = None
    add_79 = 1.0 + tanh_10;  tanh_10 = None
    mul_100 = mul_97 * add_79;  mul_97 = add_79 = None
    encoder_block_10_layer_1_dense_relu_dense_wi_1 = getattr(getattr(self.encoder.block, "10").layer, "1").DenseReluDense.wi_1(mul_96);  mul_96 = None
    mul_101 = mul_100 * encoder_block_10_layer_1_dense_relu_dense_wi_1;  mul_100 = encoder_block_10_layer_1_dense_relu_dense_wi_1 = None
    encoder_block_10_layer_1_dense_relu_dense_dropout = getattr(getattr(self.encoder.block, "10").layer, "1").DenseReluDense.dropout(mul_101);  mul_101 = None
    encoder_block_10_layer_1_dense_relu_dense_wo_weight = getattr(getattr(self.encoder.block, "10").layer, "1").DenseReluDense.wo.weight
    encoder_block_10_layer_1_dense_relu_dense_wo = getattr(getattr(self.encoder.block, "10").layer, "1").DenseReluDense.wo(encoder_block_10_layer_1_dense_relu_dense_dropout);  encoder_block_10_layer_1_dense_relu_dense_dropout = None
    encoder_block_10_layer_1_dropout = getattr(getattr(self.encoder.block, "10").layer, "1").dropout(encoder_block_10_layer_1_dense_relu_dense_wo);  encoder_block_10_layer_1_dense_relu_dense_wo = None
    add_80 = add_76 + encoder_block_10_layer_1_dropout;  add_76 = encoder_block_10_layer_1_dropout = None
    getattr_68 = add_80.dtype
    eq_46 = getattr_68 == torch.float16;  getattr_68 = None
    to_47 = add_80.to(torch.float32)
    pow_34 = to_47.pow(2);  to_47 = None
    mean_22 = pow_34.mean(-1, keepdim = True);  pow_34 = None
    add_81 = mean_22 + 1e-06;  mean_22 = None
    rsqrt_22 = torch.rsqrt(add_81);  add_81 = None
    mul_102 = add_80 * rsqrt_22;  rsqrt_22 = None
    encoder_block_11_layer_0_layer_norm_weight = getattr(getattr(self.encoder.block, "11").layer, "0").layer_norm.weight
    getattr_69 = encoder_block_11_layer_0_layer_norm_weight.dtype
    eq_47 = getattr_69 == torch.float16;  getattr_69 = None
    getattr_70 = encoder_block_11_layer_0_layer_norm_weight.dtype
    to_48 = mul_102.to(getattr_70);  mul_102 = getattr_70 = None
    mul_103 = encoder_block_11_layer_0_layer_norm_weight * to_48;  encoder_block_11_layer_0_layer_norm_weight = to_48 = None
    size_12 = mul_103.size()
    getitem_39 = size_12[slice(None, 2, None)];  size_12 = None
    getitem_40 = getitem_39[0]
    getitem_41 = getitem_39[1];  getitem_39 = None
    encoder_block_11_layer_0_self_attention_q = getattr(getattr(self.encoder.block, "11").layer, "0").SelfAttention.q(mul_103)
    view_45 = encoder_block_11_layer_0_self_attention_q.view(getitem_40, -1, 12, 64);  encoder_block_11_layer_0_self_attention_q = None
    transpose_55 = view_45.transpose(1, 2);  view_45 = None
    encoder_block_11_layer_0_self_attention_k = getattr(getattr(self.encoder.block, "11").layer, "0").SelfAttention.k(mul_103)
    view_46 = encoder_block_11_layer_0_self_attention_k.view(getitem_40, -1, 12, 64);  encoder_block_11_layer_0_self_attention_k = None
    transpose_56 = view_46.transpose(1, 2);  view_46 = None
    encoder_block_11_layer_0_self_attention_v = getattr(getattr(self.encoder.block, "11").layer, "0").SelfAttention.v(mul_103);  mul_103 = None
    view_47 = encoder_block_11_layer_0_self_attention_v.view(getitem_40, -1, 12, 64);  encoder_block_11_layer_0_self_attention_v = None
    transpose_57 = view_47.transpose(1, 2);  view_47 = None
    transpose_58 = transpose_56.transpose(3, 2);  transpose_56 = None
    matmul_22 = torch.matmul(transpose_55, transpose_58);  transpose_55 = transpose_58 = None
    add_82 = matmul_22 + add_4;  matmul_22 = add_4 = None
    float_13 = add_82.float()
    softmax_11 = torch.nn.functional.softmax(float_13, dim = -1, _stacklevel = 3, dtype = None);  float_13 = None
    type_as_11 = softmax_11.type_as(add_82);  softmax_11 = add_82 = None
    dropout_11 = torch.nn.functional.dropout(type_as_11, p = 0.1, training = False, inplace = False);  type_as_11 = None
    matmul_23 = torch.matmul(dropout_11, transpose_57);  dropout_11 = transpose_57 = None
    transpose_59 = matmul_23.transpose(1, 2);  matmul_23 = None
    contiguous_11 = transpose_59.contiguous();  transpose_59 = None
    view_48 = contiguous_11.view(getitem_40, -1, 768);  contiguous_11 = getitem_40 = None
    encoder_block_11_layer_0_self_attention_o = getattr(getattr(self.encoder.block, "11").layer, "0").SelfAttention.o(view_48);  view_48 = None
    encoder_block_11_layer_0_dropout = getattr(getattr(self.encoder.block, "11").layer, "0").dropout(encoder_block_11_layer_0_self_attention_o);  encoder_block_11_layer_0_self_attention_o = None
    add_83 = add_80 + encoder_block_11_layer_0_dropout;  add_80 = encoder_block_11_layer_0_dropout = None
    getattr_71 = add_83.dtype
    eq_48 = getattr_71 == torch.float16;  getattr_71 = None
    to_49 = add_83.to(torch.float32)
    pow_35 = to_49.pow(2);  to_49 = None
    mean_23 = pow_35.mean(-1, keepdim = True);  pow_35 = None
    add_84 = mean_23 + 1e-06;  mean_23 = None
    rsqrt_23 = torch.rsqrt(add_84);  add_84 = None
    mul_104 = add_83 * rsqrt_23;  rsqrt_23 = None
    encoder_block_11_layer_1_layer_norm_weight = getattr(getattr(self.encoder.block, "11").layer, "1").layer_norm.weight
    getattr_72 = encoder_block_11_layer_1_layer_norm_weight.dtype
    eq_49 = getattr_72 == torch.float16;  getattr_72 = None
    getattr_73 = encoder_block_11_layer_1_layer_norm_weight.dtype
    to_50 = mul_104.to(getattr_73);  mul_104 = getattr_73 = None
    mul_105 = encoder_block_11_layer_1_layer_norm_weight * to_50;  encoder_block_11_layer_1_layer_norm_weight = to_50 = None
    encoder_block_11_layer_1_dense_relu_dense_wi_0 = getattr(getattr(self.encoder.block, "11").layer, "1").DenseReluDense.wi_0(mul_105)
    mul_106 = 0.5 * encoder_block_11_layer_1_dense_relu_dense_wi_0
    pow_36 = torch.pow(encoder_block_11_layer_1_dense_relu_dense_wi_0, 3.0)
    mul_107 = 0.044715 * pow_36;  pow_36 = None
    add_85 = encoder_block_11_layer_1_dense_relu_dense_wi_0 + mul_107;  encoder_block_11_layer_1_dense_relu_dense_wi_0 = mul_107 = None
    mul_108 = 0.7978845608028654 * add_85;  add_85 = None
    tanh_11 = torch.tanh(mul_108);  mul_108 = None
    add_86 = 1.0 + tanh_11;  tanh_11 = None
    mul_109 = mul_106 * add_86;  mul_106 = add_86 = None
    encoder_block_11_layer_1_dense_relu_dense_wi_1 = getattr(getattr(self.encoder.block, "11").layer, "1").DenseReluDense.wi_1(mul_105);  mul_105 = None
    mul_110 = mul_109 * encoder_block_11_layer_1_dense_relu_dense_wi_1;  mul_109 = encoder_block_11_layer_1_dense_relu_dense_wi_1 = None
    encoder_block_11_layer_1_dense_relu_dense_dropout = getattr(getattr(self.encoder.block, "11").layer, "1").DenseReluDense.dropout(mul_110);  mul_110 = None
    encoder_block_11_layer_1_dense_relu_dense_wo_weight = getattr(getattr(self.encoder.block, "11").layer, "1").DenseReluDense.wo.weight
    encoder_block_11_layer_1_dense_relu_dense_wo = getattr(getattr(self.encoder.block, "11").layer, "1").DenseReluDense.wo(encoder_block_11_layer_1_dense_relu_dense_dropout);  encoder_block_11_layer_1_dense_relu_dense_dropout = None
    encoder_block_11_layer_1_dropout = getattr(getattr(self.encoder.block, "11").layer, "1").dropout(encoder_block_11_layer_1_dense_relu_dense_wo);  encoder_block_11_layer_1_dense_relu_dense_wo = None
    add_87 = add_83 + encoder_block_11_layer_1_dropout;  add_83 = encoder_block_11_layer_1_dropout = None
    getattr_74 = add_87.dtype
    eq_50 = getattr_74 == torch.float16;  getattr_74 = None
    to_51 = add_87.to(torch.float32)
    pow_37 = to_51.pow(2);  to_51 = None
    mean_24 = pow_37.mean(-1, keepdim = True);  pow_37 = None
    add_88 = mean_24 + 1e-06;  mean_24 = None
    rsqrt_24 = torch.rsqrt(add_88);  add_88 = None
    mul_111 = add_87 * rsqrt_24;  add_87 = rsqrt_24 = None
    encoder_final_layer_norm_weight = self.encoder.final_layer_norm.weight
    getattr_75 = encoder_final_layer_norm_weight.dtype
    eq_51 = getattr_75 == torch.float16;  getattr_75 = None
    getattr_76 = encoder_final_layer_norm_weight.dtype
    to_52 = mul_111.to(getattr_76);  mul_111 = getattr_76 = None
    mul_112 = encoder_final_layer_norm_weight * to_52;  encoder_final_layer_norm_weight = to_52 = None
    encoder_dropout_1 = self.encoder.dropout(mul_112);  mul_112 = None
    size_13 = labels.size()
    getitem_42 = size_13[slice(None, -1, None)];  size_13 = None
    add_89 = getitem_42 + (1,);  getitem_42 = None
    full = torch.full(add_89, 0);  add_89 = None
    getitem_43 = labels[(Ellipsis, slice(None, -1, None))]
    cat = torch.cat([full, getitem_43], dim = -1);  full = getitem_43 = None
    eq_52 = cat == -100
    masked_fill_ = cat.masked_fill_(eq_52, 0);  eq_52 = None
    size_14 = cat.size()
    getitem_44 = size_14[-1]
    view_49 = cat.view(-1, getitem_44);  cat = getitem_44 = None
    shared_1 = self.shared(view_49);  view_49 = None
    getitem_45 = size_14[0]
    getitem_46 = size_14[1]
    getattr_77 = shared_1.device
    ones_1 = torch.ones(getitem_45, getitem_46, device = getattr_77);  getitem_46 = getattr_77 = None
    size_15 = encoder_dropout_1.size()
    getitem_47 = size_15[1];  size_15 = None
    getattr_78 = shared_1.device
    ones_2 = torch.ones(getitem_45, getitem_47, device = getattr_78, dtype = torch.int64);  getitem_45 = getitem_47 = getattr_78 = None
    dim_3 = ones_1.dim()
    eq_53 = dim_3 == 2;  dim_3 = None
    dim_4 = ones_1.dim()
    eq_54 = dim_4 == 3;  dim_4 = None
    dim_5 = ones_1.dim()
    eq_55 = dim_5 == 2;  dim_5 = None
    getitem_48 = size_14[0]
    getitem_49 = size_14[1];  size_14 = None
    getattr_79 = ones_1.device
    arange_2 = torch.arange(getitem_49, device = getattr_79);  getattr_79 = None
    getitem_50 = arange_2[(None, None, slice(None, None, None))]
    repeat = getitem_50.repeat(getitem_48, getitem_49, 1);  getitem_50 = getitem_48 = getitem_49 = None
    getitem_51 = arange_2[(None, slice(None, None, None), None)];  arange_2 = None
    le = repeat <= getitem_51;  repeat = getitem_51 = None
    getattr_80 = ones_1.dtype
    to_53 = le.to(getattr_80);  le = getattr_80 = None
    size_16 = to_53.size()
    getitem_52 = size_16[1];  size_16 = None
    size_17 = ones_1.size()
    getitem_53 = size_17[1];  size_17 = None
    lt_1 = getitem_52 < getitem_53;  getitem_52 = getitem_53 = None
    getitem_54 = to_53[(slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  to_53 = None
    getitem_55 = ones_1[(slice(None, None, None), None, None, slice(None, None, None))];  ones_1 = None
    mul_113 = getitem_54 * getitem_55;  getitem_54 = getitem_55 = None
    to_54 = mul_113.to(dtype = torch.float16);  mul_113 = None
    sub_2 = 1.0 - to_54;  to_54 = None
    mul_114 = sub_2 * -65504.0;  sub_2 = None
    size_18 = encoder_dropout_1.size()
    getitem_56 = size_18[0]
    getitem_57 = size_18[1]
    getitem_58 = size_18[2];  size_18 = None
    dim_6 = ones_2.dim()
    eq_56 = dim_6 == 3;  dim_6 = None
    dim_7 = ones_2.dim()
    eq_57 = dim_7 == 2;  dim_7 = None
    getitem_59 = ones_2[(slice(None, None, None), None, None, slice(None, None, None))];  ones_2 = None
    to_55 = getitem_59.to(dtype = torch.float16);  getitem_59 = None
    sub_3 = 1.0 - to_55;  to_55 = None
    mul_115 = sub_3 * -65504.0;  sub_3 = None
    decoder_dropout = self.decoder.dropout(shared_1);  shared_1 = None
    to_56 = decoder_dropout.to(torch.float32)
    pow_38 = to_56.pow(2);  to_56 = None
    mean_25 = pow_38.mean(-1, keepdim = True);  pow_38 = None
    add_90 = mean_25 + 1e-06;  mean_25 = None
    rsqrt_25 = torch.rsqrt(add_90);  add_90 = None
    mul_116 = decoder_dropout * rsqrt_25;  rsqrt_25 = None
    decoder_block_0_layer_0_layer_norm_weight = getattr(getattr(self.decoder.block, "0").layer, "0").layer_norm.weight
    getattr_81 = decoder_block_0_layer_0_layer_norm_weight.dtype
    eq_58 = getattr_81 == torch.float16;  getattr_81 = None
    getattr_82 = decoder_block_0_layer_0_layer_norm_weight.dtype
    to_57 = mul_116.to(getattr_82);  mul_116 = getattr_82 = None
    mul_117 = decoder_block_0_layer_0_layer_norm_weight * to_57;  decoder_block_0_layer_0_layer_norm_weight = to_57 = None
    size_19 = mul_117.size()
    getitem_60 = size_19[slice(None, 2, None)];  size_19 = None
    getitem_61 = getitem_60[0]
    getitem_62 = getitem_60[1];  getitem_60 = None
    decoder_block_0_layer_0_self_attention_q = getattr(getattr(self.decoder.block, "0").layer, "0").SelfAttention.q(mul_117)
    view_50 = decoder_block_0_layer_0_self_attention_q.view(getitem_61, -1, 12, 64);  decoder_block_0_layer_0_self_attention_q = None
    transpose_60 = view_50.transpose(1, 2);  view_50 = None
    decoder_block_0_layer_0_self_attention_k = getattr(getattr(self.decoder.block, "0").layer, "0").SelfAttention.k(mul_117)
    view_51 = decoder_block_0_layer_0_self_attention_k.view(getitem_61, -1, 12, 64);  decoder_block_0_layer_0_self_attention_k = None
    transpose_61 = view_51.transpose(1, 2);  view_51 = None
    decoder_block_0_layer_0_self_attention_v = getattr(getattr(self.decoder.block, "0").layer, "0").SelfAttention.v(mul_117);  mul_117 = None
    view_52 = decoder_block_0_layer_0_self_attention_v.view(getitem_61, -1, 12, 64);  decoder_block_0_layer_0_self_attention_v = None
    transpose_62 = view_52.transpose(1, 2);  view_52 = None
    transpose_63 = transpose_61.transpose(3, 2)
    matmul_24 = torch.matmul(transpose_60, transpose_63);  transpose_60 = transpose_63 = None
    getattr_83 = matmul_24.device
    arange_3 = torch.arange(getitem_62, dtype = torch.int64, device = getattr_83)
    getitem_63 = arange_3[(slice(None, None, None), None)];  arange_3 = None
    arange_4 = torch.arange(getitem_62, dtype = torch.int64, device = getattr_83);  getitem_62 = getattr_83 = None
    getitem_64 = arange_4[(None, slice(None, None, None))];  arange_4 = None
    sub_4 = getitem_64 - getitem_63;  getitem_64 = getitem_63 = None
    zeros_like = torch.zeros_like(sub_4)
    min_2 = torch.min(sub_4, zeros_like);  sub_4 = zeros_like = None
    neg = -min_2;  min_2 = None
    lt_2 = neg < 16
    float_14 = neg.float()
    truediv_2 = float_14 / 16;  float_14 = None
    log_1 = torch.log(truediv_2);  truediv_2 = None
    truediv_3 = log_1 / 2.0794415416798357;  log_1 = None
    mul_118 = truediv_3 * 16;  truediv_3 = None
    to_58 = mul_118.to(torch.int64);  mul_118 = None
    add_91 = 16 + to_58;  to_58 = None
    full_like_1 = torch.full_like(add_91, 31)
    min_3 = torch.min(add_91, full_like_1);  add_91 = full_like_1 = None
    where_1 = torch.where(lt_2, neg, min_3);  lt_2 = neg = min_3 = None
    add_92 = 0 + where_1;  where_1 = None
    decoder_block_0_layer_0_self_attention_relative_attention_bias = getattr(getattr(self.decoder.block, "0").layer, "0").SelfAttention.relative_attention_bias(add_92);  add_92 = None
    permute_1 = decoder_block_0_layer_0_self_attention_relative_attention_bias.permute([2, 0, 1]);  decoder_block_0_layer_0_self_attention_relative_attention_bias = None
    unsqueeze_1 = permute_1.unsqueeze(0);  permute_1 = None
    add_93 = unsqueeze_1 + mul_114;  unsqueeze_1 = mul_114 = None
    add_94 = matmul_24 + add_93;  matmul_24 = None
    float_15 = add_94.float()
    softmax_12 = torch.nn.functional.softmax(float_15, dim = -1, _stacklevel = 3, dtype = None);  float_15 = None
    type_as_12 = softmax_12.type_as(add_94);  softmax_12 = add_94 = None
    dropout_12 = torch.nn.functional.dropout(type_as_12, p = 0.1, training = False, inplace = False);  type_as_12 = None
    matmul_25 = torch.matmul(dropout_12, transpose_62);  dropout_12 = None
    transpose_64 = matmul_25.transpose(1, 2);  matmul_25 = None
    contiguous_12 = transpose_64.contiguous();  transpose_64 = None
    view_53 = contiguous_12.view(getitem_61, -1, 768);  contiguous_12 = getitem_61 = None
    decoder_block_0_layer_0_self_attention_o = getattr(getattr(self.decoder.block, "0").layer, "0").SelfAttention.o(view_53);  view_53 = None
    decoder_block_0_layer_0_dropout = getattr(getattr(self.decoder.block, "0").layer, "0").dropout(decoder_block_0_layer_0_self_attention_o);  decoder_block_0_layer_0_self_attention_o = None
    add_95 = decoder_dropout + decoder_block_0_layer_0_dropout;  decoder_dropout = decoder_block_0_layer_0_dropout = None
    getattr_84 = add_95.dtype
    eq_59 = getattr_84 == torch.float16;  getattr_84 = None
    size_20 = transpose_61.size()
    getitem_65 = size_20[2];  size_20 = None
    to_59 = add_95.to(torch.float32)
    pow_39 = to_59.pow(2);  to_59 = None
    mean_26 = pow_39.mean(-1, keepdim = True);  pow_39 = None
    add_96 = mean_26 + 1e-06;  mean_26 = None
    rsqrt_26 = torch.rsqrt(add_96);  add_96 = None
    mul_119 = add_95 * rsqrt_26;  rsqrt_26 = None
    decoder_block_0_layer_1_layer_norm_weight = getattr(getattr(self.decoder.block, "0").layer, "1").layer_norm.weight
    getattr_85 = decoder_block_0_layer_1_layer_norm_weight.dtype
    eq_60 = getattr_85 == torch.float16;  getattr_85 = None
    getattr_86 = decoder_block_0_layer_1_layer_norm_weight.dtype
    to_60 = mul_119.to(getattr_86);  mul_119 = getattr_86 = None
    mul_120 = decoder_block_0_layer_1_layer_norm_weight * to_60;  decoder_block_0_layer_1_layer_norm_weight = to_60 = None
    size_21 = mul_120.size()
    getitem_66 = size_21[slice(None, 2, None)];  size_21 = None
    getitem_67 = getitem_66[0]
    getitem_68 = getitem_66[1];  getitem_66 = None
    size_22 = encoder_dropout_1.size()
    getitem_69 = size_22[1];  size_22 = None
    decoder_block_0_layer_1_enc_dec_attention_q = getattr(getattr(self.decoder.block, "0").layer, "1").EncDecAttention.q(mul_120);  mul_120 = None
    view_54 = decoder_block_0_layer_1_enc_dec_attention_q.view(getitem_67, -1, 12, 64);  decoder_block_0_layer_1_enc_dec_attention_q = None
    transpose_65 = view_54.transpose(1, 2);  view_54 = None
    decoder_block_0_layer_1_enc_dec_attention_k = getattr(getattr(self.decoder.block, "0").layer, "1").EncDecAttention.k(encoder_dropout_1)
    view_55 = decoder_block_0_layer_1_enc_dec_attention_k.view(getitem_67, -1, 12, 64);  decoder_block_0_layer_1_enc_dec_attention_k = None
    transpose_66 = view_55.transpose(1, 2);  view_55 = None
    decoder_block_0_layer_1_enc_dec_attention_v = getattr(getattr(self.decoder.block, "0").layer, "1").EncDecAttention.v(encoder_dropout_1)
    view_56 = decoder_block_0_layer_1_enc_dec_attention_v.view(getitem_67, -1, 12, 64);  decoder_block_0_layer_1_enc_dec_attention_v = None
    transpose_67 = view_56.transpose(1, 2);  view_56 = None
    transpose_68 = transpose_66.transpose(3, 2)
    matmul_26 = torch.matmul(transpose_65, transpose_68);  transpose_65 = transpose_68 = None
    getattr_87 = matmul_26.device
    getattr_88 = matmul_26.dtype
    zeros = torch.zeros((1, 12, getitem_68, getitem_69), device = getattr_87, dtype = getattr_88);  getitem_68 = getitem_69 = getattr_87 = getattr_88 = None
    add_97 = zeros + mul_115;  zeros = mul_115 = None
    add_98 = matmul_26 + add_97;  matmul_26 = None
    float_16 = add_98.float()
    softmax_13 = torch.nn.functional.softmax(float_16, dim = -1, _stacklevel = 3, dtype = None);  float_16 = None
    type_as_13 = softmax_13.type_as(add_98);  softmax_13 = add_98 = None
    dropout_13 = torch.nn.functional.dropout(type_as_13, p = 0.1, training = False, inplace = False);  type_as_13 = None
    matmul_27 = torch.matmul(dropout_13, transpose_67);  dropout_13 = None
    transpose_69 = matmul_27.transpose(1, 2);  matmul_27 = None
    contiguous_13 = transpose_69.contiguous();  transpose_69 = None
    view_57 = contiguous_13.view(getitem_67, -1, 768);  contiguous_13 = getitem_67 = None
    decoder_block_0_layer_1_enc_dec_attention_o = getattr(getattr(self.decoder.block, "0").layer, "1").EncDecAttention.o(view_57);  view_57 = None
    decoder_block_0_layer_1_dropout = getattr(getattr(self.decoder.block, "0").layer, "1").dropout(decoder_block_0_layer_1_enc_dec_attention_o);  decoder_block_0_layer_1_enc_dec_attention_o = None
    add_99 = add_95 + decoder_block_0_layer_1_dropout;  add_95 = decoder_block_0_layer_1_dropout = None
    getattr_89 = add_99.dtype
    eq_61 = getattr_89 == torch.float16;  getattr_89 = None
    to_61 = add_99.to(torch.float32)
    pow_40 = to_61.pow(2);  to_61 = None
    mean_27 = pow_40.mean(-1, keepdim = True);  pow_40 = None
    add_100 = mean_27 + 1e-06;  mean_27 = None
    rsqrt_27 = torch.rsqrt(add_100);  add_100 = None
    mul_121 = add_99 * rsqrt_27;  rsqrt_27 = None
    decoder_block_0_layer_2_layer_norm_weight = getattr(getattr(self.decoder.block, "0").layer, "2").layer_norm.weight
    getattr_90 = decoder_block_0_layer_2_layer_norm_weight.dtype
    eq_62 = getattr_90 == torch.float16;  getattr_90 = None
    getattr_91 = decoder_block_0_layer_2_layer_norm_weight.dtype
    to_62 = mul_121.to(getattr_91);  mul_121 = getattr_91 = None
    mul_122 = decoder_block_0_layer_2_layer_norm_weight * to_62;  decoder_block_0_layer_2_layer_norm_weight = to_62 = None
    decoder_block_0_layer_2_dense_relu_dense_wi_0 = getattr(getattr(self.decoder.block, "0").layer, "2").DenseReluDense.wi_0(mul_122)
    mul_123 = 0.5 * decoder_block_0_layer_2_dense_relu_dense_wi_0
    pow_41 = torch.pow(decoder_block_0_layer_2_dense_relu_dense_wi_0, 3.0)
    mul_124 = 0.044715 * pow_41;  pow_41 = None
    add_101 = decoder_block_0_layer_2_dense_relu_dense_wi_0 + mul_124;  decoder_block_0_layer_2_dense_relu_dense_wi_0 = mul_124 = None
    mul_125 = 0.7978845608028654 * add_101;  add_101 = None
    tanh_12 = torch.tanh(mul_125);  mul_125 = None
    add_102 = 1.0 + tanh_12;  tanh_12 = None
    mul_126 = mul_123 * add_102;  mul_123 = add_102 = None
    decoder_block_0_layer_2_dense_relu_dense_wi_1 = getattr(getattr(self.decoder.block, "0").layer, "2").DenseReluDense.wi_1(mul_122);  mul_122 = None
    mul_127 = mul_126 * decoder_block_0_layer_2_dense_relu_dense_wi_1;  mul_126 = decoder_block_0_layer_2_dense_relu_dense_wi_1 = None
    decoder_block_0_layer_2_dense_relu_dense_dropout = getattr(getattr(self.decoder.block, "0").layer, "2").DenseReluDense.dropout(mul_127);  mul_127 = None
    decoder_block_0_layer_2_dense_relu_dense_wo_weight = getattr(getattr(self.decoder.block, "0").layer, "2").DenseReluDense.wo.weight
    decoder_block_0_layer_2_dense_relu_dense_wo = getattr(getattr(self.decoder.block, "0").layer, "2").DenseReluDense.wo(decoder_block_0_layer_2_dense_relu_dense_dropout);  decoder_block_0_layer_2_dense_relu_dense_dropout = None
    decoder_block_0_layer_2_dropout = getattr(getattr(self.decoder.block, "0").layer, "2").dropout(decoder_block_0_layer_2_dense_relu_dense_wo);  decoder_block_0_layer_2_dense_relu_dense_wo = None
    add_103 = add_99 + decoder_block_0_layer_2_dropout;  add_99 = decoder_block_0_layer_2_dropout = None
    getattr_92 = add_103.dtype
    eq_63 = getattr_92 == torch.float16;  getattr_92 = None
    to_63 = add_103.to(torch.float32)
    pow_42 = to_63.pow(2);  to_63 = None
    mean_28 = pow_42.mean(-1, keepdim = True);  pow_42 = None
    add_104 = mean_28 + 1e-06;  mean_28 = None
    rsqrt_28 = torch.rsqrt(add_104);  add_104 = None
    mul_128 = add_103 * rsqrt_28;  rsqrt_28 = None
    decoder_block_1_layer_0_layer_norm_weight = getattr(getattr(self.decoder.block, "1").layer, "0").layer_norm.weight
    getattr_93 = decoder_block_1_layer_0_layer_norm_weight.dtype
    eq_64 = getattr_93 == torch.float16;  getattr_93 = None
    getattr_94 = decoder_block_1_layer_0_layer_norm_weight.dtype
    to_64 = mul_128.to(getattr_94);  mul_128 = getattr_94 = None
    mul_129 = decoder_block_1_layer_0_layer_norm_weight * to_64;  decoder_block_1_layer_0_layer_norm_weight = to_64 = None
    size_23 = mul_129.size()
    getitem_70 = size_23[slice(None, 2, None)];  size_23 = None
    getitem_71 = getitem_70[0]
    getitem_72 = getitem_70[1];  getitem_70 = None
    decoder_block_1_layer_0_self_attention_q = getattr(getattr(self.decoder.block, "1").layer, "0").SelfAttention.q(mul_129)
    view_58 = decoder_block_1_layer_0_self_attention_q.view(getitem_71, -1, 12, 64);  decoder_block_1_layer_0_self_attention_q = None
    transpose_70 = view_58.transpose(1, 2);  view_58 = None
    decoder_block_1_layer_0_self_attention_k = getattr(getattr(self.decoder.block, "1").layer, "0").SelfAttention.k(mul_129)
    view_59 = decoder_block_1_layer_0_self_attention_k.view(getitem_71, -1, 12, 64);  decoder_block_1_layer_0_self_attention_k = None
    transpose_71 = view_59.transpose(1, 2);  view_59 = None
    decoder_block_1_layer_0_self_attention_v = getattr(getattr(self.decoder.block, "1").layer, "0").SelfAttention.v(mul_129);  mul_129 = None
    view_60 = decoder_block_1_layer_0_self_attention_v.view(getitem_71, -1, 12, 64);  decoder_block_1_layer_0_self_attention_v = None
    transpose_72 = view_60.transpose(1, 2);  view_60 = None
    transpose_73 = transpose_71.transpose(3, 2)
    matmul_28 = torch.matmul(transpose_70, transpose_73);  transpose_70 = transpose_73 = None
    add_105 = matmul_28 + add_93;  matmul_28 = None
    float_17 = add_105.float()
    softmax_14 = torch.nn.functional.softmax(float_17, dim = -1, _stacklevel = 3, dtype = None);  float_17 = None
    type_as_14 = softmax_14.type_as(add_105);  softmax_14 = add_105 = None
    dropout_14 = torch.nn.functional.dropout(type_as_14, p = 0.1, training = False, inplace = False);  type_as_14 = None
    matmul_29 = torch.matmul(dropout_14, transpose_72);  dropout_14 = None
    transpose_74 = matmul_29.transpose(1, 2);  matmul_29 = None
    contiguous_14 = transpose_74.contiguous();  transpose_74 = None
    view_61 = contiguous_14.view(getitem_71, -1, 768);  contiguous_14 = getitem_71 = None
    decoder_block_1_layer_0_self_attention_o = getattr(getattr(self.decoder.block, "1").layer, "0").SelfAttention.o(view_61);  view_61 = None
    decoder_block_1_layer_0_dropout = getattr(getattr(self.decoder.block, "1").layer, "0").dropout(decoder_block_1_layer_0_self_attention_o);  decoder_block_1_layer_0_self_attention_o = None
    add_106 = add_103 + decoder_block_1_layer_0_dropout;  add_103 = decoder_block_1_layer_0_dropout = None
    getattr_95 = add_106.dtype
    eq_65 = getattr_95 == torch.float16;  getattr_95 = None
    size_24 = transpose_71.size()
    getitem_73 = size_24[2];  size_24 = None
    to_65 = add_106.to(torch.float32)
    pow_43 = to_65.pow(2);  to_65 = None
    mean_29 = pow_43.mean(-1, keepdim = True);  pow_43 = None
    add_107 = mean_29 + 1e-06;  mean_29 = None
    rsqrt_29 = torch.rsqrt(add_107);  add_107 = None
    mul_130 = add_106 * rsqrt_29;  rsqrt_29 = None
    decoder_block_1_layer_1_layer_norm_weight = getattr(getattr(self.decoder.block, "1").layer, "1").layer_norm.weight
    getattr_96 = decoder_block_1_layer_1_layer_norm_weight.dtype
    eq_66 = getattr_96 == torch.float16;  getattr_96 = None
    getattr_97 = decoder_block_1_layer_1_layer_norm_weight.dtype
    to_66 = mul_130.to(getattr_97);  mul_130 = getattr_97 = None
    mul_131 = decoder_block_1_layer_1_layer_norm_weight * to_66;  decoder_block_1_layer_1_layer_norm_weight = to_66 = None
    size_25 = mul_131.size()
    getitem_74 = size_25[slice(None, 2, None)];  size_25 = None
    getitem_75 = getitem_74[0]
    getitem_76 = getitem_74[1];  getitem_74 = None
    size_26 = encoder_dropout_1.size()
    getitem_77 = size_26[1];  size_26 = None
    decoder_block_1_layer_1_enc_dec_attention_q = getattr(getattr(self.decoder.block, "1").layer, "1").EncDecAttention.q(mul_131);  mul_131 = None
    view_62 = decoder_block_1_layer_1_enc_dec_attention_q.view(getitem_75, -1, 12, 64);  decoder_block_1_layer_1_enc_dec_attention_q = None
    transpose_75 = view_62.transpose(1, 2);  view_62 = None
    decoder_block_1_layer_1_enc_dec_attention_k = getattr(getattr(self.decoder.block, "1").layer, "1").EncDecAttention.k(encoder_dropout_1)
    view_63 = decoder_block_1_layer_1_enc_dec_attention_k.view(getitem_75, -1, 12, 64);  decoder_block_1_layer_1_enc_dec_attention_k = None
    transpose_76 = view_63.transpose(1, 2);  view_63 = None
    decoder_block_1_layer_1_enc_dec_attention_v = getattr(getattr(self.decoder.block, "1").layer, "1").EncDecAttention.v(encoder_dropout_1)
    view_64 = decoder_block_1_layer_1_enc_dec_attention_v.view(getitem_75, -1, 12, 64);  decoder_block_1_layer_1_enc_dec_attention_v = None
    transpose_77 = view_64.transpose(1, 2);  view_64 = None
    transpose_78 = transpose_76.transpose(3, 2)
    matmul_30 = torch.matmul(transpose_75, transpose_78);  transpose_75 = transpose_78 = None
    add_108 = matmul_30 + add_97;  matmul_30 = None
    float_18 = add_108.float()
    softmax_15 = torch.nn.functional.softmax(float_18, dim = -1, _stacklevel = 3, dtype = None);  float_18 = None
    type_as_15 = softmax_15.type_as(add_108);  softmax_15 = add_108 = None
    dropout_15 = torch.nn.functional.dropout(type_as_15, p = 0.1, training = False, inplace = False);  type_as_15 = None
    matmul_31 = torch.matmul(dropout_15, transpose_77);  dropout_15 = None
    transpose_79 = matmul_31.transpose(1, 2);  matmul_31 = None
    contiguous_15 = transpose_79.contiguous();  transpose_79 = None
    view_65 = contiguous_15.view(getitem_75, -1, 768);  contiguous_15 = getitem_75 = None
    decoder_block_1_layer_1_enc_dec_attention_o = getattr(getattr(self.decoder.block, "1").layer, "1").EncDecAttention.o(view_65);  view_65 = None
    decoder_block_1_layer_1_dropout = getattr(getattr(self.decoder.block, "1").layer, "1").dropout(decoder_block_1_layer_1_enc_dec_attention_o);  decoder_block_1_layer_1_enc_dec_attention_o = None
    add_109 = add_106 + decoder_block_1_layer_1_dropout;  add_106 = decoder_block_1_layer_1_dropout = None
    getattr_98 = add_109.dtype
    eq_67 = getattr_98 == torch.float16;  getattr_98 = None
    to_67 = add_109.to(torch.float32)
    pow_44 = to_67.pow(2);  to_67 = None
    mean_30 = pow_44.mean(-1, keepdim = True);  pow_44 = None
    add_110 = mean_30 + 1e-06;  mean_30 = None
    rsqrt_30 = torch.rsqrt(add_110);  add_110 = None
    mul_132 = add_109 * rsqrt_30;  rsqrt_30 = None
    decoder_block_1_layer_2_layer_norm_weight = getattr(getattr(self.decoder.block, "1").layer, "2").layer_norm.weight
    getattr_99 = decoder_block_1_layer_2_layer_norm_weight.dtype
    eq_68 = getattr_99 == torch.float16;  getattr_99 = None
    getattr_100 = decoder_block_1_layer_2_layer_norm_weight.dtype
    to_68 = mul_132.to(getattr_100);  mul_132 = getattr_100 = None
    mul_133 = decoder_block_1_layer_2_layer_norm_weight * to_68;  decoder_block_1_layer_2_layer_norm_weight = to_68 = None
    decoder_block_1_layer_2_dense_relu_dense_wi_0 = getattr(getattr(self.decoder.block, "1").layer, "2").DenseReluDense.wi_0(mul_133)
    mul_134 = 0.5 * decoder_block_1_layer_2_dense_relu_dense_wi_0
    pow_45 = torch.pow(decoder_block_1_layer_2_dense_relu_dense_wi_0, 3.0)
    mul_135 = 0.044715 * pow_45;  pow_45 = None
    add_111 = decoder_block_1_layer_2_dense_relu_dense_wi_0 + mul_135;  decoder_block_1_layer_2_dense_relu_dense_wi_0 = mul_135 = None
    mul_136 = 0.7978845608028654 * add_111;  add_111 = None
    tanh_13 = torch.tanh(mul_136);  mul_136 = None
    add_112 = 1.0 + tanh_13;  tanh_13 = None
    mul_137 = mul_134 * add_112;  mul_134 = add_112 = None
    decoder_block_1_layer_2_dense_relu_dense_wi_1 = getattr(getattr(self.decoder.block, "1").layer, "2").DenseReluDense.wi_1(mul_133);  mul_133 = None
    mul_138 = mul_137 * decoder_block_1_layer_2_dense_relu_dense_wi_1;  mul_137 = decoder_block_1_layer_2_dense_relu_dense_wi_1 = None
    decoder_block_1_layer_2_dense_relu_dense_dropout = getattr(getattr(self.decoder.block, "1").layer, "2").DenseReluDense.dropout(mul_138);  mul_138 = None
    decoder_block_1_layer_2_dense_relu_dense_wo_weight = getattr(getattr(self.decoder.block, "1").layer, "2").DenseReluDense.wo.weight
    decoder_block_1_layer_2_dense_relu_dense_wo = getattr(getattr(self.decoder.block, "1").layer, "2").DenseReluDense.wo(decoder_block_1_layer_2_dense_relu_dense_dropout);  decoder_block_1_layer_2_dense_relu_dense_dropout = None
    decoder_block_1_layer_2_dropout = getattr(getattr(self.decoder.block, "1").layer, "2").dropout(decoder_block_1_layer_2_dense_relu_dense_wo);  decoder_block_1_layer_2_dense_relu_dense_wo = None
    add_113 = add_109 + decoder_block_1_layer_2_dropout;  add_109 = decoder_block_1_layer_2_dropout = None
    getattr_101 = add_113.dtype
    eq_69 = getattr_101 == torch.float16;  getattr_101 = None
    to_69 = add_113.to(torch.float32)
    pow_46 = to_69.pow(2);  to_69 = None
    mean_31 = pow_46.mean(-1, keepdim = True);  pow_46 = None
    add_114 = mean_31 + 1e-06;  mean_31 = None
    rsqrt_31 = torch.rsqrt(add_114);  add_114 = None
    mul_139 = add_113 * rsqrt_31;  rsqrt_31 = None
    decoder_block_2_layer_0_layer_norm_weight = getattr(getattr(self.decoder.block, "2").layer, "0").layer_norm.weight
    getattr_102 = decoder_block_2_layer_0_layer_norm_weight.dtype
    eq_70 = getattr_102 == torch.float16;  getattr_102 = None
    getattr_103 = decoder_block_2_layer_0_layer_norm_weight.dtype
    to_70 = mul_139.to(getattr_103);  mul_139 = getattr_103 = None
    mul_140 = decoder_block_2_layer_0_layer_norm_weight * to_70;  decoder_block_2_layer_0_layer_norm_weight = to_70 = None
    size_27 = mul_140.size()
    getitem_78 = size_27[slice(None, 2, None)];  size_27 = None
    getitem_79 = getitem_78[0]
    getitem_80 = getitem_78[1];  getitem_78 = None
    decoder_block_2_layer_0_self_attention_q = getattr(getattr(self.decoder.block, "2").layer, "0").SelfAttention.q(mul_140)
    view_66 = decoder_block_2_layer_0_self_attention_q.view(getitem_79, -1, 12, 64);  decoder_block_2_layer_0_self_attention_q = None
    transpose_80 = view_66.transpose(1, 2);  view_66 = None
    decoder_block_2_layer_0_self_attention_k = getattr(getattr(self.decoder.block, "2").layer, "0").SelfAttention.k(mul_140)
    view_67 = decoder_block_2_layer_0_self_attention_k.view(getitem_79, -1, 12, 64);  decoder_block_2_layer_0_self_attention_k = None
    transpose_81 = view_67.transpose(1, 2);  view_67 = None
    decoder_block_2_layer_0_self_attention_v = getattr(getattr(self.decoder.block, "2").layer, "0").SelfAttention.v(mul_140);  mul_140 = None
    view_68 = decoder_block_2_layer_0_self_attention_v.view(getitem_79, -1, 12, 64);  decoder_block_2_layer_0_self_attention_v = None
    transpose_82 = view_68.transpose(1, 2);  view_68 = None
    transpose_83 = transpose_81.transpose(3, 2)
    matmul_32 = torch.matmul(transpose_80, transpose_83);  transpose_80 = transpose_83 = None
    add_115 = matmul_32 + add_93;  matmul_32 = None
    float_19 = add_115.float()
    softmax_16 = torch.nn.functional.softmax(float_19, dim = -1, _stacklevel = 3, dtype = None);  float_19 = None
    type_as_16 = softmax_16.type_as(add_115);  softmax_16 = add_115 = None
    dropout_16 = torch.nn.functional.dropout(type_as_16, p = 0.1, training = False, inplace = False);  type_as_16 = None
    matmul_33 = torch.matmul(dropout_16, transpose_82);  dropout_16 = None
    transpose_84 = matmul_33.transpose(1, 2);  matmul_33 = None
    contiguous_16 = transpose_84.contiguous();  transpose_84 = None
    view_69 = contiguous_16.view(getitem_79, -1, 768);  contiguous_16 = getitem_79 = None
    decoder_block_2_layer_0_self_attention_o = getattr(getattr(self.decoder.block, "2").layer, "0").SelfAttention.o(view_69);  view_69 = None
    decoder_block_2_layer_0_dropout = getattr(getattr(self.decoder.block, "2").layer, "0").dropout(decoder_block_2_layer_0_self_attention_o);  decoder_block_2_layer_0_self_attention_o = None
    add_116 = add_113 + decoder_block_2_layer_0_dropout;  add_113 = decoder_block_2_layer_0_dropout = None
    getattr_104 = add_116.dtype
    eq_71 = getattr_104 == torch.float16;  getattr_104 = None
    size_28 = transpose_81.size()
    getitem_81 = size_28[2];  size_28 = None
    to_71 = add_116.to(torch.float32)
    pow_47 = to_71.pow(2);  to_71 = None
    mean_32 = pow_47.mean(-1, keepdim = True);  pow_47 = None
    add_117 = mean_32 + 1e-06;  mean_32 = None
    rsqrt_32 = torch.rsqrt(add_117);  add_117 = None
    mul_141 = add_116 * rsqrt_32;  rsqrt_32 = None
    decoder_block_2_layer_1_layer_norm_weight = getattr(getattr(self.decoder.block, "2").layer, "1").layer_norm.weight
    getattr_105 = decoder_block_2_layer_1_layer_norm_weight.dtype
    eq_72 = getattr_105 == torch.float16;  getattr_105 = None
    getattr_106 = decoder_block_2_layer_1_layer_norm_weight.dtype
    to_72 = mul_141.to(getattr_106);  mul_141 = getattr_106 = None
    mul_142 = decoder_block_2_layer_1_layer_norm_weight * to_72;  decoder_block_2_layer_1_layer_norm_weight = to_72 = None
    size_29 = mul_142.size()
    getitem_82 = size_29[slice(None, 2, None)];  size_29 = None
    getitem_83 = getitem_82[0]
    getitem_84 = getitem_82[1];  getitem_82 = None
    size_30 = encoder_dropout_1.size()
    getitem_85 = size_30[1];  size_30 = None
    decoder_block_2_layer_1_enc_dec_attention_q = getattr(getattr(self.decoder.block, "2").layer, "1").EncDecAttention.q(mul_142);  mul_142 = None
    view_70 = decoder_block_2_layer_1_enc_dec_attention_q.view(getitem_83, -1, 12, 64);  decoder_block_2_layer_1_enc_dec_attention_q = None
    transpose_85 = view_70.transpose(1, 2);  view_70 = None
    decoder_block_2_layer_1_enc_dec_attention_k = getattr(getattr(self.decoder.block, "2").layer, "1").EncDecAttention.k(encoder_dropout_1)
    view_71 = decoder_block_2_layer_1_enc_dec_attention_k.view(getitem_83, -1, 12, 64);  decoder_block_2_layer_1_enc_dec_attention_k = None
    transpose_86 = view_71.transpose(1, 2);  view_71 = None
    decoder_block_2_layer_1_enc_dec_attention_v = getattr(getattr(self.decoder.block, "2").layer, "1").EncDecAttention.v(encoder_dropout_1)
    view_72 = decoder_block_2_layer_1_enc_dec_attention_v.view(getitem_83, -1, 12, 64);  decoder_block_2_layer_1_enc_dec_attention_v = None
    transpose_87 = view_72.transpose(1, 2);  view_72 = None
    transpose_88 = transpose_86.transpose(3, 2)
    matmul_34 = torch.matmul(transpose_85, transpose_88);  transpose_85 = transpose_88 = None
    add_118 = matmul_34 + add_97;  matmul_34 = None
    float_20 = add_118.float()
    softmax_17 = torch.nn.functional.softmax(float_20, dim = -1, _stacklevel = 3, dtype = None);  float_20 = None
    type_as_17 = softmax_17.type_as(add_118);  softmax_17 = add_118 = None
    dropout_17 = torch.nn.functional.dropout(type_as_17, p = 0.1, training = False, inplace = False);  type_as_17 = None
    matmul_35 = torch.matmul(dropout_17, transpose_87);  dropout_17 = None
    transpose_89 = matmul_35.transpose(1, 2);  matmul_35 = None
    contiguous_17 = transpose_89.contiguous();  transpose_89 = None
    view_73 = contiguous_17.view(getitem_83, -1, 768);  contiguous_17 = getitem_83 = None
    decoder_block_2_layer_1_enc_dec_attention_o = getattr(getattr(self.decoder.block, "2").layer, "1").EncDecAttention.o(view_73);  view_73 = None
    decoder_block_2_layer_1_dropout = getattr(getattr(self.decoder.block, "2").layer, "1").dropout(decoder_block_2_layer_1_enc_dec_attention_o);  decoder_block_2_layer_1_enc_dec_attention_o = None
    add_119 = add_116 + decoder_block_2_layer_1_dropout;  add_116 = decoder_block_2_layer_1_dropout = None
    getattr_107 = add_119.dtype
    eq_73 = getattr_107 == torch.float16;  getattr_107 = None
    to_73 = add_119.to(torch.float32)
    pow_48 = to_73.pow(2);  to_73 = None
    mean_33 = pow_48.mean(-1, keepdim = True);  pow_48 = None
    add_120 = mean_33 + 1e-06;  mean_33 = None
    rsqrt_33 = torch.rsqrt(add_120);  add_120 = None
    mul_143 = add_119 * rsqrt_33;  rsqrt_33 = None
    decoder_block_2_layer_2_layer_norm_weight = getattr(getattr(self.decoder.block, "2").layer, "2").layer_norm.weight
    getattr_108 = decoder_block_2_layer_2_layer_norm_weight.dtype
    eq_74 = getattr_108 == torch.float16;  getattr_108 = None
    getattr_109 = decoder_block_2_layer_2_layer_norm_weight.dtype
    to_74 = mul_143.to(getattr_109);  mul_143 = getattr_109 = None
    mul_144 = decoder_block_2_layer_2_layer_norm_weight * to_74;  decoder_block_2_layer_2_layer_norm_weight = to_74 = None
    decoder_block_2_layer_2_dense_relu_dense_wi_0 = getattr(getattr(self.decoder.block, "2").layer, "2").DenseReluDense.wi_0(mul_144)
    mul_145 = 0.5 * decoder_block_2_layer_2_dense_relu_dense_wi_0
    pow_49 = torch.pow(decoder_block_2_layer_2_dense_relu_dense_wi_0, 3.0)
    mul_146 = 0.044715 * pow_49;  pow_49 = None
    add_121 = decoder_block_2_layer_2_dense_relu_dense_wi_0 + mul_146;  decoder_block_2_layer_2_dense_relu_dense_wi_0 = mul_146 = None
    mul_147 = 0.7978845608028654 * add_121;  add_121 = None
    tanh_14 = torch.tanh(mul_147);  mul_147 = None
    add_122 = 1.0 + tanh_14;  tanh_14 = None
    mul_148 = mul_145 * add_122;  mul_145 = add_122 = None
    decoder_block_2_layer_2_dense_relu_dense_wi_1 = getattr(getattr(self.decoder.block, "2").layer, "2").DenseReluDense.wi_1(mul_144);  mul_144 = None
    mul_149 = mul_148 * decoder_block_2_layer_2_dense_relu_dense_wi_1;  mul_148 = decoder_block_2_layer_2_dense_relu_dense_wi_1 = None
    decoder_block_2_layer_2_dense_relu_dense_dropout = getattr(getattr(self.decoder.block, "2").layer, "2").DenseReluDense.dropout(mul_149);  mul_149 = None
    decoder_block_2_layer_2_dense_relu_dense_wo_weight = getattr(getattr(self.decoder.block, "2").layer, "2").DenseReluDense.wo.weight
    decoder_block_2_layer_2_dense_relu_dense_wo = getattr(getattr(self.decoder.block, "2").layer, "2").DenseReluDense.wo(decoder_block_2_layer_2_dense_relu_dense_dropout);  decoder_block_2_layer_2_dense_relu_dense_dropout = None
    decoder_block_2_layer_2_dropout = getattr(getattr(self.decoder.block, "2").layer, "2").dropout(decoder_block_2_layer_2_dense_relu_dense_wo);  decoder_block_2_layer_2_dense_relu_dense_wo = None
    add_123 = add_119 + decoder_block_2_layer_2_dropout;  add_119 = decoder_block_2_layer_2_dropout = None
    getattr_110 = add_123.dtype
    eq_75 = getattr_110 == torch.float16;  getattr_110 = None
    to_75 = add_123.to(torch.float32)
    pow_50 = to_75.pow(2);  to_75 = None
    mean_34 = pow_50.mean(-1, keepdim = True);  pow_50 = None
    add_124 = mean_34 + 1e-06;  mean_34 = None
    rsqrt_34 = torch.rsqrt(add_124);  add_124 = None
    mul_150 = add_123 * rsqrt_34;  rsqrt_34 = None
    decoder_block_3_layer_0_layer_norm_weight = getattr(getattr(self.decoder.block, "3").layer, "0").layer_norm.weight
    getattr_111 = decoder_block_3_layer_0_layer_norm_weight.dtype
    eq_76 = getattr_111 == torch.float16;  getattr_111 = None
    getattr_112 = decoder_block_3_layer_0_layer_norm_weight.dtype
    to_76 = mul_150.to(getattr_112);  mul_150 = getattr_112 = None
    mul_151 = decoder_block_3_layer_0_layer_norm_weight * to_76;  decoder_block_3_layer_0_layer_norm_weight = to_76 = None
    size_31 = mul_151.size()
    getitem_86 = size_31[slice(None, 2, None)];  size_31 = None
    getitem_87 = getitem_86[0]
    getitem_88 = getitem_86[1];  getitem_86 = None
    decoder_block_3_layer_0_self_attention_q = getattr(getattr(self.decoder.block, "3").layer, "0").SelfAttention.q(mul_151)
    view_74 = decoder_block_3_layer_0_self_attention_q.view(getitem_87, -1, 12, 64);  decoder_block_3_layer_0_self_attention_q = None
    transpose_90 = view_74.transpose(1, 2);  view_74 = None
    decoder_block_3_layer_0_self_attention_k = getattr(getattr(self.decoder.block, "3").layer, "0").SelfAttention.k(mul_151)
    view_75 = decoder_block_3_layer_0_self_attention_k.view(getitem_87, -1, 12, 64);  decoder_block_3_layer_0_self_attention_k = None
    transpose_91 = view_75.transpose(1, 2);  view_75 = None
    decoder_block_3_layer_0_self_attention_v = getattr(getattr(self.decoder.block, "3").layer, "0").SelfAttention.v(mul_151);  mul_151 = None
    view_76 = decoder_block_3_layer_0_self_attention_v.view(getitem_87, -1, 12, 64);  decoder_block_3_layer_0_self_attention_v = None
    transpose_92 = view_76.transpose(1, 2);  view_76 = None
    transpose_93 = transpose_91.transpose(3, 2)
    matmul_36 = torch.matmul(transpose_90, transpose_93);  transpose_90 = transpose_93 = None
    add_125 = matmul_36 + add_93;  matmul_36 = None
    float_21 = add_125.float()
    softmax_18 = torch.nn.functional.softmax(float_21, dim = -1, _stacklevel = 3, dtype = None);  float_21 = None
    type_as_18 = softmax_18.type_as(add_125);  softmax_18 = add_125 = None
    dropout_18 = torch.nn.functional.dropout(type_as_18, p = 0.1, training = False, inplace = False);  type_as_18 = None
    matmul_37 = torch.matmul(dropout_18, transpose_92);  dropout_18 = None
    transpose_94 = matmul_37.transpose(1, 2);  matmul_37 = None
    contiguous_18 = transpose_94.contiguous();  transpose_94 = None
    view_77 = contiguous_18.view(getitem_87, -1, 768);  contiguous_18 = getitem_87 = None
    decoder_block_3_layer_0_self_attention_o = getattr(getattr(self.decoder.block, "3").layer, "0").SelfAttention.o(view_77);  view_77 = None
    decoder_block_3_layer_0_dropout = getattr(getattr(self.decoder.block, "3").layer, "0").dropout(decoder_block_3_layer_0_self_attention_o);  decoder_block_3_layer_0_self_attention_o = None
    add_126 = add_123 + decoder_block_3_layer_0_dropout;  add_123 = decoder_block_3_layer_0_dropout = None
    getattr_113 = add_126.dtype
    eq_77 = getattr_113 == torch.float16;  getattr_113 = None
    size_32 = transpose_91.size()
    getitem_89 = size_32[2];  size_32 = None
    to_77 = add_126.to(torch.float32)
    pow_51 = to_77.pow(2);  to_77 = None
    mean_35 = pow_51.mean(-1, keepdim = True);  pow_51 = None
    add_127 = mean_35 + 1e-06;  mean_35 = None
    rsqrt_35 = torch.rsqrt(add_127);  add_127 = None
    mul_152 = add_126 * rsqrt_35;  rsqrt_35 = None
    decoder_block_3_layer_1_layer_norm_weight = getattr(getattr(self.decoder.block, "3").layer, "1").layer_norm.weight
    getattr_114 = decoder_block_3_layer_1_layer_norm_weight.dtype
    eq_78 = getattr_114 == torch.float16;  getattr_114 = None
    getattr_115 = decoder_block_3_layer_1_layer_norm_weight.dtype
    to_78 = mul_152.to(getattr_115);  mul_152 = getattr_115 = None
    mul_153 = decoder_block_3_layer_1_layer_norm_weight * to_78;  decoder_block_3_layer_1_layer_norm_weight = to_78 = None
    size_33 = mul_153.size()
    getitem_90 = size_33[slice(None, 2, None)];  size_33 = None
    getitem_91 = getitem_90[0]
    getitem_92 = getitem_90[1];  getitem_90 = None
    size_34 = encoder_dropout_1.size()
    getitem_93 = size_34[1];  size_34 = None
    decoder_block_3_layer_1_enc_dec_attention_q = getattr(getattr(self.decoder.block, "3").layer, "1").EncDecAttention.q(mul_153);  mul_153 = None
    view_78 = decoder_block_3_layer_1_enc_dec_attention_q.view(getitem_91, -1, 12, 64);  decoder_block_3_layer_1_enc_dec_attention_q = None
    transpose_95 = view_78.transpose(1, 2);  view_78 = None
    decoder_block_3_layer_1_enc_dec_attention_k = getattr(getattr(self.decoder.block, "3").layer, "1").EncDecAttention.k(encoder_dropout_1)
    view_79 = decoder_block_3_layer_1_enc_dec_attention_k.view(getitem_91, -1, 12, 64);  decoder_block_3_layer_1_enc_dec_attention_k = None
    transpose_96 = view_79.transpose(1, 2);  view_79 = None
    decoder_block_3_layer_1_enc_dec_attention_v = getattr(getattr(self.decoder.block, "3").layer, "1").EncDecAttention.v(encoder_dropout_1)
    view_80 = decoder_block_3_layer_1_enc_dec_attention_v.view(getitem_91, -1, 12, 64);  decoder_block_3_layer_1_enc_dec_attention_v = None
    transpose_97 = view_80.transpose(1, 2);  view_80 = None
    transpose_98 = transpose_96.transpose(3, 2)
    matmul_38 = torch.matmul(transpose_95, transpose_98);  transpose_95 = transpose_98 = None
    add_128 = matmul_38 + add_97;  matmul_38 = None
    float_22 = add_128.float()
    softmax_19 = torch.nn.functional.softmax(float_22, dim = -1, _stacklevel = 3, dtype = None);  float_22 = None
    type_as_19 = softmax_19.type_as(add_128);  softmax_19 = add_128 = None
    dropout_19 = torch.nn.functional.dropout(type_as_19, p = 0.1, training = False, inplace = False);  type_as_19 = None
    matmul_39 = torch.matmul(dropout_19, transpose_97);  dropout_19 = None
    transpose_99 = matmul_39.transpose(1, 2);  matmul_39 = None
    contiguous_19 = transpose_99.contiguous();  transpose_99 = None
    view_81 = contiguous_19.view(getitem_91, -1, 768);  contiguous_19 = getitem_91 = None
    decoder_block_3_layer_1_enc_dec_attention_o = getattr(getattr(self.decoder.block, "3").layer, "1").EncDecAttention.o(view_81);  view_81 = None
    decoder_block_3_layer_1_dropout = getattr(getattr(self.decoder.block, "3").layer, "1").dropout(decoder_block_3_layer_1_enc_dec_attention_o);  decoder_block_3_layer_1_enc_dec_attention_o = None
    add_129 = add_126 + decoder_block_3_layer_1_dropout;  add_126 = decoder_block_3_layer_1_dropout = None
    getattr_116 = add_129.dtype
    eq_79 = getattr_116 == torch.float16;  getattr_116 = None
    to_79 = add_129.to(torch.float32)
    pow_52 = to_79.pow(2);  to_79 = None
    mean_36 = pow_52.mean(-1, keepdim = True);  pow_52 = None
    add_130 = mean_36 + 1e-06;  mean_36 = None
    rsqrt_36 = torch.rsqrt(add_130);  add_130 = None
    mul_154 = add_129 * rsqrt_36;  rsqrt_36 = None
    decoder_block_3_layer_2_layer_norm_weight = getattr(getattr(self.decoder.block, "3").layer, "2").layer_norm.weight
    getattr_117 = decoder_block_3_layer_2_layer_norm_weight.dtype
    eq_80 = getattr_117 == torch.float16;  getattr_117 = None
    getattr_118 = decoder_block_3_layer_2_layer_norm_weight.dtype
    to_80 = mul_154.to(getattr_118);  mul_154 = getattr_118 = None
    mul_155 = decoder_block_3_layer_2_layer_norm_weight * to_80;  decoder_block_3_layer_2_layer_norm_weight = to_80 = None
    decoder_block_3_layer_2_dense_relu_dense_wi_0 = getattr(getattr(self.decoder.block, "3").layer, "2").DenseReluDense.wi_0(mul_155)
    mul_156 = 0.5 * decoder_block_3_layer_2_dense_relu_dense_wi_0
    pow_53 = torch.pow(decoder_block_3_layer_2_dense_relu_dense_wi_0, 3.0)
    mul_157 = 0.044715 * pow_53;  pow_53 = None
    add_131 = decoder_block_3_layer_2_dense_relu_dense_wi_0 + mul_157;  decoder_block_3_layer_2_dense_relu_dense_wi_0 = mul_157 = None
    mul_158 = 0.7978845608028654 * add_131;  add_131 = None
    tanh_15 = torch.tanh(mul_158);  mul_158 = None
    add_132 = 1.0 + tanh_15;  tanh_15 = None
    mul_159 = mul_156 * add_132;  mul_156 = add_132 = None
    decoder_block_3_layer_2_dense_relu_dense_wi_1 = getattr(getattr(self.decoder.block, "3").layer, "2").DenseReluDense.wi_1(mul_155);  mul_155 = None
    mul_160 = mul_159 * decoder_block_3_layer_2_dense_relu_dense_wi_1;  mul_159 = decoder_block_3_layer_2_dense_relu_dense_wi_1 = None
    decoder_block_3_layer_2_dense_relu_dense_dropout = getattr(getattr(self.decoder.block, "3").layer, "2").DenseReluDense.dropout(mul_160);  mul_160 = None
    decoder_block_3_layer_2_dense_relu_dense_wo_weight = getattr(getattr(self.decoder.block, "3").layer, "2").DenseReluDense.wo.weight
    decoder_block_3_layer_2_dense_relu_dense_wo = getattr(getattr(self.decoder.block, "3").layer, "2").DenseReluDense.wo(decoder_block_3_layer_2_dense_relu_dense_dropout);  decoder_block_3_layer_2_dense_relu_dense_dropout = None
    decoder_block_3_layer_2_dropout = getattr(getattr(self.decoder.block, "3").layer, "2").dropout(decoder_block_3_layer_2_dense_relu_dense_wo);  decoder_block_3_layer_2_dense_relu_dense_wo = None
    add_133 = add_129 + decoder_block_3_layer_2_dropout;  add_129 = decoder_block_3_layer_2_dropout = None
    getattr_119 = add_133.dtype
    eq_81 = getattr_119 == torch.float16;  getattr_119 = None
    to_81 = add_133.to(torch.float32)
    pow_54 = to_81.pow(2);  to_81 = None
    mean_37 = pow_54.mean(-1, keepdim = True);  pow_54 = None
    add_134 = mean_37 + 1e-06;  mean_37 = None
    rsqrt_37 = torch.rsqrt(add_134);  add_134 = None
    mul_161 = add_133 * rsqrt_37;  rsqrt_37 = None
    decoder_block_4_layer_0_layer_norm_weight = getattr(getattr(self.decoder.block, "4").layer, "0").layer_norm.weight
    getattr_120 = decoder_block_4_layer_0_layer_norm_weight.dtype
    eq_82 = getattr_120 == torch.float16;  getattr_120 = None
    getattr_121 = decoder_block_4_layer_0_layer_norm_weight.dtype
    to_82 = mul_161.to(getattr_121);  mul_161 = getattr_121 = None
    mul_162 = decoder_block_4_layer_0_layer_norm_weight * to_82;  decoder_block_4_layer_0_layer_norm_weight = to_82 = None
    size_35 = mul_162.size()
    getitem_94 = size_35[slice(None, 2, None)];  size_35 = None
    getitem_95 = getitem_94[0]
    getitem_96 = getitem_94[1];  getitem_94 = None
    decoder_block_4_layer_0_self_attention_q = getattr(getattr(self.decoder.block, "4").layer, "0").SelfAttention.q(mul_162)
    view_82 = decoder_block_4_layer_0_self_attention_q.view(getitem_95, -1, 12, 64);  decoder_block_4_layer_0_self_attention_q = None
    transpose_100 = view_82.transpose(1, 2);  view_82 = None
    decoder_block_4_layer_0_self_attention_k = getattr(getattr(self.decoder.block, "4").layer, "0").SelfAttention.k(mul_162)
    view_83 = decoder_block_4_layer_0_self_attention_k.view(getitem_95, -1, 12, 64);  decoder_block_4_layer_0_self_attention_k = None
    transpose_101 = view_83.transpose(1, 2);  view_83 = None
    decoder_block_4_layer_0_self_attention_v = getattr(getattr(self.decoder.block, "4").layer, "0").SelfAttention.v(mul_162);  mul_162 = None
    view_84 = decoder_block_4_layer_0_self_attention_v.view(getitem_95, -1, 12, 64);  decoder_block_4_layer_0_self_attention_v = None
    transpose_102 = view_84.transpose(1, 2);  view_84 = None
    transpose_103 = transpose_101.transpose(3, 2)
    matmul_40 = torch.matmul(transpose_100, transpose_103);  transpose_100 = transpose_103 = None
    add_135 = matmul_40 + add_93;  matmul_40 = None
    float_23 = add_135.float()
    softmax_20 = torch.nn.functional.softmax(float_23, dim = -1, _stacklevel = 3, dtype = None);  float_23 = None
    type_as_20 = softmax_20.type_as(add_135);  softmax_20 = add_135 = None
    dropout_20 = torch.nn.functional.dropout(type_as_20, p = 0.1, training = False, inplace = False);  type_as_20 = None
    matmul_41 = torch.matmul(dropout_20, transpose_102);  dropout_20 = None
    transpose_104 = matmul_41.transpose(1, 2);  matmul_41 = None
    contiguous_20 = transpose_104.contiguous();  transpose_104 = None
    view_85 = contiguous_20.view(getitem_95, -1, 768);  contiguous_20 = getitem_95 = None
    decoder_block_4_layer_0_self_attention_o = getattr(getattr(self.decoder.block, "4").layer, "0").SelfAttention.o(view_85);  view_85 = None
    decoder_block_4_layer_0_dropout = getattr(getattr(self.decoder.block, "4").layer, "0").dropout(decoder_block_4_layer_0_self_attention_o);  decoder_block_4_layer_0_self_attention_o = None
    add_136 = add_133 + decoder_block_4_layer_0_dropout;  add_133 = decoder_block_4_layer_0_dropout = None
    getattr_122 = add_136.dtype
    eq_83 = getattr_122 == torch.float16;  getattr_122 = None
    size_36 = transpose_101.size()
    getitem_97 = size_36[2];  size_36 = None
    to_83 = add_136.to(torch.float32)
    pow_55 = to_83.pow(2);  to_83 = None
    mean_38 = pow_55.mean(-1, keepdim = True);  pow_55 = None
    add_137 = mean_38 + 1e-06;  mean_38 = None
    rsqrt_38 = torch.rsqrt(add_137);  add_137 = None
    mul_163 = add_136 * rsqrt_38;  rsqrt_38 = None
    decoder_block_4_layer_1_layer_norm_weight = getattr(getattr(self.decoder.block, "4").layer, "1").layer_norm.weight
    getattr_123 = decoder_block_4_layer_1_layer_norm_weight.dtype
    eq_84 = getattr_123 == torch.float16;  getattr_123 = None
    getattr_124 = decoder_block_4_layer_1_layer_norm_weight.dtype
    to_84 = mul_163.to(getattr_124);  mul_163 = getattr_124 = None
    mul_164 = decoder_block_4_layer_1_layer_norm_weight * to_84;  decoder_block_4_layer_1_layer_norm_weight = to_84 = None
    size_37 = mul_164.size()
    getitem_98 = size_37[slice(None, 2, None)];  size_37 = None
    getitem_99 = getitem_98[0]
    getitem_100 = getitem_98[1];  getitem_98 = None
    size_38 = encoder_dropout_1.size()
    getitem_101 = size_38[1];  size_38 = None
    decoder_block_4_layer_1_enc_dec_attention_q = getattr(getattr(self.decoder.block, "4").layer, "1").EncDecAttention.q(mul_164);  mul_164 = None
    view_86 = decoder_block_4_layer_1_enc_dec_attention_q.view(getitem_99, -1, 12, 64);  decoder_block_4_layer_1_enc_dec_attention_q = None
    transpose_105 = view_86.transpose(1, 2);  view_86 = None
    decoder_block_4_layer_1_enc_dec_attention_k = getattr(getattr(self.decoder.block, "4").layer, "1").EncDecAttention.k(encoder_dropout_1)
    view_87 = decoder_block_4_layer_1_enc_dec_attention_k.view(getitem_99, -1, 12, 64);  decoder_block_4_layer_1_enc_dec_attention_k = None
    transpose_106 = view_87.transpose(1, 2);  view_87 = None
    decoder_block_4_layer_1_enc_dec_attention_v = getattr(getattr(self.decoder.block, "4").layer, "1").EncDecAttention.v(encoder_dropout_1)
    view_88 = decoder_block_4_layer_1_enc_dec_attention_v.view(getitem_99, -1, 12, 64);  decoder_block_4_layer_1_enc_dec_attention_v = None
    transpose_107 = view_88.transpose(1, 2);  view_88 = None
    transpose_108 = transpose_106.transpose(3, 2)
    matmul_42 = torch.matmul(transpose_105, transpose_108);  transpose_105 = transpose_108 = None
    add_138 = matmul_42 + add_97;  matmul_42 = None
    float_24 = add_138.float()
    softmax_21 = torch.nn.functional.softmax(float_24, dim = -1, _stacklevel = 3, dtype = None);  float_24 = None
    type_as_21 = softmax_21.type_as(add_138);  softmax_21 = add_138 = None
    dropout_21 = torch.nn.functional.dropout(type_as_21, p = 0.1, training = False, inplace = False);  type_as_21 = None
    matmul_43 = torch.matmul(dropout_21, transpose_107);  dropout_21 = None
    transpose_109 = matmul_43.transpose(1, 2);  matmul_43 = None
    contiguous_21 = transpose_109.contiguous();  transpose_109 = None
    view_89 = contiguous_21.view(getitem_99, -1, 768);  contiguous_21 = getitem_99 = None
    decoder_block_4_layer_1_enc_dec_attention_o = getattr(getattr(self.decoder.block, "4").layer, "1").EncDecAttention.o(view_89);  view_89 = None
    decoder_block_4_layer_1_dropout = getattr(getattr(self.decoder.block, "4").layer, "1").dropout(decoder_block_4_layer_1_enc_dec_attention_o);  decoder_block_4_layer_1_enc_dec_attention_o = None
    add_139 = add_136 + decoder_block_4_layer_1_dropout;  add_136 = decoder_block_4_layer_1_dropout = None
    getattr_125 = add_139.dtype
    eq_85 = getattr_125 == torch.float16;  getattr_125 = None
    to_85 = add_139.to(torch.float32)
    pow_56 = to_85.pow(2);  to_85 = None
    mean_39 = pow_56.mean(-1, keepdim = True);  pow_56 = None
    add_140 = mean_39 + 1e-06;  mean_39 = None
    rsqrt_39 = torch.rsqrt(add_140);  add_140 = None
    mul_165 = add_139 * rsqrt_39;  rsqrt_39 = None
    decoder_block_4_layer_2_layer_norm_weight = getattr(getattr(self.decoder.block, "4").layer, "2").layer_norm.weight
    getattr_126 = decoder_block_4_layer_2_layer_norm_weight.dtype
    eq_86 = getattr_126 == torch.float16;  getattr_126 = None
    getattr_127 = decoder_block_4_layer_2_layer_norm_weight.dtype
    to_86 = mul_165.to(getattr_127);  mul_165 = getattr_127 = None
    mul_166 = decoder_block_4_layer_2_layer_norm_weight * to_86;  decoder_block_4_layer_2_layer_norm_weight = to_86 = None
    decoder_block_4_layer_2_dense_relu_dense_wi_0 = getattr(getattr(self.decoder.block, "4").layer, "2").DenseReluDense.wi_0(mul_166)
    mul_167 = 0.5 * decoder_block_4_layer_2_dense_relu_dense_wi_0
    pow_57 = torch.pow(decoder_block_4_layer_2_dense_relu_dense_wi_0, 3.0)
    mul_168 = 0.044715 * pow_57;  pow_57 = None
    add_141 = decoder_block_4_layer_2_dense_relu_dense_wi_0 + mul_168;  decoder_block_4_layer_2_dense_relu_dense_wi_0 = mul_168 = None
    mul_169 = 0.7978845608028654 * add_141;  add_141 = None
    tanh_16 = torch.tanh(mul_169);  mul_169 = None
    add_142 = 1.0 + tanh_16;  tanh_16 = None
    mul_170 = mul_167 * add_142;  mul_167 = add_142 = None
    decoder_block_4_layer_2_dense_relu_dense_wi_1 = getattr(getattr(self.decoder.block, "4").layer, "2").DenseReluDense.wi_1(mul_166);  mul_166 = None
    mul_171 = mul_170 * decoder_block_4_layer_2_dense_relu_dense_wi_1;  mul_170 = decoder_block_4_layer_2_dense_relu_dense_wi_1 = None
    decoder_block_4_layer_2_dense_relu_dense_dropout = getattr(getattr(self.decoder.block, "4").layer, "2").DenseReluDense.dropout(mul_171);  mul_171 = None
    decoder_block_4_layer_2_dense_relu_dense_wo_weight = getattr(getattr(self.decoder.block, "4").layer, "2").DenseReluDense.wo.weight
    decoder_block_4_layer_2_dense_relu_dense_wo = getattr(getattr(self.decoder.block, "4").layer, "2").DenseReluDense.wo(decoder_block_4_layer_2_dense_relu_dense_dropout);  decoder_block_4_layer_2_dense_relu_dense_dropout = None
    decoder_block_4_layer_2_dropout = getattr(getattr(self.decoder.block, "4").layer, "2").dropout(decoder_block_4_layer_2_dense_relu_dense_wo);  decoder_block_4_layer_2_dense_relu_dense_wo = None
    add_143 = add_139 + decoder_block_4_layer_2_dropout;  add_139 = decoder_block_4_layer_2_dropout = None
    getattr_128 = add_143.dtype
    eq_87 = getattr_128 == torch.float16;  getattr_128 = None
    to_87 = add_143.to(torch.float32)
    pow_58 = to_87.pow(2);  to_87 = None
    mean_40 = pow_58.mean(-1, keepdim = True);  pow_58 = None
    add_144 = mean_40 + 1e-06;  mean_40 = None
    rsqrt_40 = torch.rsqrt(add_144);  add_144 = None
    mul_172 = add_143 * rsqrt_40;  rsqrt_40 = None
    decoder_block_5_layer_0_layer_norm_weight = getattr(getattr(self.decoder.block, "5").layer, "0").layer_norm.weight
    getattr_129 = decoder_block_5_layer_0_layer_norm_weight.dtype
    eq_88 = getattr_129 == torch.float16;  getattr_129 = None
    getattr_130 = decoder_block_5_layer_0_layer_norm_weight.dtype
    to_88 = mul_172.to(getattr_130);  mul_172 = getattr_130 = None
    mul_173 = decoder_block_5_layer_0_layer_norm_weight * to_88;  decoder_block_5_layer_0_layer_norm_weight = to_88 = None
    size_39 = mul_173.size()
    getitem_102 = size_39[slice(None, 2, None)];  size_39 = None
    getitem_103 = getitem_102[0]
    getitem_104 = getitem_102[1];  getitem_102 = None
    decoder_block_5_layer_0_self_attention_q = getattr(getattr(self.decoder.block, "5").layer, "0").SelfAttention.q(mul_173)
    view_90 = decoder_block_5_layer_0_self_attention_q.view(getitem_103, -1, 12, 64);  decoder_block_5_layer_0_self_attention_q = None
    transpose_110 = view_90.transpose(1, 2);  view_90 = None
    decoder_block_5_layer_0_self_attention_k = getattr(getattr(self.decoder.block, "5").layer, "0").SelfAttention.k(mul_173)
    view_91 = decoder_block_5_layer_0_self_attention_k.view(getitem_103, -1, 12, 64);  decoder_block_5_layer_0_self_attention_k = None
    transpose_111 = view_91.transpose(1, 2);  view_91 = None
    decoder_block_5_layer_0_self_attention_v = getattr(getattr(self.decoder.block, "5").layer, "0").SelfAttention.v(mul_173);  mul_173 = None
    view_92 = decoder_block_5_layer_0_self_attention_v.view(getitem_103, -1, 12, 64);  decoder_block_5_layer_0_self_attention_v = None
    transpose_112 = view_92.transpose(1, 2);  view_92 = None
    transpose_113 = transpose_111.transpose(3, 2)
    matmul_44 = torch.matmul(transpose_110, transpose_113);  transpose_110 = transpose_113 = None
    add_145 = matmul_44 + add_93;  matmul_44 = None
    float_25 = add_145.float()
    softmax_22 = torch.nn.functional.softmax(float_25, dim = -1, _stacklevel = 3, dtype = None);  float_25 = None
    type_as_22 = softmax_22.type_as(add_145);  softmax_22 = add_145 = None
    dropout_22 = torch.nn.functional.dropout(type_as_22, p = 0.1, training = False, inplace = False);  type_as_22 = None
    matmul_45 = torch.matmul(dropout_22, transpose_112);  dropout_22 = None
    transpose_114 = matmul_45.transpose(1, 2);  matmul_45 = None
    contiguous_22 = transpose_114.contiguous();  transpose_114 = None
    view_93 = contiguous_22.view(getitem_103, -1, 768);  contiguous_22 = getitem_103 = None
    decoder_block_5_layer_0_self_attention_o = getattr(getattr(self.decoder.block, "5").layer, "0").SelfAttention.o(view_93);  view_93 = None
    decoder_block_5_layer_0_dropout = getattr(getattr(self.decoder.block, "5").layer, "0").dropout(decoder_block_5_layer_0_self_attention_o);  decoder_block_5_layer_0_self_attention_o = None
    add_146 = add_143 + decoder_block_5_layer_0_dropout;  add_143 = decoder_block_5_layer_0_dropout = None
    getattr_131 = add_146.dtype
    eq_89 = getattr_131 == torch.float16;  getattr_131 = None
    size_40 = transpose_111.size()
    getitem_105 = size_40[2];  size_40 = None
    to_89 = add_146.to(torch.float32)
    pow_59 = to_89.pow(2);  to_89 = None
    mean_41 = pow_59.mean(-1, keepdim = True);  pow_59 = None
    add_147 = mean_41 + 1e-06;  mean_41 = None
    rsqrt_41 = torch.rsqrt(add_147);  add_147 = None
    mul_174 = add_146 * rsqrt_41;  rsqrt_41 = None
    decoder_block_5_layer_1_layer_norm_weight = getattr(getattr(self.decoder.block, "5").layer, "1").layer_norm.weight
    getattr_132 = decoder_block_5_layer_1_layer_norm_weight.dtype
    eq_90 = getattr_132 == torch.float16;  getattr_132 = None
    getattr_133 = decoder_block_5_layer_1_layer_norm_weight.dtype
    to_90 = mul_174.to(getattr_133);  mul_174 = getattr_133 = None
    mul_175 = decoder_block_5_layer_1_layer_norm_weight * to_90;  decoder_block_5_layer_1_layer_norm_weight = to_90 = None
    size_41 = mul_175.size()
    getitem_106 = size_41[slice(None, 2, None)];  size_41 = None
    getitem_107 = getitem_106[0]
    getitem_108 = getitem_106[1];  getitem_106 = None
    size_42 = encoder_dropout_1.size()
    getitem_109 = size_42[1];  size_42 = None
    decoder_block_5_layer_1_enc_dec_attention_q = getattr(getattr(self.decoder.block, "5").layer, "1").EncDecAttention.q(mul_175);  mul_175 = None
    view_94 = decoder_block_5_layer_1_enc_dec_attention_q.view(getitem_107, -1, 12, 64);  decoder_block_5_layer_1_enc_dec_attention_q = None
    transpose_115 = view_94.transpose(1, 2);  view_94 = None
    decoder_block_5_layer_1_enc_dec_attention_k = getattr(getattr(self.decoder.block, "5").layer, "1").EncDecAttention.k(encoder_dropout_1)
    view_95 = decoder_block_5_layer_1_enc_dec_attention_k.view(getitem_107, -1, 12, 64);  decoder_block_5_layer_1_enc_dec_attention_k = None
    transpose_116 = view_95.transpose(1, 2);  view_95 = None
    decoder_block_5_layer_1_enc_dec_attention_v = getattr(getattr(self.decoder.block, "5").layer, "1").EncDecAttention.v(encoder_dropout_1)
    view_96 = decoder_block_5_layer_1_enc_dec_attention_v.view(getitem_107, -1, 12, 64);  decoder_block_5_layer_1_enc_dec_attention_v = None
    transpose_117 = view_96.transpose(1, 2);  view_96 = None
    transpose_118 = transpose_116.transpose(3, 2)
    matmul_46 = torch.matmul(transpose_115, transpose_118);  transpose_115 = transpose_118 = None
    add_148 = matmul_46 + add_97;  matmul_46 = None
    float_26 = add_148.float()
    softmax_23 = torch.nn.functional.softmax(float_26, dim = -1, _stacklevel = 3, dtype = None);  float_26 = None
    type_as_23 = softmax_23.type_as(add_148);  softmax_23 = add_148 = None
    dropout_23 = torch.nn.functional.dropout(type_as_23, p = 0.1, training = False, inplace = False);  type_as_23 = None
    matmul_47 = torch.matmul(dropout_23, transpose_117);  dropout_23 = None
    transpose_119 = matmul_47.transpose(1, 2);  matmul_47 = None
    contiguous_23 = transpose_119.contiguous();  transpose_119 = None
    view_97 = contiguous_23.view(getitem_107, -1, 768);  contiguous_23 = getitem_107 = None
    decoder_block_5_layer_1_enc_dec_attention_o = getattr(getattr(self.decoder.block, "5").layer, "1").EncDecAttention.o(view_97);  view_97 = None
    decoder_block_5_layer_1_dropout = getattr(getattr(self.decoder.block, "5").layer, "1").dropout(decoder_block_5_layer_1_enc_dec_attention_o);  decoder_block_5_layer_1_enc_dec_attention_o = None
    add_149 = add_146 + decoder_block_5_layer_1_dropout;  add_146 = decoder_block_5_layer_1_dropout = None
    getattr_134 = add_149.dtype
    eq_91 = getattr_134 == torch.float16;  getattr_134 = None
    to_91 = add_149.to(torch.float32)
    pow_60 = to_91.pow(2);  to_91 = None
    mean_42 = pow_60.mean(-1, keepdim = True);  pow_60 = None
    add_150 = mean_42 + 1e-06;  mean_42 = None
    rsqrt_42 = torch.rsqrt(add_150);  add_150 = None
    mul_176 = add_149 * rsqrt_42;  rsqrt_42 = None
    decoder_block_5_layer_2_layer_norm_weight = getattr(getattr(self.decoder.block, "5").layer, "2").layer_norm.weight
    getattr_135 = decoder_block_5_layer_2_layer_norm_weight.dtype
    eq_92 = getattr_135 == torch.float16;  getattr_135 = None
    getattr_136 = decoder_block_5_layer_2_layer_norm_weight.dtype
    to_92 = mul_176.to(getattr_136);  mul_176 = getattr_136 = None
    mul_177 = decoder_block_5_layer_2_layer_norm_weight * to_92;  decoder_block_5_layer_2_layer_norm_weight = to_92 = None
    decoder_block_5_layer_2_dense_relu_dense_wi_0 = getattr(getattr(self.decoder.block, "5").layer, "2").DenseReluDense.wi_0(mul_177)
    mul_178 = 0.5 * decoder_block_5_layer_2_dense_relu_dense_wi_0
    pow_61 = torch.pow(decoder_block_5_layer_2_dense_relu_dense_wi_0, 3.0)
    mul_179 = 0.044715 * pow_61;  pow_61 = None
    add_151 = decoder_block_5_layer_2_dense_relu_dense_wi_0 + mul_179;  decoder_block_5_layer_2_dense_relu_dense_wi_0 = mul_179 = None
    mul_180 = 0.7978845608028654 * add_151;  add_151 = None
    tanh_17 = torch.tanh(mul_180);  mul_180 = None
    add_152 = 1.0 + tanh_17;  tanh_17 = None
    mul_181 = mul_178 * add_152;  mul_178 = add_152 = None
    decoder_block_5_layer_2_dense_relu_dense_wi_1 = getattr(getattr(self.decoder.block, "5").layer, "2").DenseReluDense.wi_1(mul_177);  mul_177 = None
    mul_182 = mul_181 * decoder_block_5_layer_2_dense_relu_dense_wi_1;  mul_181 = decoder_block_5_layer_2_dense_relu_dense_wi_1 = None
    decoder_block_5_layer_2_dense_relu_dense_dropout = getattr(getattr(self.decoder.block, "5").layer, "2").DenseReluDense.dropout(mul_182);  mul_182 = None
    decoder_block_5_layer_2_dense_relu_dense_wo_weight = getattr(getattr(self.decoder.block, "5").layer, "2").DenseReluDense.wo.weight
    decoder_block_5_layer_2_dense_relu_dense_wo = getattr(getattr(self.decoder.block, "5").layer, "2").DenseReluDense.wo(decoder_block_5_layer_2_dense_relu_dense_dropout);  decoder_block_5_layer_2_dense_relu_dense_dropout = None
    decoder_block_5_layer_2_dropout = getattr(getattr(self.decoder.block, "5").layer, "2").dropout(decoder_block_5_layer_2_dense_relu_dense_wo);  decoder_block_5_layer_2_dense_relu_dense_wo = None
    add_153 = add_149 + decoder_block_5_layer_2_dropout;  add_149 = decoder_block_5_layer_2_dropout = None
    getattr_137 = add_153.dtype
    eq_93 = getattr_137 == torch.float16;  getattr_137 = None
    to_93 = add_153.to(torch.float32)
    pow_62 = to_93.pow(2);  to_93 = None
    mean_43 = pow_62.mean(-1, keepdim = True);  pow_62 = None
    add_154 = mean_43 + 1e-06;  mean_43 = None
    rsqrt_43 = torch.rsqrt(add_154);  add_154 = None
    mul_183 = add_153 * rsqrt_43;  rsqrt_43 = None
    decoder_block_6_layer_0_layer_norm_weight = getattr(getattr(self.decoder.block, "6").layer, "0").layer_norm.weight
    getattr_138 = decoder_block_6_layer_0_layer_norm_weight.dtype
    eq_94 = getattr_138 == torch.float16;  getattr_138 = None
    getattr_139 = decoder_block_6_layer_0_layer_norm_weight.dtype
    to_94 = mul_183.to(getattr_139);  mul_183 = getattr_139 = None
    mul_184 = decoder_block_6_layer_0_layer_norm_weight * to_94;  decoder_block_6_layer_0_layer_norm_weight = to_94 = None
    size_43 = mul_184.size()
    getitem_110 = size_43[slice(None, 2, None)];  size_43 = None
    getitem_111 = getitem_110[0]
    getitem_112 = getitem_110[1];  getitem_110 = None
    decoder_block_6_layer_0_self_attention_q = getattr(getattr(self.decoder.block, "6").layer, "0").SelfAttention.q(mul_184)
    view_98 = decoder_block_6_layer_0_self_attention_q.view(getitem_111, -1, 12, 64);  decoder_block_6_layer_0_self_attention_q = None
    transpose_120 = view_98.transpose(1, 2);  view_98 = None
    decoder_block_6_layer_0_self_attention_k = getattr(getattr(self.decoder.block, "6").layer, "0").SelfAttention.k(mul_184)
    view_99 = decoder_block_6_layer_0_self_attention_k.view(getitem_111, -1, 12, 64);  decoder_block_6_layer_0_self_attention_k = None
    transpose_121 = view_99.transpose(1, 2);  view_99 = None
    decoder_block_6_layer_0_self_attention_v = getattr(getattr(self.decoder.block, "6").layer, "0").SelfAttention.v(mul_184);  mul_184 = None
    view_100 = decoder_block_6_layer_0_self_attention_v.view(getitem_111, -1, 12, 64);  decoder_block_6_layer_0_self_attention_v = None
    transpose_122 = view_100.transpose(1, 2);  view_100 = None
    transpose_123 = transpose_121.transpose(3, 2)
    matmul_48 = torch.matmul(transpose_120, transpose_123);  transpose_120 = transpose_123 = None
    add_155 = matmul_48 + add_93;  matmul_48 = None
    float_27 = add_155.float()
    softmax_24 = torch.nn.functional.softmax(float_27, dim = -1, _stacklevel = 3, dtype = None);  float_27 = None
    type_as_24 = softmax_24.type_as(add_155);  softmax_24 = add_155 = None
    dropout_24 = torch.nn.functional.dropout(type_as_24, p = 0.1, training = False, inplace = False);  type_as_24 = None
    matmul_49 = torch.matmul(dropout_24, transpose_122);  dropout_24 = None
    transpose_124 = matmul_49.transpose(1, 2);  matmul_49 = None
    contiguous_24 = transpose_124.contiguous();  transpose_124 = None
    view_101 = contiguous_24.view(getitem_111, -1, 768);  contiguous_24 = getitem_111 = None
    decoder_block_6_layer_0_self_attention_o = getattr(getattr(self.decoder.block, "6").layer, "0").SelfAttention.o(view_101);  view_101 = None
    decoder_block_6_layer_0_dropout = getattr(getattr(self.decoder.block, "6").layer, "0").dropout(decoder_block_6_layer_0_self_attention_o);  decoder_block_6_layer_0_self_attention_o = None
    add_156 = add_153 + decoder_block_6_layer_0_dropout;  add_153 = decoder_block_6_layer_0_dropout = None
    getattr_140 = add_156.dtype
    eq_95 = getattr_140 == torch.float16;  getattr_140 = None
    size_44 = transpose_121.size()
    getitem_113 = size_44[2];  size_44 = None
    to_95 = add_156.to(torch.float32)
    pow_63 = to_95.pow(2);  to_95 = None
    mean_44 = pow_63.mean(-1, keepdim = True);  pow_63 = None
    add_157 = mean_44 + 1e-06;  mean_44 = None
    rsqrt_44 = torch.rsqrt(add_157);  add_157 = None
    mul_185 = add_156 * rsqrt_44;  rsqrt_44 = None
    decoder_block_6_layer_1_layer_norm_weight = getattr(getattr(self.decoder.block, "6").layer, "1").layer_norm.weight
    getattr_141 = decoder_block_6_layer_1_layer_norm_weight.dtype
    eq_96 = getattr_141 == torch.float16;  getattr_141 = None
    getattr_142 = decoder_block_6_layer_1_layer_norm_weight.dtype
    to_96 = mul_185.to(getattr_142);  mul_185 = getattr_142 = None
    mul_186 = decoder_block_6_layer_1_layer_norm_weight * to_96;  decoder_block_6_layer_1_layer_norm_weight = to_96 = None
    size_45 = mul_186.size()
    getitem_114 = size_45[slice(None, 2, None)];  size_45 = None
    getitem_115 = getitem_114[0]
    getitem_116 = getitem_114[1];  getitem_114 = None
    size_46 = encoder_dropout_1.size()
    getitem_117 = size_46[1];  size_46 = None
    decoder_block_6_layer_1_enc_dec_attention_q = getattr(getattr(self.decoder.block, "6").layer, "1").EncDecAttention.q(mul_186);  mul_186 = None
    view_102 = decoder_block_6_layer_1_enc_dec_attention_q.view(getitem_115, -1, 12, 64);  decoder_block_6_layer_1_enc_dec_attention_q = None
    transpose_125 = view_102.transpose(1, 2);  view_102 = None
    decoder_block_6_layer_1_enc_dec_attention_k = getattr(getattr(self.decoder.block, "6").layer, "1").EncDecAttention.k(encoder_dropout_1)
    view_103 = decoder_block_6_layer_1_enc_dec_attention_k.view(getitem_115, -1, 12, 64);  decoder_block_6_layer_1_enc_dec_attention_k = None
    transpose_126 = view_103.transpose(1, 2);  view_103 = None
    decoder_block_6_layer_1_enc_dec_attention_v = getattr(getattr(self.decoder.block, "6").layer, "1").EncDecAttention.v(encoder_dropout_1)
    view_104 = decoder_block_6_layer_1_enc_dec_attention_v.view(getitem_115, -1, 12, 64);  decoder_block_6_layer_1_enc_dec_attention_v = None
    transpose_127 = view_104.transpose(1, 2);  view_104 = None
    transpose_128 = transpose_126.transpose(3, 2)
    matmul_50 = torch.matmul(transpose_125, transpose_128);  transpose_125 = transpose_128 = None
    add_158 = matmul_50 + add_97;  matmul_50 = None
    float_28 = add_158.float()
    softmax_25 = torch.nn.functional.softmax(float_28, dim = -1, _stacklevel = 3, dtype = None);  float_28 = None
    type_as_25 = softmax_25.type_as(add_158);  softmax_25 = add_158 = None
    dropout_25 = torch.nn.functional.dropout(type_as_25, p = 0.1, training = False, inplace = False);  type_as_25 = None
    matmul_51 = torch.matmul(dropout_25, transpose_127);  dropout_25 = None
    transpose_129 = matmul_51.transpose(1, 2);  matmul_51 = None
    contiguous_25 = transpose_129.contiguous();  transpose_129 = None
    view_105 = contiguous_25.view(getitem_115, -1, 768);  contiguous_25 = getitem_115 = None
    decoder_block_6_layer_1_enc_dec_attention_o = getattr(getattr(self.decoder.block, "6").layer, "1").EncDecAttention.o(view_105);  view_105 = None
    decoder_block_6_layer_1_dropout = getattr(getattr(self.decoder.block, "6").layer, "1").dropout(decoder_block_6_layer_1_enc_dec_attention_o);  decoder_block_6_layer_1_enc_dec_attention_o = None
    add_159 = add_156 + decoder_block_6_layer_1_dropout;  add_156 = decoder_block_6_layer_1_dropout = None
    getattr_143 = add_159.dtype
    eq_97 = getattr_143 == torch.float16;  getattr_143 = None
    to_97 = add_159.to(torch.float32)
    pow_64 = to_97.pow(2);  to_97 = None
    mean_45 = pow_64.mean(-1, keepdim = True);  pow_64 = None
    add_160 = mean_45 + 1e-06;  mean_45 = None
    rsqrt_45 = torch.rsqrt(add_160);  add_160 = None
    mul_187 = add_159 * rsqrt_45;  rsqrt_45 = None
    decoder_block_6_layer_2_layer_norm_weight = getattr(getattr(self.decoder.block, "6").layer, "2").layer_norm.weight
    getattr_144 = decoder_block_6_layer_2_layer_norm_weight.dtype
    eq_98 = getattr_144 == torch.float16;  getattr_144 = None
    getattr_145 = decoder_block_6_layer_2_layer_norm_weight.dtype
    to_98 = mul_187.to(getattr_145);  mul_187 = getattr_145 = None
    mul_188 = decoder_block_6_layer_2_layer_norm_weight * to_98;  decoder_block_6_layer_2_layer_norm_weight = to_98 = None
    decoder_block_6_layer_2_dense_relu_dense_wi_0 = getattr(getattr(self.decoder.block, "6").layer, "2").DenseReluDense.wi_0(mul_188)
    mul_189 = 0.5 * decoder_block_6_layer_2_dense_relu_dense_wi_0
    pow_65 = torch.pow(decoder_block_6_layer_2_dense_relu_dense_wi_0, 3.0)
    mul_190 = 0.044715 * pow_65;  pow_65 = None
    add_161 = decoder_block_6_layer_2_dense_relu_dense_wi_0 + mul_190;  decoder_block_6_layer_2_dense_relu_dense_wi_0 = mul_190 = None
    mul_191 = 0.7978845608028654 * add_161;  add_161 = None
    tanh_18 = torch.tanh(mul_191);  mul_191 = None
    add_162 = 1.0 + tanh_18;  tanh_18 = None
    mul_192 = mul_189 * add_162;  mul_189 = add_162 = None
    decoder_block_6_layer_2_dense_relu_dense_wi_1 = getattr(getattr(self.decoder.block, "6").layer, "2").DenseReluDense.wi_1(mul_188);  mul_188 = None
    mul_193 = mul_192 * decoder_block_6_layer_2_dense_relu_dense_wi_1;  mul_192 = decoder_block_6_layer_2_dense_relu_dense_wi_1 = None
    decoder_block_6_layer_2_dense_relu_dense_dropout = getattr(getattr(self.decoder.block, "6").layer, "2").DenseReluDense.dropout(mul_193);  mul_193 = None
    decoder_block_6_layer_2_dense_relu_dense_wo_weight = getattr(getattr(self.decoder.block, "6").layer, "2").DenseReluDense.wo.weight
    decoder_block_6_layer_2_dense_relu_dense_wo = getattr(getattr(self.decoder.block, "6").layer, "2").DenseReluDense.wo(decoder_block_6_layer_2_dense_relu_dense_dropout);  decoder_block_6_layer_2_dense_relu_dense_dropout = None
    decoder_block_6_layer_2_dropout = getattr(getattr(self.decoder.block, "6").layer, "2").dropout(decoder_block_6_layer_2_dense_relu_dense_wo);  decoder_block_6_layer_2_dense_relu_dense_wo = None
    add_163 = add_159 + decoder_block_6_layer_2_dropout;  add_159 = decoder_block_6_layer_2_dropout = None
    getattr_146 = add_163.dtype
    eq_99 = getattr_146 == torch.float16;  getattr_146 = None
    to_99 = add_163.to(torch.float32)
    pow_66 = to_99.pow(2);  to_99 = None
    mean_46 = pow_66.mean(-1, keepdim = True);  pow_66 = None
    add_164 = mean_46 + 1e-06;  mean_46 = None
    rsqrt_46 = torch.rsqrt(add_164);  add_164 = None
    mul_194 = add_163 * rsqrt_46;  rsqrt_46 = None
    decoder_block_7_layer_0_layer_norm_weight = getattr(getattr(self.decoder.block, "7").layer, "0").layer_norm.weight
    getattr_147 = decoder_block_7_layer_0_layer_norm_weight.dtype
    eq_100 = getattr_147 == torch.float16;  getattr_147 = None
    getattr_148 = decoder_block_7_layer_0_layer_norm_weight.dtype
    to_100 = mul_194.to(getattr_148);  mul_194 = getattr_148 = None
    mul_195 = decoder_block_7_layer_0_layer_norm_weight * to_100;  decoder_block_7_layer_0_layer_norm_weight = to_100 = None
    size_47 = mul_195.size()
    getitem_118 = size_47[slice(None, 2, None)];  size_47 = None
    getitem_119 = getitem_118[0]
    getitem_120 = getitem_118[1];  getitem_118 = None
    decoder_block_7_layer_0_self_attention_q = getattr(getattr(self.decoder.block, "7").layer, "0").SelfAttention.q(mul_195)
    view_106 = decoder_block_7_layer_0_self_attention_q.view(getitem_119, -1, 12, 64);  decoder_block_7_layer_0_self_attention_q = None
    transpose_130 = view_106.transpose(1, 2);  view_106 = None
    decoder_block_7_layer_0_self_attention_k = getattr(getattr(self.decoder.block, "7").layer, "0").SelfAttention.k(mul_195)
    view_107 = decoder_block_7_layer_0_self_attention_k.view(getitem_119, -1, 12, 64);  decoder_block_7_layer_0_self_attention_k = None
    transpose_131 = view_107.transpose(1, 2);  view_107 = None
    decoder_block_7_layer_0_self_attention_v = getattr(getattr(self.decoder.block, "7").layer, "0").SelfAttention.v(mul_195);  mul_195 = None
    view_108 = decoder_block_7_layer_0_self_attention_v.view(getitem_119, -1, 12, 64);  decoder_block_7_layer_0_self_attention_v = None
    transpose_132 = view_108.transpose(1, 2);  view_108 = None
    transpose_133 = transpose_131.transpose(3, 2)
    matmul_52 = torch.matmul(transpose_130, transpose_133);  transpose_130 = transpose_133 = None
    add_165 = matmul_52 + add_93;  matmul_52 = None
    float_29 = add_165.float()
    softmax_26 = torch.nn.functional.softmax(float_29, dim = -1, _stacklevel = 3, dtype = None);  float_29 = None
    type_as_26 = softmax_26.type_as(add_165);  softmax_26 = add_165 = None
    dropout_26 = torch.nn.functional.dropout(type_as_26, p = 0.1, training = False, inplace = False);  type_as_26 = None
    matmul_53 = torch.matmul(dropout_26, transpose_132);  dropout_26 = None
    transpose_134 = matmul_53.transpose(1, 2);  matmul_53 = None
    contiguous_26 = transpose_134.contiguous();  transpose_134 = None
    view_109 = contiguous_26.view(getitem_119, -1, 768);  contiguous_26 = getitem_119 = None
    decoder_block_7_layer_0_self_attention_o = getattr(getattr(self.decoder.block, "7").layer, "0").SelfAttention.o(view_109);  view_109 = None
    decoder_block_7_layer_0_dropout = getattr(getattr(self.decoder.block, "7").layer, "0").dropout(decoder_block_7_layer_0_self_attention_o);  decoder_block_7_layer_0_self_attention_o = None
    add_166 = add_163 + decoder_block_7_layer_0_dropout;  add_163 = decoder_block_7_layer_0_dropout = None
    getattr_149 = add_166.dtype
    eq_101 = getattr_149 == torch.float16;  getattr_149 = None
    size_48 = transpose_131.size()
    getitem_121 = size_48[2];  size_48 = None
    to_101 = add_166.to(torch.float32)
    pow_67 = to_101.pow(2);  to_101 = None
    mean_47 = pow_67.mean(-1, keepdim = True);  pow_67 = None
    add_167 = mean_47 + 1e-06;  mean_47 = None
    rsqrt_47 = torch.rsqrt(add_167);  add_167 = None
    mul_196 = add_166 * rsqrt_47;  rsqrt_47 = None
    decoder_block_7_layer_1_layer_norm_weight = getattr(getattr(self.decoder.block, "7").layer, "1").layer_norm.weight
    getattr_150 = decoder_block_7_layer_1_layer_norm_weight.dtype
    eq_102 = getattr_150 == torch.float16;  getattr_150 = None
    getattr_151 = decoder_block_7_layer_1_layer_norm_weight.dtype
    to_102 = mul_196.to(getattr_151);  mul_196 = getattr_151 = None
    mul_197 = decoder_block_7_layer_1_layer_norm_weight * to_102;  decoder_block_7_layer_1_layer_norm_weight = to_102 = None
    size_49 = mul_197.size()
    getitem_122 = size_49[slice(None, 2, None)];  size_49 = None
    getitem_123 = getitem_122[0]
    getitem_124 = getitem_122[1];  getitem_122 = None
    size_50 = encoder_dropout_1.size()
    getitem_125 = size_50[1];  size_50 = None
    decoder_block_7_layer_1_enc_dec_attention_q = getattr(getattr(self.decoder.block, "7").layer, "1").EncDecAttention.q(mul_197);  mul_197 = None
    view_110 = decoder_block_7_layer_1_enc_dec_attention_q.view(getitem_123, -1, 12, 64);  decoder_block_7_layer_1_enc_dec_attention_q = None
    transpose_135 = view_110.transpose(1, 2);  view_110 = None
    decoder_block_7_layer_1_enc_dec_attention_k = getattr(getattr(self.decoder.block, "7").layer, "1").EncDecAttention.k(encoder_dropout_1)
    view_111 = decoder_block_7_layer_1_enc_dec_attention_k.view(getitem_123, -1, 12, 64);  decoder_block_7_layer_1_enc_dec_attention_k = None
    transpose_136 = view_111.transpose(1, 2);  view_111 = None
    decoder_block_7_layer_1_enc_dec_attention_v = getattr(getattr(self.decoder.block, "7").layer, "1").EncDecAttention.v(encoder_dropout_1)
    view_112 = decoder_block_7_layer_1_enc_dec_attention_v.view(getitem_123, -1, 12, 64);  decoder_block_7_layer_1_enc_dec_attention_v = None
    transpose_137 = view_112.transpose(1, 2);  view_112 = None
    transpose_138 = transpose_136.transpose(3, 2)
    matmul_54 = torch.matmul(transpose_135, transpose_138);  transpose_135 = transpose_138 = None
    add_168 = matmul_54 + add_97;  matmul_54 = None
    float_30 = add_168.float()
    softmax_27 = torch.nn.functional.softmax(float_30, dim = -1, _stacklevel = 3, dtype = None);  float_30 = None
    type_as_27 = softmax_27.type_as(add_168);  softmax_27 = add_168 = None
    dropout_27 = torch.nn.functional.dropout(type_as_27, p = 0.1, training = False, inplace = False);  type_as_27 = None
    matmul_55 = torch.matmul(dropout_27, transpose_137);  dropout_27 = None
    transpose_139 = matmul_55.transpose(1, 2);  matmul_55 = None
    contiguous_27 = transpose_139.contiguous();  transpose_139 = None
    view_113 = contiguous_27.view(getitem_123, -1, 768);  contiguous_27 = getitem_123 = None
    decoder_block_7_layer_1_enc_dec_attention_o = getattr(getattr(self.decoder.block, "7").layer, "1").EncDecAttention.o(view_113);  view_113 = None
    decoder_block_7_layer_1_dropout = getattr(getattr(self.decoder.block, "7").layer, "1").dropout(decoder_block_7_layer_1_enc_dec_attention_o);  decoder_block_7_layer_1_enc_dec_attention_o = None
    add_169 = add_166 + decoder_block_7_layer_1_dropout;  add_166 = decoder_block_7_layer_1_dropout = None
    getattr_152 = add_169.dtype
    eq_103 = getattr_152 == torch.float16;  getattr_152 = None
    to_103 = add_169.to(torch.float32)
    pow_68 = to_103.pow(2);  to_103 = None
    mean_48 = pow_68.mean(-1, keepdim = True);  pow_68 = None
    add_170 = mean_48 + 1e-06;  mean_48 = None
    rsqrt_48 = torch.rsqrt(add_170);  add_170 = None
    mul_198 = add_169 * rsqrt_48;  rsqrt_48 = None
    decoder_block_7_layer_2_layer_norm_weight = getattr(getattr(self.decoder.block, "7").layer, "2").layer_norm.weight
    getattr_153 = decoder_block_7_layer_2_layer_norm_weight.dtype
    eq_104 = getattr_153 == torch.float16;  getattr_153 = None
    getattr_154 = decoder_block_7_layer_2_layer_norm_weight.dtype
    to_104 = mul_198.to(getattr_154);  mul_198 = getattr_154 = None
    mul_199 = decoder_block_7_layer_2_layer_norm_weight * to_104;  decoder_block_7_layer_2_layer_norm_weight = to_104 = None
    decoder_block_7_layer_2_dense_relu_dense_wi_0 = getattr(getattr(self.decoder.block, "7").layer, "2").DenseReluDense.wi_0(mul_199)
    mul_200 = 0.5 * decoder_block_7_layer_2_dense_relu_dense_wi_0
    pow_69 = torch.pow(decoder_block_7_layer_2_dense_relu_dense_wi_0, 3.0)
    mul_201 = 0.044715 * pow_69;  pow_69 = None
    add_171 = decoder_block_7_layer_2_dense_relu_dense_wi_0 + mul_201;  decoder_block_7_layer_2_dense_relu_dense_wi_0 = mul_201 = None
    mul_202 = 0.7978845608028654 * add_171;  add_171 = None
    tanh_19 = torch.tanh(mul_202);  mul_202 = None
    add_172 = 1.0 + tanh_19;  tanh_19 = None
    mul_203 = mul_200 * add_172;  mul_200 = add_172 = None
    decoder_block_7_layer_2_dense_relu_dense_wi_1 = getattr(getattr(self.decoder.block, "7").layer, "2").DenseReluDense.wi_1(mul_199);  mul_199 = None
    mul_204 = mul_203 * decoder_block_7_layer_2_dense_relu_dense_wi_1;  mul_203 = decoder_block_7_layer_2_dense_relu_dense_wi_1 = None
    decoder_block_7_layer_2_dense_relu_dense_dropout = getattr(getattr(self.decoder.block, "7").layer, "2").DenseReluDense.dropout(mul_204);  mul_204 = None
    decoder_block_7_layer_2_dense_relu_dense_wo_weight = getattr(getattr(self.decoder.block, "7").layer, "2").DenseReluDense.wo.weight
    decoder_block_7_layer_2_dense_relu_dense_wo = getattr(getattr(self.decoder.block, "7").layer, "2").DenseReluDense.wo(decoder_block_7_layer_2_dense_relu_dense_dropout);  decoder_block_7_layer_2_dense_relu_dense_dropout = None
    decoder_block_7_layer_2_dropout = getattr(getattr(self.decoder.block, "7").layer, "2").dropout(decoder_block_7_layer_2_dense_relu_dense_wo);  decoder_block_7_layer_2_dense_relu_dense_wo = None
    add_173 = add_169 + decoder_block_7_layer_2_dropout;  add_169 = decoder_block_7_layer_2_dropout = None
    getattr_155 = add_173.dtype
    eq_105 = getattr_155 == torch.float16;  getattr_155 = None
    to_105 = add_173.to(torch.float32)
    pow_70 = to_105.pow(2);  to_105 = None
    mean_49 = pow_70.mean(-1, keepdim = True);  pow_70 = None
    add_174 = mean_49 + 1e-06;  mean_49 = None
    rsqrt_49 = torch.rsqrt(add_174);  add_174 = None
    mul_205 = add_173 * rsqrt_49;  rsqrt_49 = None
    decoder_block_8_layer_0_layer_norm_weight = getattr(getattr(self.decoder.block, "8").layer, "0").layer_norm.weight
    getattr_156 = decoder_block_8_layer_0_layer_norm_weight.dtype
    eq_106 = getattr_156 == torch.float16;  getattr_156 = None
    getattr_157 = decoder_block_8_layer_0_layer_norm_weight.dtype
    to_106 = mul_205.to(getattr_157);  mul_205 = getattr_157 = None
    mul_206 = decoder_block_8_layer_0_layer_norm_weight * to_106;  decoder_block_8_layer_0_layer_norm_weight = to_106 = None
    size_51 = mul_206.size()
    getitem_126 = size_51[slice(None, 2, None)];  size_51 = None
    getitem_127 = getitem_126[0]
    getitem_128 = getitem_126[1];  getitem_126 = None
    decoder_block_8_layer_0_self_attention_q = getattr(getattr(self.decoder.block, "8").layer, "0").SelfAttention.q(mul_206)
    view_114 = decoder_block_8_layer_0_self_attention_q.view(getitem_127, -1, 12, 64);  decoder_block_8_layer_0_self_attention_q = None
    transpose_140 = view_114.transpose(1, 2);  view_114 = None
    decoder_block_8_layer_0_self_attention_k = getattr(getattr(self.decoder.block, "8").layer, "0").SelfAttention.k(mul_206)
    view_115 = decoder_block_8_layer_0_self_attention_k.view(getitem_127, -1, 12, 64);  decoder_block_8_layer_0_self_attention_k = None
    transpose_141 = view_115.transpose(1, 2);  view_115 = None
    decoder_block_8_layer_0_self_attention_v = getattr(getattr(self.decoder.block, "8").layer, "0").SelfAttention.v(mul_206);  mul_206 = None
    view_116 = decoder_block_8_layer_0_self_attention_v.view(getitem_127, -1, 12, 64);  decoder_block_8_layer_0_self_attention_v = None
    transpose_142 = view_116.transpose(1, 2);  view_116 = None
    transpose_143 = transpose_141.transpose(3, 2)
    matmul_56 = torch.matmul(transpose_140, transpose_143);  transpose_140 = transpose_143 = None
    add_175 = matmul_56 + add_93;  matmul_56 = None
    float_31 = add_175.float()
    softmax_28 = torch.nn.functional.softmax(float_31, dim = -1, _stacklevel = 3, dtype = None);  float_31 = None
    type_as_28 = softmax_28.type_as(add_175);  softmax_28 = add_175 = None
    dropout_28 = torch.nn.functional.dropout(type_as_28, p = 0.1, training = False, inplace = False);  type_as_28 = None
    matmul_57 = torch.matmul(dropout_28, transpose_142);  dropout_28 = None
    transpose_144 = matmul_57.transpose(1, 2);  matmul_57 = None
    contiguous_28 = transpose_144.contiguous();  transpose_144 = None
    view_117 = contiguous_28.view(getitem_127, -1, 768);  contiguous_28 = getitem_127 = None
    decoder_block_8_layer_0_self_attention_o = getattr(getattr(self.decoder.block, "8").layer, "0").SelfAttention.o(view_117);  view_117 = None
    decoder_block_8_layer_0_dropout = getattr(getattr(self.decoder.block, "8").layer, "0").dropout(decoder_block_8_layer_0_self_attention_o);  decoder_block_8_layer_0_self_attention_o = None
    add_176 = add_173 + decoder_block_8_layer_0_dropout;  add_173 = decoder_block_8_layer_0_dropout = None
    getattr_158 = add_176.dtype
    eq_107 = getattr_158 == torch.float16;  getattr_158 = None
    size_52 = transpose_141.size()
    getitem_129 = size_52[2];  size_52 = None
    to_107 = add_176.to(torch.float32)
    pow_71 = to_107.pow(2);  to_107 = None
    mean_50 = pow_71.mean(-1, keepdim = True);  pow_71 = None
    add_177 = mean_50 + 1e-06;  mean_50 = None
    rsqrt_50 = torch.rsqrt(add_177);  add_177 = None
    mul_207 = add_176 * rsqrt_50;  rsqrt_50 = None
    decoder_block_8_layer_1_layer_norm_weight = getattr(getattr(self.decoder.block, "8").layer, "1").layer_norm.weight
    getattr_159 = decoder_block_8_layer_1_layer_norm_weight.dtype
    eq_108 = getattr_159 == torch.float16;  getattr_159 = None
    getattr_160 = decoder_block_8_layer_1_layer_norm_weight.dtype
    to_108 = mul_207.to(getattr_160);  mul_207 = getattr_160 = None
    mul_208 = decoder_block_8_layer_1_layer_norm_weight * to_108;  decoder_block_8_layer_1_layer_norm_weight = to_108 = None
    size_53 = mul_208.size()
    getitem_130 = size_53[slice(None, 2, None)];  size_53 = None
    getitem_131 = getitem_130[0]
    getitem_132 = getitem_130[1];  getitem_130 = None
    size_54 = encoder_dropout_1.size()
    getitem_133 = size_54[1];  size_54 = None
    decoder_block_8_layer_1_enc_dec_attention_q = getattr(getattr(self.decoder.block, "8").layer, "1").EncDecAttention.q(mul_208);  mul_208 = None
    view_118 = decoder_block_8_layer_1_enc_dec_attention_q.view(getitem_131, -1, 12, 64);  decoder_block_8_layer_1_enc_dec_attention_q = None
    transpose_145 = view_118.transpose(1, 2);  view_118 = None
    decoder_block_8_layer_1_enc_dec_attention_k = getattr(getattr(self.decoder.block, "8").layer, "1").EncDecAttention.k(encoder_dropout_1)
    view_119 = decoder_block_8_layer_1_enc_dec_attention_k.view(getitem_131, -1, 12, 64);  decoder_block_8_layer_1_enc_dec_attention_k = None
    transpose_146 = view_119.transpose(1, 2);  view_119 = None
    decoder_block_8_layer_1_enc_dec_attention_v = getattr(getattr(self.decoder.block, "8").layer, "1").EncDecAttention.v(encoder_dropout_1)
    view_120 = decoder_block_8_layer_1_enc_dec_attention_v.view(getitem_131, -1, 12, 64);  decoder_block_8_layer_1_enc_dec_attention_v = None
    transpose_147 = view_120.transpose(1, 2);  view_120 = None
    transpose_148 = transpose_146.transpose(3, 2)
    matmul_58 = torch.matmul(transpose_145, transpose_148);  transpose_145 = transpose_148 = None
    add_178 = matmul_58 + add_97;  matmul_58 = None
    float_32 = add_178.float()
    softmax_29 = torch.nn.functional.softmax(float_32, dim = -1, _stacklevel = 3, dtype = None);  float_32 = None
    type_as_29 = softmax_29.type_as(add_178);  softmax_29 = add_178 = None
    dropout_29 = torch.nn.functional.dropout(type_as_29, p = 0.1, training = False, inplace = False);  type_as_29 = None
    matmul_59 = torch.matmul(dropout_29, transpose_147);  dropout_29 = None
    transpose_149 = matmul_59.transpose(1, 2);  matmul_59 = None
    contiguous_29 = transpose_149.contiguous();  transpose_149 = None
    view_121 = contiguous_29.view(getitem_131, -1, 768);  contiguous_29 = getitem_131 = None
    decoder_block_8_layer_1_enc_dec_attention_o = getattr(getattr(self.decoder.block, "8").layer, "1").EncDecAttention.o(view_121);  view_121 = None
    decoder_block_8_layer_1_dropout = getattr(getattr(self.decoder.block, "8").layer, "1").dropout(decoder_block_8_layer_1_enc_dec_attention_o);  decoder_block_8_layer_1_enc_dec_attention_o = None
    add_179 = add_176 + decoder_block_8_layer_1_dropout;  add_176 = decoder_block_8_layer_1_dropout = None
    getattr_161 = add_179.dtype
    eq_109 = getattr_161 == torch.float16;  getattr_161 = None
    to_109 = add_179.to(torch.float32)
    pow_72 = to_109.pow(2);  to_109 = None
    mean_51 = pow_72.mean(-1, keepdim = True);  pow_72 = None
    add_180 = mean_51 + 1e-06;  mean_51 = None
    rsqrt_51 = torch.rsqrt(add_180);  add_180 = None
    mul_209 = add_179 * rsqrt_51;  rsqrt_51 = None
    decoder_block_8_layer_2_layer_norm_weight = getattr(getattr(self.decoder.block, "8").layer, "2").layer_norm.weight
    getattr_162 = decoder_block_8_layer_2_layer_norm_weight.dtype
    eq_110 = getattr_162 == torch.float16;  getattr_162 = None
    getattr_163 = decoder_block_8_layer_2_layer_norm_weight.dtype
    to_110 = mul_209.to(getattr_163);  mul_209 = getattr_163 = None
    mul_210 = decoder_block_8_layer_2_layer_norm_weight * to_110;  decoder_block_8_layer_2_layer_norm_weight = to_110 = None
    decoder_block_8_layer_2_dense_relu_dense_wi_0 = getattr(getattr(self.decoder.block, "8").layer, "2").DenseReluDense.wi_0(mul_210)
    mul_211 = 0.5 * decoder_block_8_layer_2_dense_relu_dense_wi_0
    pow_73 = torch.pow(decoder_block_8_layer_2_dense_relu_dense_wi_0, 3.0)
    mul_212 = 0.044715 * pow_73;  pow_73 = None
    add_181 = decoder_block_8_layer_2_dense_relu_dense_wi_0 + mul_212;  decoder_block_8_layer_2_dense_relu_dense_wi_0 = mul_212 = None
    mul_213 = 0.7978845608028654 * add_181;  add_181 = None
    tanh_20 = torch.tanh(mul_213);  mul_213 = None
    add_182 = 1.0 + tanh_20;  tanh_20 = None
    mul_214 = mul_211 * add_182;  mul_211 = add_182 = None
    decoder_block_8_layer_2_dense_relu_dense_wi_1 = getattr(getattr(self.decoder.block, "8").layer, "2").DenseReluDense.wi_1(mul_210);  mul_210 = None
    mul_215 = mul_214 * decoder_block_8_layer_2_dense_relu_dense_wi_1;  mul_214 = decoder_block_8_layer_2_dense_relu_dense_wi_1 = None
    decoder_block_8_layer_2_dense_relu_dense_dropout = getattr(getattr(self.decoder.block, "8").layer, "2").DenseReluDense.dropout(mul_215);  mul_215 = None
    decoder_block_8_layer_2_dense_relu_dense_wo_weight = getattr(getattr(self.decoder.block, "8").layer, "2").DenseReluDense.wo.weight
    decoder_block_8_layer_2_dense_relu_dense_wo = getattr(getattr(self.decoder.block, "8").layer, "2").DenseReluDense.wo(decoder_block_8_layer_2_dense_relu_dense_dropout);  decoder_block_8_layer_2_dense_relu_dense_dropout = None
    decoder_block_8_layer_2_dropout = getattr(getattr(self.decoder.block, "8").layer, "2").dropout(decoder_block_8_layer_2_dense_relu_dense_wo);  decoder_block_8_layer_2_dense_relu_dense_wo = None
    add_183 = add_179 + decoder_block_8_layer_2_dropout;  add_179 = decoder_block_8_layer_2_dropout = None
    getattr_164 = add_183.dtype
    eq_111 = getattr_164 == torch.float16;  getattr_164 = None
    to_111 = add_183.to(torch.float32)
    pow_74 = to_111.pow(2);  to_111 = None
    mean_52 = pow_74.mean(-1, keepdim = True);  pow_74 = None
    add_184 = mean_52 + 1e-06;  mean_52 = None
    rsqrt_52 = torch.rsqrt(add_184);  add_184 = None
    mul_216 = add_183 * rsqrt_52;  rsqrt_52 = None
    decoder_block_9_layer_0_layer_norm_weight = getattr(getattr(self.decoder.block, "9").layer, "0").layer_norm.weight
    getattr_165 = decoder_block_9_layer_0_layer_norm_weight.dtype
    eq_112 = getattr_165 == torch.float16;  getattr_165 = None
    getattr_166 = decoder_block_9_layer_0_layer_norm_weight.dtype
    to_112 = mul_216.to(getattr_166);  mul_216 = getattr_166 = None
    mul_217 = decoder_block_9_layer_0_layer_norm_weight * to_112;  decoder_block_9_layer_0_layer_norm_weight = to_112 = None
    size_55 = mul_217.size()
    getitem_134 = size_55[slice(None, 2, None)];  size_55 = None
    getitem_135 = getitem_134[0]
    getitem_136 = getitem_134[1];  getitem_134 = None
    decoder_block_9_layer_0_self_attention_q = getattr(getattr(self.decoder.block, "9").layer, "0").SelfAttention.q(mul_217)
    view_122 = decoder_block_9_layer_0_self_attention_q.view(getitem_135, -1, 12, 64);  decoder_block_9_layer_0_self_attention_q = None
    transpose_150 = view_122.transpose(1, 2);  view_122 = None
    decoder_block_9_layer_0_self_attention_k = getattr(getattr(self.decoder.block, "9").layer, "0").SelfAttention.k(mul_217)
    view_123 = decoder_block_9_layer_0_self_attention_k.view(getitem_135, -1, 12, 64);  decoder_block_9_layer_0_self_attention_k = None
    transpose_151 = view_123.transpose(1, 2);  view_123 = None
    decoder_block_9_layer_0_self_attention_v = getattr(getattr(self.decoder.block, "9").layer, "0").SelfAttention.v(mul_217);  mul_217 = None
    view_124 = decoder_block_9_layer_0_self_attention_v.view(getitem_135, -1, 12, 64);  decoder_block_9_layer_0_self_attention_v = None
    transpose_152 = view_124.transpose(1, 2);  view_124 = None
    transpose_153 = transpose_151.transpose(3, 2)
    matmul_60 = torch.matmul(transpose_150, transpose_153);  transpose_150 = transpose_153 = None
    add_185 = matmul_60 + add_93;  matmul_60 = None
    float_33 = add_185.float()
    softmax_30 = torch.nn.functional.softmax(float_33, dim = -1, _stacklevel = 3, dtype = None);  float_33 = None
    type_as_30 = softmax_30.type_as(add_185);  softmax_30 = add_185 = None
    dropout_30 = torch.nn.functional.dropout(type_as_30, p = 0.1, training = False, inplace = False);  type_as_30 = None
    matmul_61 = torch.matmul(dropout_30, transpose_152);  dropout_30 = None
    transpose_154 = matmul_61.transpose(1, 2);  matmul_61 = None
    contiguous_30 = transpose_154.contiguous();  transpose_154 = None
    view_125 = contiguous_30.view(getitem_135, -1, 768);  contiguous_30 = getitem_135 = None
    decoder_block_9_layer_0_self_attention_o = getattr(getattr(self.decoder.block, "9").layer, "0").SelfAttention.o(view_125);  view_125 = None
    decoder_block_9_layer_0_dropout = getattr(getattr(self.decoder.block, "9").layer, "0").dropout(decoder_block_9_layer_0_self_attention_o);  decoder_block_9_layer_0_self_attention_o = None
    add_186 = add_183 + decoder_block_9_layer_0_dropout;  add_183 = decoder_block_9_layer_0_dropout = None
    getattr_167 = add_186.dtype
    eq_113 = getattr_167 == torch.float16;  getattr_167 = None
    size_56 = transpose_151.size()
    getitem_137 = size_56[2];  size_56 = None
    to_113 = add_186.to(torch.float32)
    pow_75 = to_113.pow(2);  to_113 = None
    mean_53 = pow_75.mean(-1, keepdim = True);  pow_75 = None
    add_187 = mean_53 + 1e-06;  mean_53 = None
    rsqrt_53 = torch.rsqrt(add_187);  add_187 = None
    mul_218 = add_186 * rsqrt_53;  rsqrt_53 = None
    decoder_block_9_layer_1_layer_norm_weight = getattr(getattr(self.decoder.block, "9").layer, "1").layer_norm.weight
    getattr_168 = decoder_block_9_layer_1_layer_norm_weight.dtype
    eq_114 = getattr_168 == torch.float16;  getattr_168 = None
    getattr_169 = decoder_block_9_layer_1_layer_norm_weight.dtype
    to_114 = mul_218.to(getattr_169);  mul_218 = getattr_169 = None
    mul_219 = decoder_block_9_layer_1_layer_norm_weight * to_114;  decoder_block_9_layer_1_layer_norm_weight = to_114 = None
    size_57 = mul_219.size()
    getitem_138 = size_57[slice(None, 2, None)];  size_57 = None
    getitem_139 = getitem_138[0]
    getitem_140 = getitem_138[1];  getitem_138 = None
    size_58 = encoder_dropout_1.size()
    getitem_141 = size_58[1];  size_58 = None
    decoder_block_9_layer_1_enc_dec_attention_q = getattr(getattr(self.decoder.block, "9").layer, "1").EncDecAttention.q(mul_219);  mul_219 = None
    view_126 = decoder_block_9_layer_1_enc_dec_attention_q.view(getitem_139, -1, 12, 64);  decoder_block_9_layer_1_enc_dec_attention_q = None
    transpose_155 = view_126.transpose(1, 2);  view_126 = None
    decoder_block_9_layer_1_enc_dec_attention_k = getattr(getattr(self.decoder.block, "9").layer, "1").EncDecAttention.k(encoder_dropout_1)
    view_127 = decoder_block_9_layer_1_enc_dec_attention_k.view(getitem_139, -1, 12, 64);  decoder_block_9_layer_1_enc_dec_attention_k = None
    transpose_156 = view_127.transpose(1, 2);  view_127 = None
    decoder_block_9_layer_1_enc_dec_attention_v = getattr(getattr(self.decoder.block, "9").layer, "1").EncDecAttention.v(encoder_dropout_1)
    view_128 = decoder_block_9_layer_1_enc_dec_attention_v.view(getitem_139, -1, 12, 64);  decoder_block_9_layer_1_enc_dec_attention_v = None
    transpose_157 = view_128.transpose(1, 2);  view_128 = None
    transpose_158 = transpose_156.transpose(3, 2)
    matmul_62 = torch.matmul(transpose_155, transpose_158);  transpose_155 = transpose_158 = None
    add_188 = matmul_62 + add_97;  matmul_62 = None
    float_34 = add_188.float()
    softmax_31 = torch.nn.functional.softmax(float_34, dim = -1, _stacklevel = 3, dtype = None);  float_34 = None
    type_as_31 = softmax_31.type_as(add_188);  softmax_31 = add_188 = None
    dropout_31 = torch.nn.functional.dropout(type_as_31, p = 0.1, training = False, inplace = False);  type_as_31 = None
    matmul_63 = torch.matmul(dropout_31, transpose_157);  dropout_31 = None
    transpose_159 = matmul_63.transpose(1, 2);  matmul_63 = None
    contiguous_31 = transpose_159.contiguous();  transpose_159 = None
    view_129 = contiguous_31.view(getitem_139, -1, 768);  contiguous_31 = getitem_139 = None
    decoder_block_9_layer_1_enc_dec_attention_o = getattr(getattr(self.decoder.block, "9").layer, "1").EncDecAttention.o(view_129);  view_129 = None
    decoder_block_9_layer_1_dropout = getattr(getattr(self.decoder.block, "9").layer, "1").dropout(decoder_block_9_layer_1_enc_dec_attention_o);  decoder_block_9_layer_1_enc_dec_attention_o = None
    add_189 = add_186 + decoder_block_9_layer_1_dropout;  add_186 = decoder_block_9_layer_1_dropout = None
    getattr_170 = add_189.dtype
    eq_115 = getattr_170 == torch.float16;  getattr_170 = None
    to_115 = add_189.to(torch.float32)
    pow_76 = to_115.pow(2);  to_115 = None
    mean_54 = pow_76.mean(-1, keepdim = True);  pow_76 = None
    add_190 = mean_54 + 1e-06;  mean_54 = None
    rsqrt_54 = torch.rsqrt(add_190);  add_190 = None
    mul_220 = add_189 * rsqrt_54;  rsqrt_54 = None
    decoder_block_9_layer_2_layer_norm_weight = getattr(getattr(self.decoder.block, "9").layer, "2").layer_norm.weight
    getattr_171 = decoder_block_9_layer_2_layer_norm_weight.dtype
    eq_116 = getattr_171 == torch.float16;  getattr_171 = None
    getattr_172 = decoder_block_9_layer_2_layer_norm_weight.dtype
    to_116 = mul_220.to(getattr_172);  mul_220 = getattr_172 = None
    mul_221 = decoder_block_9_layer_2_layer_norm_weight * to_116;  decoder_block_9_layer_2_layer_norm_weight = to_116 = None
    decoder_block_9_layer_2_dense_relu_dense_wi_0 = getattr(getattr(self.decoder.block, "9").layer, "2").DenseReluDense.wi_0(mul_221)
    mul_222 = 0.5 * decoder_block_9_layer_2_dense_relu_dense_wi_0
    pow_77 = torch.pow(decoder_block_9_layer_2_dense_relu_dense_wi_0, 3.0)
    mul_223 = 0.044715 * pow_77;  pow_77 = None
    add_191 = decoder_block_9_layer_2_dense_relu_dense_wi_0 + mul_223;  decoder_block_9_layer_2_dense_relu_dense_wi_0 = mul_223 = None
    mul_224 = 0.7978845608028654 * add_191;  add_191 = None
    tanh_21 = torch.tanh(mul_224);  mul_224 = None
    add_192 = 1.0 + tanh_21;  tanh_21 = None
    mul_225 = mul_222 * add_192;  mul_222 = add_192 = None
    decoder_block_9_layer_2_dense_relu_dense_wi_1 = getattr(getattr(self.decoder.block, "9").layer, "2").DenseReluDense.wi_1(mul_221);  mul_221 = None
    mul_226 = mul_225 * decoder_block_9_layer_2_dense_relu_dense_wi_1;  mul_225 = decoder_block_9_layer_2_dense_relu_dense_wi_1 = None
    decoder_block_9_layer_2_dense_relu_dense_dropout = getattr(getattr(self.decoder.block, "9").layer, "2").DenseReluDense.dropout(mul_226);  mul_226 = None
    decoder_block_9_layer_2_dense_relu_dense_wo_weight = getattr(getattr(self.decoder.block, "9").layer, "2").DenseReluDense.wo.weight
    decoder_block_9_layer_2_dense_relu_dense_wo = getattr(getattr(self.decoder.block, "9").layer, "2").DenseReluDense.wo(decoder_block_9_layer_2_dense_relu_dense_dropout);  decoder_block_9_layer_2_dense_relu_dense_dropout = None
    decoder_block_9_layer_2_dropout = getattr(getattr(self.decoder.block, "9").layer, "2").dropout(decoder_block_9_layer_2_dense_relu_dense_wo);  decoder_block_9_layer_2_dense_relu_dense_wo = None
    add_193 = add_189 + decoder_block_9_layer_2_dropout;  add_189 = decoder_block_9_layer_2_dropout = None
    getattr_173 = add_193.dtype
    eq_117 = getattr_173 == torch.float16;  getattr_173 = None
    to_117 = add_193.to(torch.float32)
    pow_78 = to_117.pow(2);  to_117 = None
    mean_55 = pow_78.mean(-1, keepdim = True);  pow_78 = None
    add_194 = mean_55 + 1e-06;  mean_55 = None
    rsqrt_55 = torch.rsqrt(add_194);  add_194 = None
    mul_227 = add_193 * rsqrt_55;  rsqrt_55 = None
    decoder_block_10_layer_0_layer_norm_weight = getattr(getattr(self.decoder.block, "10").layer, "0").layer_norm.weight
    getattr_174 = decoder_block_10_layer_0_layer_norm_weight.dtype
    eq_118 = getattr_174 == torch.float16;  getattr_174 = None
    getattr_175 = decoder_block_10_layer_0_layer_norm_weight.dtype
    to_118 = mul_227.to(getattr_175);  mul_227 = getattr_175 = None
    mul_228 = decoder_block_10_layer_0_layer_norm_weight * to_118;  decoder_block_10_layer_0_layer_norm_weight = to_118 = None
    size_59 = mul_228.size()
    getitem_142 = size_59[slice(None, 2, None)];  size_59 = None
    getitem_143 = getitem_142[0]
    getitem_144 = getitem_142[1];  getitem_142 = None
    decoder_block_10_layer_0_self_attention_q = getattr(getattr(self.decoder.block, "10").layer, "0").SelfAttention.q(mul_228)
    view_130 = decoder_block_10_layer_0_self_attention_q.view(getitem_143, -1, 12, 64);  decoder_block_10_layer_0_self_attention_q = None
    transpose_160 = view_130.transpose(1, 2);  view_130 = None
    decoder_block_10_layer_0_self_attention_k = getattr(getattr(self.decoder.block, "10").layer, "0").SelfAttention.k(mul_228)
    view_131 = decoder_block_10_layer_0_self_attention_k.view(getitem_143, -1, 12, 64);  decoder_block_10_layer_0_self_attention_k = None
    transpose_161 = view_131.transpose(1, 2);  view_131 = None
    decoder_block_10_layer_0_self_attention_v = getattr(getattr(self.decoder.block, "10").layer, "0").SelfAttention.v(mul_228);  mul_228 = None
    view_132 = decoder_block_10_layer_0_self_attention_v.view(getitem_143, -1, 12, 64);  decoder_block_10_layer_0_self_attention_v = None
    transpose_162 = view_132.transpose(1, 2);  view_132 = None
    transpose_163 = transpose_161.transpose(3, 2)
    matmul_64 = torch.matmul(transpose_160, transpose_163);  transpose_160 = transpose_163 = None
    add_195 = matmul_64 + add_93;  matmul_64 = None
    float_35 = add_195.float()
    softmax_32 = torch.nn.functional.softmax(float_35, dim = -1, _stacklevel = 3, dtype = None);  float_35 = None
    type_as_32 = softmax_32.type_as(add_195);  softmax_32 = add_195 = None
    dropout_32 = torch.nn.functional.dropout(type_as_32, p = 0.1, training = False, inplace = False);  type_as_32 = None
    matmul_65 = torch.matmul(dropout_32, transpose_162);  dropout_32 = None
    transpose_164 = matmul_65.transpose(1, 2);  matmul_65 = None
    contiguous_32 = transpose_164.contiguous();  transpose_164 = None
    view_133 = contiguous_32.view(getitem_143, -1, 768);  contiguous_32 = getitem_143 = None
    decoder_block_10_layer_0_self_attention_o = getattr(getattr(self.decoder.block, "10").layer, "0").SelfAttention.o(view_133);  view_133 = None
    decoder_block_10_layer_0_dropout = getattr(getattr(self.decoder.block, "10").layer, "0").dropout(decoder_block_10_layer_0_self_attention_o);  decoder_block_10_layer_0_self_attention_o = None
    add_196 = add_193 + decoder_block_10_layer_0_dropout;  add_193 = decoder_block_10_layer_0_dropout = None
    getattr_176 = add_196.dtype
    eq_119 = getattr_176 == torch.float16;  getattr_176 = None
    size_60 = transpose_161.size()
    getitem_145 = size_60[2];  size_60 = None
    to_119 = add_196.to(torch.float32)
    pow_79 = to_119.pow(2);  to_119 = None
    mean_56 = pow_79.mean(-1, keepdim = True);  pow_79 = None
    add_197 = mean_56 + 1e-06;  mean_56 = None
    rsqrt_56 = torch.rsqrt(add_197);  add_197 = None
    mul_229 = add_196 * rsqrt_56;  rsqrt_56 = None
    decoder_block_10_layer_1_layer_norm_weight = getattr(getattr(self.decoder.block, "10").layer, "1").layer_norm.weight
    getattr_177 = decoder_block_10_layer_1_layer_norm_weight.dtype
    eq_120 = getattr_177 == torch.float16;  getattr_177 = None
    getattr_178 = decoder_block_10_layer_1_layer_norm_weight.dtype
    to_120 = mul_229.to(getattr_178);  mul_229 = getattr_178 = None
    mul_230 = decoder_block_10_layer_1_layer_norm_weight * to_120;  decoder_block_10_layer_1_layer_norm_weight = to_120 = None
    size_61 = mul_230.size()
    getitem_146 = size_61[slice(None, 2, None)];  size_61 = None
    getitem_147 = getitem_146[0]
    getitem_148 = getitem_146[1];  getitem_146 = None
    size_62 = encoder_dropout_1.size()
    getitem_149 = size_62[1];  size_62 = None
    decoder_block_10_layer_1_enc_dec_attention_q = getattr(getattr(self.decoder.block, "10").layer, "1").EncDecAttention.q(mul_230);  mul_230 = None
    view_134 = decoder_block_10_layer_1_enc_dec_attention_q.view(getitem_147, -1, 12, 64);  decoder_block_10_layer_1_enc_dec_attention_q = None
    transpose_165 = view_134.transpose(1, 2);  view_134 = None
    decoder_block_10_layer_1_enc_dec_attention_k = getattr(getattr(self.decoder.block, "10").layer, "1").EncDecAttention.k(encoder_dropout_1)
    view_135 = decoder_block_10_layer_1_enc_dec_attention_k.view(getitem_147, -1, 12, 64);  decoder_block_10_layer_1_enc_dec_attention_k = None
    transpose_166 = view_135.transpose(1, 2);  view_135 = None
    decoder_block_10_layer_1_enc_dec_attention_v = getattr(getattr(self.decoder.block, "10").layer, "1").EncDecAttention.v(encoder_dropout_1)
    view_136 = decoder_block_10_layer_1_enc_dec_attention_v.view(getitem_147, -1, 12, 64);  decoder_block_10_layer_1_enc_dec_attention_v = None
    transpose_167 = view_136.transpose(1, 2);  view_136 = None
    transpose_168 = transpose_166.transpose(3, 2)
    matmul_66 = torch.matmul(transpose_165, transpose_168);  transpose_165 = transpose_168 = None
    add_198 = matmul_66 + add_97;  matmul_66 = None
    float_36 = add_198.float()
    softmax_33 = torch.nn.functional.softmax(float_36, dim = -1, _stacklevel = 3, dtype = None);  float_36 = None
    type_as_33 = softmax_33.type_as(add_198);  softmax_33 = add_198 = None
    dropout_33 = torch.nn.functional.dropout(type_as_33, p = 0.1, training = False, inplace = False);  type_as_33 = None
    matmul_67 = torch.matmul(dropout_33, transpose_167);  dropout_33 = None
    transpose_169 = matmul_67.transpose(1, 2);  matmul_67 = None
    contiguous_33 = transpose_169.contiguous();  transpose_169 = None
    view_137 = contiguous_33.view(getitem_147, -1, 768);  contiguous_33 = getitem_147 = None
    decoder_block_10_layer_1_enc_dec_attention_o = getattr(getattr(self.decoder.block, "10").layer, "1").EncDecAttention.o(view_137);  view_137 = None
    decoder_block_10_layer_1_dropout = getattr(getattr(self.decoder.block, "10").layer, "1").dropout(decoder_block_10_layer_1_enc_dec_attention_o);  decoder_block_10_layer_1_enc_dec_attention_o = None
    add_199 = add_196 + decoder_block_10_layer_1_dropout;  add_196 = decoder_block_10_layer_1_dropout = None
    getattr_179 = add_199.dtype
    eq_121 = getattr_179 == torch.float16;  getattr_179 = None
    to_121 = add_199.to(torch.float32)
    pow_80 = to_121.pow(2);  to_121 = None
    mean_57 = pow_80.mean(-1, keepdim = True);  pow_80 = None
    add_200 = mean_57 + 1e-06;  mean_57 = None
    rsqrt_57 = torch.rsqrt(add_200);  add_200 = None
    mul_231 = add_199 * rsqrt_57;  rsqrt_57 = None
    decoder_block_10_layer_2_layer_norm_weight = getattr(getattr(self.decoder.block, "10").layer, "2").layer_norm.weight
    getattr_180 = decoder_block_10_layer_2_layer_norm_weight.dtype
    eq_122 = getattr_180 == torch.float16;  getattr_180 = None
    getattr_181 = decoder_block_10_layer_2_layer_norm_weight.dtype
    to_122 = mul_231.to(getattr_181);  mul_231 = getattr_181 = None
    mul_232 = decoder_block_10_layer_2_layer_norm_weight * to_122;  decoder_block_10_layer_2_layer_norm_weight = to_122 = None
    decoder_block_10_layer_2_dense_relu_dense_wi_0 = getattr(getattr(self.decoder.block, "10").layer, "2").DenseReluDense.wi_0(mul_232)
    mul_233 = 0.5 * decoder_block_10_layer_2_dense_relu_dense_wi_0
    pow_81 = torch.pow(decoder_block_10_layer_2_dense_relu_dense_wi_0, 3.0)
    mul_234 = 0.044715 * pow_81;  pow_81 = None
    add_201 = decoder_block_10_layer_2_dense_relu_dense_wi_0 + mul_234;  decoder_block_10_layer_2_dense_relu_dense_wi_0 = mul_234 = None
    mul_235 = 0.7978845608028654 * add_201;  add_201 = None
    tanh_22 = torch.tanh(mul_235);  mul_235 = None
    add_202 = 1.0 + tanh_22;  tanh_22 = None
    mul_236 = mul_233 * add_202;  mul_233 = add_202 = None
    decoder_block_10_layer_2_dense_relu_dense_wi_1 = getattr(getattr(self.decoder.block, "10").layer, "2").DenseReluDense.wi_1(mul_232);  mul_232 = None
    mul_237 = mul_236 * decoder_block_10_layer_2_dense_relu_dense_wi_1;  mul_236 = decoder_block_10_layer_2_dense_relu_dense_wi_1 = None
    decoder_block_10_layer_2_dense_relu_dense_dropout = getattr(getattr(self.decoder.block, "10").layer, "2").DenseReluDense.dropout(mul_237);  mul_237 = None
    decoder_block_10_layer_2_dense_relu_dense_wo_weight = getattr(getattr(self.decoder.block, "10").layer, "2").DenseReluDense.wo.weight
    decoder_block_10_layer_2_dense_relu_dense_wo = getattr(getattr(self.decoder.block, "10").layer, "2").DenseReluDense.wo(decoder_block_10_layer_2_dense_relu_dense_dropout);  decoder_block_10_layer_2_dense_relu_dense_dropout = None
    decoder_block_10_layer_2_dropout = getattr(getattr(self.decoder.block, "10").layer, "2").dropout(decoder_block_10_layer_2_dense_relu_dense_wo);  decoder_block_10_layer_2_dense_relu_dense_wo = None
    add_203 = add_199 + decoder_block_10_layer_2_dropout;  add_199 = decoder_block_10_layer_2_dropout = None
    getattr_182 = add_203.dtype
    eq_123 = getattr_182 == torch.float16;  getattr_182 = None
    to_123 = add_203.to(torch.float32)
    pow_82 = to_123.pow(2);  to_123 = None
    mean_58 = pow_82.mean(-1, keepdim = True);  pow_82 = None
    add_204 = mean_58 + 1e-06;  mean_58 = None
    rsqrt_58 = torch.rsqrt(add_204);  add_204 = None
    mul_238 = add_203 * rsqrt_58;  rsqrt_58 = None
    decoder_block_11_layer_0_layer_norm_weight = getattr(getattr(self.decoder.block, "11").layer, "0").layer_norm.weight
    getattr_183 = decoder_block_11_layer_0_layer_norm_weight.dtype
    eq_124 = getattr_183 == torch.float16;  getattr_183 = None
    getattr_184 = decoder_block_11_layer_0_layer_norm_weight.dtype
    to_124 = mul_238.to(getattr_184);  mul_238 = getattr_184 = None
    mul_239 = decoder_block_11_layer_0_layer_norm_weight * to_124;  decoder_block_11_layer_0_layer_norm_weight = to_124 = None
    size_63 = mul_239.size()
    getitem_150 = size_63[slice(None, 2, None)];  size_63 = None
    getitem_151 = getitem_150[0]
    getitem_152 = getitem_150[1];  getitem_150 = None
    decoder_block_11_layer_0_self_attention_q = getattr(getattr(self.decoder.block, "11").layer, "0").SelfAttention.q(mul_239)
    view_138 = decoder_block_11_layer_0_self_attention_q.view(getitem_151, -1, 12, 64);  decoder_block_11_layer_0_self_attention_q = None
    transpose_170 = view_138.transpose(1, 2);  view_138 = None
    decoder_block_11_layer_0_self_attention_k = getattr(getattr(self.decoder.block, "11").layer, "0").SelfAttention.k(mul_239)
    view_139 = decoder_block_11_layer_0_self_attention_k.view(getitem_151, -1, 12, 64);  decoder_block_11_layer_0_self_attention_k = None
    transpose_171 = view_139.transpose(1, 2);  view_139 = None
    decoder_block_11_layer_0_self_attention_v = getattr(getattr(self.decoder.block, "11").layer, "0").SelfAttention.v(mul_239);  mul_239 = None
    view_140 = decoder_block_11_layer_0_self_attention_v.view(getitem_151, -1, 12, 64);  decoder_block_11_layer_0_self_attention_v = None
    transpose_172 = view_140.transpose(1, 2);  view_140 = None
    transpose_173 = transpose_171.transpose(3, 2)
    matmul_68 = torch.matmul(transpose_170, transpose_173);  transpose_170 = transpose_173 = None
    add_205 = matmul_68 + add_93;  matmul_68 = add_93 = None
    float_37 = add_205.float()
    softmax_34 = torch.nn.functional.softmax(float_37, dim = -1, _stacklevel = 3, dtype = None);  float_37 = None
    type_as_34 = softmax_34.type_as(add_205);  softmax_34 = add_205 = None
    dropout_34 = torch.nn.functional.dropout(type_as_34, p = 0.1, training = False, inplace = False);  type_as_34 = None
    matmul_69 = torch.matmul(dropout_34, transpose_172);  dropout_34 = None
    transpose_174 = matmul_69.transpose(1, 2);  matmul_69 = None
    contiguous_34 = transpose_174.contiguous();  transpose_174 = None
    view_141 = contiguous_34.view(getitem_151, -1, 768);  contiguous_34 = getitem_151 = None
    decoder_block_11_layer_0_self_attention_o = getattr(getattr(self.decoder.block, "11").layer, "0").SelfAttention.o(view_141);  view_141 = None
    decoder_block_11_layer_0_dropout = getattr(getattr(self.decoder.block, "11").layer, "0").dropout(decoder_block_11_layer_0_self_attention_o);  decoder_block_11_layer_0_self_attention_o = None
    add_206 = add_203 + decoder_block_11_layer_0_dropout;  add_203 = decoder_block_11_layer_0_dropout = None
    getattr_185 = add_206.dtype
    eq_125 = getattr_185 == torch.float16;  getattr_185 = None
    size_64 = transpose_171.size()
    getitem_153 = size_64[2];  size_64 = None
    to_125 = add_206.to(torch.float32)
    pow_83 = to_125.pow(2);  to_125 = None
    mean_59 = pow_83.mean(-1, keepdim = True);  pow_83 = None
    add_207 = mean_59 + 1e-06;  mean_59 = None
    rsqrt_59 = torch.rsqrt(add_207);  add_207 = None
    mul_240 = add_206 * rsqrt_59;  rsqrt_59 = None
    decoder_block_11_layer_1_layer_norm_weight = getattr(getattr(self.decoder.block, "11").layer, "1").layer_norm.weight
    getattr_186 = decoder_block_11_layer_1_layer_norm_weight.dtype
    eq_126 = getattr_186 == torch.float16;  getattr_186 = None
    getattr_187 = decoder_block_11_layer_1_layer_norm_weight.dtype
    to_126 = mul_240.to(getattr_187);  mul_240 = getattr_187 = None
    mul_241 = decoder_block_11_layer_1_layer_norm_weight * to_126;  decoder_block_11_layer_1_layer_norm_weight = to_126 = None
    size_65 = mul_241.size()
    getitem_154 = size_65[slice(None, 2, None)];  size_65 = None
    getitem_155 = getitem_154[0]
    getitem_156 = getitem_154[1];  getitem_154 = None
    size_66 = encoder_dropout_1.size()
    getitem_157 = size_66[1];  size_66 = None
    decoder_block_11_layer_1_enc_dec_attention_q = getattr(getattr(self.decoder.block, "11").layer, "1").EncDecAttention.q(mul_241);  mul_241 = None
    view_142 = decoder_block_11_layer_1_enc_dec_attention_q.view(getitem_155, -1, 12, 64);  decoder_block_11_layer_1_enc_dec_attention_q = None
    transpose_175 = view_142.transpose(1, 2);  view_142 = None
    decoder_block_11_layer_1_enc_dec_attention_k = getattr(getattr(self.decoder.block, "11").layer, "1").EncDecAttention.k(encoder_dropout_1)
    view_143 = decoder_block_11_layer_1_enc_dec_attention_k.view(getitem_155, -1, 12, 64);  decoder_block_11_layer_1_enc_dec_attention_k = None
    transpose_176 = view_143.transpose(1, 2);  view_143 = None
    decoder_block_11_layer_1_enc_dec_attention_v = getattr(getattr(self.decoder.block, "11").layer, "1").EncDecAttention.v(encoder_dropout_1)
    view_144 = decoder_block_11_layer_1_enc_dec_attention_v.view(getitem_155, -1, 12, 64);  decoder_block_11_layer_1_enc_dec_attention_v = None
    transpose_177 = view_144.transpose(1, 2);  view_144 = None
    transpose_178 = transpose_176.transpose(3, 2)
    matmul_70 = torch.matmul(transpose_175, transpose_178);  transpose_175 = transpose_178 = None
    add_208 = matmul_70 + add_97;  matmul_70 = add_97 = None
    float_38 = add_208.float()
    softmax_35 = torch.nn.functional.softmax(float_38, dim = -1, _stacklevel = 3, dtype = None);  float_38 = None
    type_as_35 = softmax_35.type_as(add_208);  softmax_35 = add_208 = None
    dropout_35 = torch.nn.functional.dropout(type_as_35, p = 0.1, training = False, inplace = False);  type_as_35 = None
    matmul_71 = torch.matmul(dropout_35, transpose_177);  dropout_35 = None
    transpose_179 = matmul_71.transpose(1, 2);  matmul_71 = None
    contiguous_35 = transpose_179.contiguous();  transpose_179 = None
    view_145 = contiguous_35.view(getitem_155, -1, 768);  contiguous_35 = getitem_155 = None
    decoder_block_11_layer_1_enc_dec_attention_o = getattr(getattr(self.decoder.block, "11").layer, "1").EncDecAttention.o(view_145);  view_145 = None
    decoder_block_11_layer_1_dropout = getattr(getattr(self.decoder.block, "11").layer, "1").dropout(decoder_block_11_layer_1_enc_dec_attention_o);  decoder_block_11_layer_1_enc_dec_attention_o = None
    add_209 = add_206 + decoder_block_11_layer_1_dropout;  add_206 = decoder_block_11_layer_1_dropout = None
    getattr_188 = add_209.dtype
    eq_127 = getattr_188 == torch.float16;  getattr_188 = None
    to_127 = add_209.to(torch.float32)
    pow_84 = to_127.pow(2);  to_127 = None
    mean_60 = pow_84.mean(-1, keepdim = True);  pow_84 = None
    add_210 = mean_60 + 1e-06;  mean_60 = None
    rsqrt_60 = torch.rsqrt(add_210);  add_210 = None
    mul_242 = add_209 * rsqrt_60;  rsqrt_60 = None
    decoder_block_11_layer_2_layer_norm_weight = getattr(getattr(self.decoder.block, "11").layer, "2").layer_norm.weight
    getattr_189 = decoder_block_11_layer_2_layer_norm_weight.dtype
    eq_128 = getattr_189 == torch.float16;  getattr_189 = None
    getattr_190 = decoder_block_11_layer_2_layer_norm_weight.dtype
    to_128 = mul_242.to(getattr_190);  mul_242 = getattr_190 = None
    mul_243 = decoder_block_11_layer_2_layer_norm_weight * to_128;  decoder_block_11_layer_2_layer_norm_weight = to_128 = None
    decoder_block_11_layer_2_dense_relu_dense_wi_0 = getattr(getattr(self.decoder.block, "11").layer, "2").DenseReluDense.wi_0(mul_243)
    mul_244 = 0.5 * decoder_block_11_layer_2_dense_relu_dense_wi_0
    pow_85 = torch.pow(decoder_block_11_layer_2_dense_relu_dense_wi_0, 3.0)
    mul_245 = 0.044715 * pow_85;  pow_85 = None
    add_211 = decoder_block_11_layer_2_dense_relu_dense_wi_0 + mul_245;  decoder_block_11_layer_2_dense_relu_dense_wi_0 = mul_245 = None
    mul_246 = 0.7978845608028654 * add_211;  add_211 = None
    tanh_23 = torch.tanh(mul_246);  mul_246 = None
    add_212 = 1.0 + tanh_23;  tanh_23 = None
    mul_247 = mul_244 * add_212;  mul_244 = add_212 = None
    decoder_block_11_layer_2_dense_relu_dense_wi_1 = getattr(getattr(self.decoder.block, "11").layer, "2").DenseReluDense.wi_1(mul_243);  mul_243 = None
    mul_248 = mul_247 * decoder_block_11_layer_2_dense_relu_dense_wi_1;  mul_247 = decoder_block_11_layer_2_dense_relu_dense_wi_1 = None
    decoder_block_11_layer_2_dense_relu_dense_dropout = getattr(getattr(self.decoder.block, "11").layer, "2").DenseReluDense.dropout(mul_248);  mul_248 = None
    decoder_block_11_layer_2_dense_relu_dense_wo_weight = getattr(getattr(self.decoder.block, "11").layer, "2").DenseReluDense.wo.weight
    decoder_block_11_layer_2_dense_relu_dense_wo = getattr(getattr(self.decoder.block, "11").layer, "2").DenseReluDense.wo(decoder_block_11_layer_2_dense_relu_dense_dropout);  decoder_block_11_layer_2_dense_relu_dense_dropout = None
    decoder_block_11_layer_2_dropout = getattr(getattr(self.decoder.block, "11").layer, "2").dropout(decoder_block_11_layer_2_dense_relu_dense_wo);  decoder_block_11_layer_2_dense_relu_dense_wo = None
    add_213 = add_209 + decoder_block_11_layer_2_dropout;  add_209 = decoder_block_11_layer_2_dropout = None
    getattr_191 = add_213.dtype
    eq_129 = getattr_191 == torch.float16;  getattr_191 = None
    to_129 = add_213.to(torch.float32)
    pow_86 = to_129.pow(2);  to_129 = None
    mean_61 = pow_86.mean(-1, keepdim = True);  pow_86 = None
    add_214 = mean_61 + 1e-06;  mean_61 = None
    rsqrt_61 = torch.rsqrt(add_214);  add_214 = None
    mul_249 = add_213 * rsqrt_61;  add_213 = rsqrt_61 = None
    decoder_final_layer_norm_weight = self.decoder.final_layer_norm.weight
    getattr_192 = decoder_final_layer_norm_weight.dtype
    eq_130 = getattr_192 == torch.float16;  getattr_192 = None
    getattr_193 = decoder_final_layer_norm_weight.dtype
    to_130 = mul_249.to(getattr_193);  mul_249 = getattr_193 = None
    mul_250 = decoder_final_layer_norm_weight * to_130;  decoder_final_layer_norm_weight = to_130 = None
    decoder_dropout_1 = self.decoder.dropout(mul_250);  mul_250 = None
    lm_head = self.lm_head(decoder_dropout_1);  decoder_dropout_1 = None
    getattr_194 = lm_head.device
    to_131 = labels.to(getattr_194);  labels = getattr_194 = None
    size_67 = lm_head.size(-1)
    view_146 = lm_head.view(-1, size_67);  size_67 = None
    view_147 = to_131.view(-1);  to_131 = None
    crossentropyloss_0 = self.crossentropyloss_0(view_146, view_147);  view_146 = view_147 = None
    return {'loss': crossentropyloss_0, 'logits': lm_head, 'past_key_values': ((transpose_61, transpose_62, transpose_66, transpose_67), (transpose_71, transpose_72, transpose_76, transpose_77), (transpose_81, transpose_82, transpose_86, transpose_87), (transpose_91, transpose_92, transpose_96, transpose_97), (transpose_101, transpose_102, transpose_106, transpose_107), (transpose_111, transpose_112, transpose_116, transpose_117), (transpose_121, transpose_122, transpose_126, transpose_127), (transpose_131, transpose_132, transpose_136, transpose_137), (transpose_141, transpose_142, transpose_146, transpose_147), (transpose_151, transpose_152, transpose_156, transpose_157), (transpose_161, transpose_162, transpose_166, transpose_167), (transpose_171, transpose_172, transpose_176, transpose_177)), 'encoder_last_hidden_state': encoder_dropout_1}
    
********************************************************************************************************************************************************************************************************

torch.fx._symbolic_trace.wrap("patch_linear_layer_linear_layer_triton_wrapper")

def forward(self, input_ids : torch.Tensor, labels : torch.Tensor):
    size = input_ids.size()
    getitem = size[-1]
    view = input_ids.view(-1, getitem);  input_ids = getitem = None
    shared = self.shared(view);  view = None
    getitem_1 = size[0]
    getitem_2 = size[1];  size = None
    getattr_1 = shared.device
    ones = torch.ones(getitem_1, getitem_2, device = getattr_1);  getitem_1 = getitem_2 = getattr_1 = None
    dim = ones.dim()
    eq = dim == 2;  dim = None
    dim_1 = ones.dim()
    eq_1 = dim_1 == 3;  dim_1 = None
    dim_2 = ones.dim()
    eq_2 = dim_2 == 2;  dim_2 = None
    getitem_3 = ones[(slice(None, None, None), None, None, slice(None, None, None))];  ones = None
    to = getitem_3.to(dtype = torch.float16);  getitem_3 = None
    sub = 1.0 - to;  to = None
    mul = sub * -65504.0;  sub = None
    encoder_dropout = self.encoder.dropout(shared);  shared = None
    to_1 = encoder_dropout.to(torch.float32)
    pow_1 = to_1.pow(2);  to_1 = None
    mean = pow_1.mean(-1, keepdim = True);  pow_1 = None
    add = mean + 1e-06;  mean = None
    rsqrt = torch.rsqrt(add);  add = None
    mul_1 = encoder_dropout * rsqrt;  rsqrt = None
    encoder_block_0_layer_0_layer_norm_weight = getattr(getattr(self.encoder.block, "0").layer, "0").layer_norm.weight
    getattr_2 = encoder_block_0_layer_0_layer_norm_weight.dtype
    eq_3 = getattr_2 == torch.float16;  getattr_2 = None
    getattr_3 = encoder_block_0_layer_0_layer_norm_weight.dtype
    to_2 = mul_1.to(getattr_3);  mul_1 = getattr_3 = None
    mul_2 = encoder_block_0_layer_0_layer_norm_weight * to_2;  encoder_block_0_layer_0_layer_norm_weight = to_2 = None
    size_1 = mul_2.size()
    getitem_4 = size_1[slice(None, 2, None)];  size_1 = None
    getitem_5 = getitem_4[0]
    getitem_6 = getitem_4[1];  getitem_4 = None
    encoder_block_0_layer_0_self_attention_q = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.q(mul_2)
    view_1 = encoder_block_0_layer_0_self_attention_q.view(getitem_5, -1, 12, 64);  encoder_block_0_layer_0_self_attention_q = None
    transpose = view_1.transpose(1, 2);  view_1 = None
    encoder_block_0_layer_0_self_attention_k = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.k(mul_2)
    view_2 = encoder_block_0_layer_0_self_attention_k.view(getitem_5, -1, 12, 64);  encoder_block_0_layer_0_self_attention_k = None
    transpose_1 = view_2.transpose(1, 2);  view_2 = None
    encoder_block_0_layer_0_self_attention_v = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.v(mul_2);  mul_2 = None
    view_3 = encoder_block_0_layer_0_self_attention_v.view(getitem_5, -1, 12, 64);  encoder_block_0_layer_0_self_attention_v = None
    transpose_2 = view_3.transpose(1, 2);  view_3 = None
    transpose_3 = transpose_1.transpose(3, 2);  transpose_1 = None
    matmul = torch.matmul(transpose, transpose_3);  transpose = transpose_3 = None
    getattr_4 = matmul.device
    arange = torch.arange(getitem_6, dtype = torch.int64, device = getattr_4)
    getitem_7 = arange[(slice(None, None, None), None)];  arange = None
    arange_1 = torch.arange(getitem_6, dtype = torch.int64, device = getattr_4);  getitem_6 = getattr_4 = None
    getitem_8 = arange_1[(None, slice(None, None, None))];  arange_1 = None
    sub_1 = getitem_8 - getitem_7;  getitem_8 = getitem_7 = None
    gt = sub_1 > 0
    to_3 = gt.to(torch.int64);  gt = None
    mul_3 = to_3 * 16;  to_3 = None
    add_1 = 0 + mul_3;  mul_3 = None
    abs_1 = torch.abs(sub_1);  sub_1 = None
    lt = abs_1 < 8
    float_1 = abs_1.float()
    truediv = float_1 / 8;  float_1 = None
    log = torch.log(truediv);  truediv = None
    truediv_1 = log / 2.772588722239781;  log = None
    mul_4 = truediv_1 * 8;  truediv_1 = None
    to_4 = mul_4.to(torch.int64);  mul_4 = None
    add_2 = 8 + to_4;  to_4 = None
    full_like = torch.full_like(add_2, 15)
    min_1 = torch.min(add_2, full_like);  add_2 = full_like = None
    where = torch.where(lt, abs_1, min_1);  lt = abs_1 = min_1 = None
    add_3 = add_1 + where;  add_1 = where = None
    encoder_block_0_layer_0_self_attention_relative_attention_bias = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.relative_attention_bias(add_3);  add_3 = None
    permute = encoder_block_0_layer_0_self_attention_relative_attention_bias.permute([2, 0, 1]);  encoder_block_0_layer_0_self_attention_relative_attention_bias = None
    unsqueeze = permute.unsqueeze(0);  permute = None
    add_4 = unsqueeze + mul;  unsqueeze = mul = None
    add_5 = matmul + add_4;  matmul = None
    float_2 = add_5.float()
    softmax = torch.nn.functional.softmax(float_2, dim = -1, _stacklevel = 3, dtype = None);  float_2 = None
    type_as = softmax.type_as(add_5);  softmax = add_5 = None
    dropout = torch.nn.functional.dropout(type_as, p = 0.1, training = False, inplace = False);  type_as = None
    matmul_1 = torch.matmul(dropout, transpose_2);  dropout = transpose_2 = None
    transpose_4 = matmul_1.transpose(1, 2);  matmul_1 = None
    contiguous = transpose_4.contiguous();  transpose_4 = None
    view_4 = contiguous.view(getitem_5, -1, 768);  contiguous = getitem_5 = None
    linear = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper = patch_linear_layer_linear_layer_triton_wrapper(view_4, linear);  view_4 = linear = None
    encoder_block_0_layer_0_dropout = getattr(getattr(self.encoder.block, "0").layer, "0").dropout(linear_layer_triton_wrapper);  linear_layer_triton_wrapper = None
    add_6 = encoder_dropout + encoder_block_0_layer_0_dropout;  encoder_dropout = encoder_block_0_layer_0_dropout = None
    getattr_5 = add_6.dtype
    eq_4 = getattr_5 == torch.float16;  getattr_5 = None
    to_5 = add_6.to(torch.float32)
    pow_2 = to_5.pow(2);  to_5 = None
    mean_1 = pow_2.mean(-1, keepdim = True);  pow_2 = None
    add_7 = mean_1 + 1e-06;  mean_1 = None
    rsqrt_1 = torch.rsqrt(add_7);  add_7 = None
    mul_5 = add_6 * rsqrt_1;  rsqrt_1 = None
    encoder_block_0_layer_1_layer_norm_weight = getattr(getattr(self.encoder.block, "0").layer, "1").layer_norm.weight
    getattr_6 = encoder_block_0_layer_1_layer_norm_weight.dtype
    eq_5 = getattr_6 == torch.float16;  getattr_6 = None
    getattr_7 = encoder_block_0_layer_1_layer_norm_weight.dtype
    to_6 = mul_5.to(getattr_7);  mul_5 = getattr_7 = None
    mul_6 = encoder_block_0_layer_1_layer_norm_weight * to_6;  encoder_block_0_layer_1_layer_norm_weight = to_6 = None
    encoder_block_0_layer_1_dense_relu_dense_wi_0 = getattr(getattr(self.encoder.block, "0").layer, "1").DenseReluDense.wi_0(mul_6)
    mul_7 = 0.5 * encoder_block_0_layer_1_dense_relu_dense_wi_0
    pow_3 = torch.pow(encoder_block_0_layer_1_dense_relu_dense_wi_0, 3.0)
    mul_8 = 0.044715 * pow_3;  pow_3 = None
    add_8 = encoder_block_0_layer_1_dense_relu_dense_wi_0 + mul_8;  encoder_block_0_layer_1_dense_relu_dense_wi_0 = mul_8 = None
    mul_9 = 0.7978845608028654 * add_8;  add_8 = None
    tanh = torch.tanh(mul_9);  mul_9 = None
    add_9 = 1.0 + tanh;  tanh = None
    mul_10 = mul_7 * add_9;  mul_7 = add_9 = None
    encoder_block_0_layer_1_dense_relu_dense_wi_1 = getattr(getattr(self.encoder.block, "0").layer, "1").DenseReluDense.wi_1(mul_6);  mul_6 = None
    mul_11 = mul_10 * encoder_block_0_layer_1_dense_relu_dense_wi_1;  mul_10 = encoder_block_0_layer_1_dense_relu_dense_wi_1 = None
    encoder_block_0_layer_1_dense_relu_dense_dropout = getattr(getattr(self.encoder.block, "0").layer, "1").DenseReluDense.dropout(mul_11);  mul_11 = None
    encoder_block_0_layer_1_dense_relu_dense_wo_weight = getattr(getattr(self.encoder.block, "0").layer, "1").DenseReluDense.wo.weight
    linear_1 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_1 = patch_linear_layer_linear_layer_triton_wrapper(encoder_block_0_layer_1_dense_relu_dense_dropout, linear_1);  encoder_block_0_layer_1_dense_relu_dense_dropout = linear_1 = None
    encoder_block_0_layer_1_dropout = getattr(getattr(self.encoder.block, "0").layer, "1").dropout(linear_layer_triton_wrapper_1);  linear_layer_triton_wrapper_1 = None
    add_10 = add_6 + encoder_block_0_layer_1_dropout;  add_6 = encoder_block_0_layer_1_dropout = None
    getattr_8 = add_10.dtype
    eq_6 = getattr_8 == torch.float16;  getattr_8 = None
    to_7 = add_10.to(torch.float32)
    pow_4 = to_7.pow(2);  to_7 = None
    mean_2 = pow_4.mean(-1, keepdim = True);  pow_4 = None
    add_11 = mean_2 + 1e-06;  mean_2 = None
    rsqrt_2 = torch.rsqrt(add_11);  add_11 = None
    mul_12 = add_10 * rsqrt_2;  rsqrt_2 = None
    encoder_block_1_layer_0_layer_norm_weight = getattr(getattr(self.encoder.block, "1").layer, "0").layer_norm.weight
    getattr_9 = encoder_block_1_layer_0_layer_norm_weight.dtype
    eq_7 = getattr_9 == torch.float16;  getattr_9 = None
    getattr_10 = encoder_block_1_layer_0_layer_norm_weight.dtype
    to_8 = mul_12.to(getattr_10);  mul_12 = getattr_10 = None
    mul_13 = encoder_block_1_layer_0_layer_norm_weight * to_8;  encoder_block_1_layer_0_layer_norm_weight = to_8 = None
    size_2 = mul_13.size()
    getitem_9 = size_2[slice(None, 2, None)];  size_2 = None
    getitem_10 = getitem_9[0]
    getitem_11 = getitem_9[1];  getitem_9 = None
    encoder_block_1_layer_0_self_attention_q = getattr(getattr(self.encoder.block, "1").layer, "0").SelfAttention.q(mul_13)
    view_5 = encoder_block_1_layer_0_self_attention_q.view(getitem_10, -1, 12, 64);  encoder_block_1_layer_0_self_attention_q = None
    transpose_5 = view_5.transpose(1, 2);  view_5 = None
    encoder_block_1_layer_0_self_attention_k = getattr(getattr(self.encoder.block, "1").layer, "0").SelfAttention.k(mul_13)
    view_6 = encoder_block_1_layer_0_self_attention_k.view(getitem_10, -1, 12, 64);  encoder_block_1_layer_0_self_attention_k = None
    transpose_6 = view_6.transpose(1, 2);  view_6 = None
    encoder_block_1_layer_0_self_attention_v = getattr(getattr(self.encoder.block, "1").layer, "0").SelfAttention.v(mul_13);  mul_13 = None
    view_7 = encoder_block_1_layer_0_self_attention_v.view(getitem_10, -1, 12, 64);  encoder_block_1_layer_0_self_attention_v = None
    transpose_7 = view_7.transpose(1, 2);  view_7 = None
    transpose_8 = transpose_6.transpose(3, 2);  transpose_6 = None
    matmul_2 = torch.matmul(transpose_5, transpose_8);  transpose_5 = transpose_8 = None
    add_12 = matmul_2 + add_4;  matmul_2 = None
    float_3 = add_12.float()
    softmax_1 = torch.nn.functional.softmax(float_3, dim = -1, _stacklevel = 3, dtype = None);  float_3 = None
    type_as_1 = softmax_1.type_as(add_12);  softmax_1 = add_12 = None
    dropout_1 = torch.nn.functional.dropout(type_as_1, p = 0.1, training = False, inplace = False);  type_as_1 = None
    matmul_3 = torch.matmul(dropout_1, transpose_7);  dropout_1 = transpose_7 = None
    transpose_9 = matmul_3.transpose(1, 2);  matmul_3 = None
    contiguous_1 = transpose_9.contiguous();  transpose_9 = None
    view_8 = contiguous_1.view(getitem_10, -1, 768);  contiguous_1 = getitem_10 = None
    linear_2 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_2 = patch_linear_layer_linear_layer_triton_wrapper(view_8, linear_2);  view_8 = linear_2 = None
    encoder_block_1_layer_0_dropout = getattr(getattr(self.encoder.block, "1").layer, "0").dropout(linear_layer_triton_wrapper_2);  linear_layer_triton_wrapper_2 = None
    add_13 = add_10 + encoder_block_1_layer_0_dropout;  add_10 = encoder_block_1_layer_0_dropout = None
    getattr_11 = add_13.dtype
    eq_8 = getattr_11 == torch.float16;  getattr_11 = None
    to_9 = add_13.to(torch.float32)
    pow_5 = to_9.pow(2);  to_9 = None
    mean_3 = pow_5.mean(-1, keepdim = True);  pow_5 = None
    add_14 = mean_3 + 1e-06;  mean_3 = None
    rsqrt_3 = torch.rsqrt(add_14);  add_14 = None
    mul_14 = add_13 * rsqrt_3;  rsqrt_3 = None
    encoder_block_1_layer_1_layer_norm_weight = getattr(getattr(self.encoder.block, "1").layer, "1").layer_norm.weight
    getattr_12 = encoder_block_1_layer_1_layer_norm_weight.dtype
    eq_9 = getattr_12 == torch.float16;  getattr_12 = None
    getattr_13 = encoder_block_1_layer_1_layer_norm_weight.dtype
    to_10 = mul_14.to(getattr_13);  mul_14 = getattr_13 = None
    mul_15 = encoder_block_1_layer_1_layer_norm_weight * to_10;  encoder_block_1_layer_1_layer_norm_weight = to_10 = None
    encoder_block_1_layer_1_dense_relu_dense_wi_0 = getattr(getattr(self.encoder.block, "1").layer, "1").DenseReluDense.wi_0(mul_15)
    mul_16 = 0.5 * encoder_block_1_layer_1_dense_relu_dense_wi_0
    pow_6 = torch.pow(encoder_block_1_layer_1_dense_relu_dense_wi_0, 3.0)
    mul_17 = 0.044715 * pow_6;  pow_6 = None
    add_15 = encoder_block_1_layer_1_dense_relu_dense_wi_0 + mul_17;  encoder_block_1_layer_1_dense_relu_dense_wi_0 = mul_17 = None
    mul_18 = 0.7978845608028654 * add_15;  add_15 = None
    tanh_1 = torch.tanh(mul_18);  mul_18 = None
    add_16 = 1.0 + tanh_1;  tanh_1 = None
    mul_19 = mul_16 * add_16;  mul_16 = add_16 = None
    encoder_block_1_layer_1_dense_relu_dense_wi_1 = getattr(getattr(self.encoder.block, "1").layer, "1").DenseReluDense.wi_1(mul_15);  mul_15 = None
    mul_20 = mul_19 * encoder_block_1_layer_1_dense_relu_dense_wi_1;  mul_19 = encoder_block_1_layer_1_dense_relu_dense_wi_1 = None
    encoder_block_1_layer_1_dense_relu_dense_dropout = getattr(getattr(self.encoder.block, "1").layer, "1").DenseReluDense.dropout(mul_20);  mul_20 = None
    encoder_block_1_layer_1_dense_relu_dense_wo_weight = getattr(getattr(self.encoder.block, "1").layer, "1").DenseReluDense.wo.weight
    linear_3 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_3 = patch_linear_layer_linear_layer_triton_wrapper(encoder_block_1_layer_1_dense_relu_dense_dropout, linear_3);  encoder_block_1_layer_1_dense_relu_dense_dropout = linear_3 = None
    encoder_block_1_layer_1_dropout = getattr(getattr(self.encoder.block, "1").layer, "1").dropout(linear_layer_triton_wrapper_3);  linear_layer_triton_wrapper_3 = None
    add_17 = add_13 + encoder_block_1_layer_1_dropout;  add_13 = encoder_block_1_layer_1_dropout = None
    getattr_14 = add_17.dtype
    eq_10 = getattr_14 == torch.float16;  getattr_14 = None
    to_11 = add_17.to(torch.float32)
    pow_7 = to_11.pow(2);  to_11 = None
    mean_4 = pow_7.mean(-1, keepdim = True);  pow_7 = None
    add_18 = mean_4 + 1e-06;  mean_4 = None
    rsqrt_4 = torch.rsqrt(add_18);  add_18 = None
    mul_21 = add_17 * rsqrt_4;  rsqrt_4 = None
    encoder_block_2_layer_0_layer_norm_weight = getattr(getattr(self.encoder.block, "2").layer, "0").layer_norm.weight
    getattr_15 = encoder_block_2_layer_0_layer_norm_weight.dtype
    eq_11 = getattr_15 == torch.float16;  getattr_15 = None
    getattr_16 = encoder_block_2_layer_0_layer_norm_weight.dtype
    to_12 = mul_21.to(getattr_16);  mul_21 = getattr_16 = None
    mul_22 = encoder_block_2_layer_0_layer_norm_weight * to_12;  encoder_block_2_layer_0_layer_norm_weight = to_12 = None
    size_3 = mul_22.size()
    getitem_12 = size_3[slice(None, 2, None)];  size_3 = None
    getitem_13 = getitem_12[0]
    getitem_14 = getitem_12[1];  getitem_12 = None
    encoder_block_2_layer_0_self_attention_q = getattr(getattr(self.encoder.block, "2").layer, "0").SelfAttention.q(mul_22)
    view_9 = encoder_block_2_layer_0_self_attention_q.view(getitem_13, -1, 12, 64);  encoder_block_2_layer_0_self_attention_q = None
    transpose_10 = view_9.transpose(1, 2);  view_9 = None
    encoder_block_2_layer_0_self_attention_k = getattr(getattr(self.encoder.block, "2").layer, "0").SelfAttention.k(mul_22)
    view_10 = encoder_block_2_layer_0_self_attention_k.view(getitem_13, -1, 12, 64);  encoder_block_2_layer_0_self_attention_k = None
    transpose_11 = view_10.transpose(1, 2);  view_10 = None
    encoder_block_2_layer_0_self_attention_v = getattr(getattr(self.encoder.block, "2").layer, "0").SelfAttention.v(mul_22);  mul_22 = None
    view_11 = encoder_block_2_layer_0_self_attention_v.view(getitem_13, -1, 12, 64);  encoder_block_2_layer_0_self_attention_v = None
    transpose_12 = view_11.transpose(1, 2);  view_11 = None
    transpose_13 = transpose_11.transpose(3, 2);  transpose_11 = None
    matmul_4 = torch.matmul(transpose_10, transpose_13);  transpose_10 = transpose_13 = None
    add_19 = matmul_4 + add_4;  matmul_4 = None
    float_4 = add_19.float()
    softmax_2 = torch.nn.functional.softmax(float_4, dim = -1, _stacklevel = 3, dtype = None);  float_4 = None
    type_as_2 = softmax_2.type_as(add_19);  softmax_2 = add_19 = None
    dropout_2 = torch.nn.functional.dropout(type_as_2, p = 0.1, training = False, inplace = False);  type_as_2 = None
    matmul_5 = torch.matmul(dropout_2, transpose_12);  dropout_2 = transpose_12 = None
    transpose_14 = matmul_5.transpose(1, 2);  matmul_5 = None
    contiguous_2 = transpose_14.contiguous();  transpose_14 = None
    view_12 = contiguous_2.view(getitem_13, -1, 768);  contiguous_2 = getitem_13 = None
    linear_4 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_4 = patch_linear_layer_linear_layer_triton_wrapper(view_12, linear_4);  view_12 = linear_4 = None
    encoder_block_2_layer_0_dropout = getattr(getattr(self.encoder.block, "2").layer, "0").dropout(linear_layer_triton_wrapper_4);  linear_layer_triton_wrapper_4 = None
    add_20 = add_17 + encoder_block_2_layer_0_dropout;  add_17 = encoder_block_2_layer_0_dropout = None
    getattr_17 = add_20.dtype
    eq_12 = getattr_17 == torch.float16;  getattr_17 = None
    to_13 = add_20.to(torch.float32)
    pow_8 = to_13.pow(2);  to_13 = None
    mean_5 = pow_8.mean(-1, keepdim = True);  pow_8 = None
    add_21 = mean_5 + 1e-06;  mean_5 = None
    rsqrt_5 = torch.rsqrt(add_21);  add_21 = None
    mul_23 = add_20 * rsqrt_5;  rsqrt_5 = None
    encoder_block_2_layer_1_layer_norm_weight = getattr(getattr(self.encoder.block, "2").layer, "1").layer_norm.weight
    getattr_18 = encoder_block_2_layer_1_layer_norm_weight.dtype
    eq_13 = getattr_18 == torch.float16;  getattr_18 = None
    getattr_19 = encoder_block_2_layer_1_layer_norm_weight.dtype
    to_14 = mul_23.to(getattr_19);  mul_23 = getattr_19 = None
    mul_24 = encoder_block_2_layer_1_layer_norm_weight * to_14;  encoder_block_2_layer_1_layer_norm_weight = to_14 = None
    encoder_block_2_layer_1_dense_relu_dense_wi_0 = getattr(getattr(self.encoder.block, "2").layer, "1").DenseReluDense.wi_0(mul_24)
    mul_25 = 0.5 * encoder_block_2_layer_1_dense_relu_dense_wi_0
    pow_9 = torch.pow(encoder_block_2_layer_1_dense_relu_dense_wi_0, 3.0)
    mul_26 = 0.044715 * pow_9;  pow_9 = None
    add_22 = encoder_block_2_layer_1_dense_relu_dense_wi_0 + mul_26;  encoder_block_2_layer_1_dense_relu_dense_wi_0 = mul_26 = None
    mul_27 = 0.7978845608028654 * add_22;  add_22 = None
    tanh_2 = torch.tanh(mul_27);  mul_27 = None
    add_23 = 1.0 + tanh_2;  tanh_2 = None
    mul_28 = mul_25 * add_23;  mul_25 = add_23 = None
    encoder_block_2_layer_1_dense_relu_dense_wi_1 = getattr(getattr(self.encoder.block, "2").layer, "1").DenseReluDense.wi_1(mul_24);  mul_24 = None
    mul_29 = mul_28 * encoder_block_2_layer_1_dense_relu_dense_wi_1;  mul_28 = encoder_block_2_layer_1_dense_relu_dense_wi_1 = None
    encoder_block_2_layer_1_dense_relu_dense_dropout = getattr(getattr(self.encoder.block, "2").layer, "1").DenseReluDense.dropout(mul_29);  mul_29 = None
    encoder_block_2_layer_1_dense_relu_dense_wo_weight = getattr(getattr(self.encoder.block, "2").layer, "1").DenseReluDense.wo.weight
    linear_5 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_5 = patch_linear_layer_linear_layer_triton_wrapper(encoder_block_2_layer_1_dense_relu_dense_dropout, linear_5);  encoder_block_2_layer_1_dense_relu_dense_dropout = linear_5 = None
    encoder_block_2_layer_1_dropout = getattr(getattr(self.encoder.block, "2").layer, "1").dropout(linear_layer_triton_wrapper_5);  linear_layer_triton_wrapper_5 = None
    add_24 = add_20 + encoder_block_2_layer_1_dropout;  add_20 = encoder_block_2_layer_1_dropout = None
    getattr_20 = add_24.dtype
    eq_14 = getattr_20 == torch.float16;  getattr_20 = None
    to_15 = add_24.to(torch.float32)
    pow_10 = to_15.pow(2);  to_15 = None
    mean_6 = pow_10.mean(-1, keepdim = True);  pow_10 = None
    add_25 = mean_6 + 1e-06;  mean_6 = None
    rsqrt_6 = torch.rsqrt(add_25);  add_25 = None
    mul_30 = add_24 * rsqrt_6;  rsqrt_6 = None
    encoder_block_3_layer_0_layer_norm_weight = getattr(getattr(self.encoder.block, "3").layer, "0").layer_norm.weight
    getattr_21 = encoder_block_3_layer_0_layer_norm_weight.dtype
    eq_15 = getattr_21 == torch.float16;  getattr_21 = None
    getattr_22 = encoder_block_3_layer_0_layer_norm_weight.dtype
    to_16 = mul_30.to(getattr_22);  mul_30 = getattr_22 = None
    mul_31 = encoder_block_3_layer_0_layer_norm_weight * to_16;  encoder_block_3_layer_0_layer_norm_weight = to_16 = None
    size_4 = mul_31.size()
    getitem_15 = size_4[slice(None, 2, None)];  size_4 = None
    getitem_16 = getitem_15[0]
    getitem_17 = getitem_15[1];  getitem_15 = None
    encoder_block_3_layer_0_self_attention_q = getattr(getattr(self.encoder.block, "3").layer, "0").SelfAttention.q(mul_31)
    view_13 = encoder_block_3_layer_0_self_attention_q.view(getitem_16, -1, 12, 64);  encoder_block_3_layer_0_self_attention_q = None
    transpose_15 = view_13.transpose(1, 2);  view_13 = None
    encoder_block_3_layer_0_self_attention_k = getattr(getattr(self.encoder.block, "3").layer, "0").SelfAttention.k(mul_31)
    view_14 = encoder_block_3_layer_0_self_attention_k.view(getitem_16, -1, 12, 64);  encoder_block_3_layer_0_self_attention_k = None
    transpose_16 = view_14.transpose(1, 2);  view_14 = None
    encoder_block_3_layer_0_self_attention_v = getattr(getattr(self.encoder.block, "3").layer, "0").SelfAttention.v(mul_31);  mul_31 = None
    view_15 = encoder_block_3_layer_0_self_attention_v.view(getitem_16, -1, 12, 64);  encoder_block_3_layer_0_self_attention_v = None
    transpose_17 = view_15.transpose(1, 2);  view_15 = None
    transpose_18 = transpose_16.transpose(3, 2);  transpose_16 = None
    matmul_6 = torch.matmul(transpose_15, transpose_18);  transpose_15 = transpose_18 = None
    add_26 = matmul_6 + add_4;  matmul_6 = None
    float_5 = add_26.float()
    softmax_3 = torch.nn.functional.softmax(float_5, dim = -1, _stacklevel = 3, dtype = None);  float_5 = None
    type_as_3 = softmax_3.type_as(add_26);  softmax_3 = add_26 = None
    dropout_3 = torch.nn.functional.dropout(type_as_3, p = 0.1, training = False, inplace = False);  type_as_3 = None
    matmul_7 = torch.matmul(dropout_3, transpose_17);  dropout_3 = transpose_17 = None
    transpose_19 = matmul_7.transpose(1, 2);  matmul_7 = None
    contiguous_3 = transpose_19.contiguous();  transpose_19 = None
    view_16 = contiguous_3.view(getitem_16, -1, 768);  contiguous_3 = getitem_16 = None
    linear_6 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_6 = patch_linear_layer_linear_layer_triton_wrapper(view_16, linear_6);  view_16 = linear_6 = None
    encoder_block_3_layer_0_dropout = getattr(getattr(self.encoder.block, "3").layer, "0").dropout(linear_layer_triton_wrapper_6);  linear_layer_triton_wrapper_6 = None
    add_27 = add_24 + encoder_block_3_layer_0_dropout;  add_24 = encoder_block_3_layer_0_dropout = None
    getattr_23 = add_27.dtype
    eq_16 = getattr_23 == torch.float16;  getattr_23 = None
    to_17 = add_27.to(torch.float32)
    pow_11 = to_17.pow(2);  to_17 = None
    mean_7 = pow_11.mean(-1, keepdim = True);  pow_11 = None
    add_28 = mean_7 + 1e-06;  mean_7 = None
    rsqrt_7 = torch.rsqrt(add_28);  add_28 = None
    mul_32 = add_27 * rsqrt_7;  rsqrt_7 = None
    encoder_block_3_layer_1_layer_norm_weight = getattr(getattr(self.encoder.block, "3").layer, "1").layer_norm.weight
    getattr_24 = encoder_block_3_layer_1_layer_norm_weight.dtype
    eq_17 = getattr_24 == torch.float16;  getattr_24 = None
    getattr_25 = encoder_block_3_layer_1_layer_norm_weight.dtype
    to_18 = mul_32.to(getattr_25);  mul_32 = getattr_25 = None
    mul_33 = encoder_block_3_layer_1_layer_norm_weight * to_18;  encoder_block_3_layer_1_layer_norm_weight = to_18 = None
    encoder_block_3_layer_1_dense_relu_dense_wi_0 = getattr(getattr(self.encoder.block, "3").layer, "1").DenseReluDense.wi_0(mul_33)
    mul_34 = 0.5 * encoder_block_3_layer_1_dense_relu_dense_wi_0
    pow_12 = torch.pow(encoder_block_3_layer_1_dense_relu_dense_wi_0, 3.0)
    mul_35 = 0.044715 * pow_12;  pow_12 = None
    add_29 = encoder_block_3_layer_1_dense_relu_dense_wi_0 + mul_35;  encoder_block_3_layer_1_dense_relu_dense_wi_0 = mul_35 = None
    mul_36 = 0.7978845608028654 * add_29;  add_29 = None
    tanh_3 = torch.tanh(mul_36);  mul_36 = None
    add_30 = 1.0 + tanh_3;  tanh_3 = None
    mul_37 = mul_34 * add_30;  mul_34 = add_30 = None
    encoder_block_3_layer_1_dense_relu_dense_wi_1 = getattr(getattr(self.encoder.block, "3").layer, "1").DenseReluDense.wi_1(mul_33);  mul_33 = None
    mul_38 = mul_37 * encoder_block_3_layer_1_dense_relu_dense_wi_1;  mul_37 = encoder_block_3_layer_1_dense_relu_dense_wi_1 = None
    encoder_block_3_layer_1_dense_relu_dense_dropout = getattr(getattr(self.encoder.block, "3").layer, "1").DenseReluDense.dropout(mul_38);  mul_38 = None
    encoder_block_3_layer_1_dense_relu_dense_wo_weight = getattr(getattr(self.encoder.block, "3").layer, "1").DenseReluDense.wo.weight
    linear_7 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_7 = patch_linear_layer_linear_layer_triton_wrapper(encoder_block_3_layer_1_dense_relu_dense_dropout, linear_7);  encoder_block_3_layer_1_dense_relu_dense_dropout = linear_7 = None
    encoder_block_3_layer_1_dropout = getattr(getattr(self.encoder.block, "3").layer, "1").dropout(linear_layer_triton_wrapper_7);  linear_layer_triton_wrapper_7 = None
    add_31 = add_27 + encoder_block_3_layer_1_dropout;  add_27 = encoder_block_3_layer_1_dropout = None
    getattr_26 = add_31.dtype
    eq_18 = getattr_26 == torch.float16;  getattr_26 = None
    to_19 = add_31.to(torch.float32)
    pow_13 = to_19.pow(2);  to_19 = None
    mean_8 = pow_13.mean(-1, keepdim = True);  pow_13 = None
    add_32 = mean_8 + 1e-06;  mean_8 = None
    rsqrt_8 = torch.rsqrt(add_32);  add_32 = None
    mul_39 = add_31 * rsqrt_8;  rsqrt_8 = None
    encoder_block_4_layer_0_layer_norm_weight = getattr(getattr(self.encoder.block, "4").layer, "0").layer_norm.weight
    getattr_27 = encoder_block_4_layer_0_layer_norm_weight.dtype
    eq_19 = getattr_27 == torch.float16;  getattr_27 = None
    getattr_28 = encoder_block_4_layer_0_layer_norm_weight.dtype
    to_20 = mul_39.to(getattr_28);  mul_39 = getattr_28 = None
    mul_40 = encoder_block_4_layer_0_layer_norm_weight * to_20;  encoder_block_4_layer_0_layer_norm_weight = to_20 = None
    size_5 = mul_40.size()
    getitem_18 = size_5[slice(None, 2, None)];  size_5 = None
    getitem_19 = getitem_18[0]
    getitem_20 = getitem_18[1];  getitem_18 = None
    encoder_block_4_layer_0_self_attention_q = getattr(getattr(self.encoder.block, "4").layer, "0").SelfAttention.q(mul_40)
    view_17 = encoder_block_4_layer_0_self_attention_q.view(getitem_19, -1, 12, 64);  encoder_block_4_layer_0_self_attention_q = None
    transpose_20 = view_17.transpose(1, 2);  view_17 = None
    encoder_block_4_layer_0_self_attention_k = getattr(getattr(self.encoder.block, "4").layer, "0").SelfAttention.k(mul_40)
    view_18 = encoder_block_4_layer_0_self_attention_k.view(getitem_19, -1, 12, 64);  encoder_block_4_layer_0_self_attention_k = None
    transpose_21 = view_18.transpose(1, 2);  view_18 = None
    encoder_block_4_layer_0_self_attention_v = getattr(getattr(self.encoder.block, "4").layer, "0").SelfAttention.v(mul_40);  mul_40 = None
    view_19 = encoder_block_4_layer_0_self_attention_v.view(getitem_19, -1, 12, 64);  encoder_block_4_layer_0_self_attention_v = None
    transpose_22 = view_19.transpose(1, 2);  view_19 = None
    transpose_23 = transpose_21.transpose(3, 2);  transpose_21 = None
    matmul_8 = torch.matmul(transpose_20, transpose_23);  transpose_20 = transpose_23 = None
    add_33 = matmul_8 + add_4;  matmul_8 = None
    float_6 = add_33.float()
    softmax_4 = torch.nn.functional.softmax(float_6, dim = -1, _stacklevel = 3, dtype = None);  float_6 = None
    type_as_4 = softmax_4.type_as(add_33);  softmax_4 = add_33 = None
    dropout_4 = torch.nn.functional.dropout(type_as_4, p = 0.1, training = False, inplace = False);  type_as_4 = None
    matmul_9 = torch.matmul(dropout_4, transpose_22);  dropout_4 = transpose_22 = None
    transpose_24 = matmul_9.transpose(1, 2);  matmul_9 = None
    contiguous_4 = transpose_24.contiguous();  transpose_24 = None
    view_20 = contiguous_4.view(getitem_19, -1, 768);  contiguous_4 = getitem_19 = None
    linear_8 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_8 = patch_linear_layer_linear_layer_triton_wrapper(view_20, linear_8);  view_20 = linear_8 = None
    encoder_block_4_layer_0_dropout = getattr(getattr(self.encoder.block, "4").layer, "0").dropout(linear_layer_triton_wrapper_8);  linear_layer_triton_wrapper_8 = None
    add_34 = add_31 + encoder_block_4_layer_0_dropout;  add_31 = encoder_block_4_layer_0_dropout = None
    getattr_29 = add_34.dtype
    eq_20 = getattr_29 == torch.float16;  getattr_29 = None
    to_21 = add_34.to(torch.float32)
    pow_14 = to_21.pow(2);  to_21 = None
    mean_9 = pow_14.mean(-1, keepdim = True);  pow_14 = None
    add_35 = mean_9 + 1e-06;  mean_9 = None
    rsqrt_9 = torch.rsqrt(add_35);  add_35 = None
    mul_41 = add_34 * rsqrt_9;  rsqrt_9 = None
    encoder_block_4_layer_1_layer_norm_weight = getattr(getattr(self.encoder.block, "4").layer, "1").layer_norm.weight
    getattr_30 = encoder_block_4_layer_1_layer_norm_weight.dtype
    eq_21 = getattr_30 == torch.float16;  getattr_30 = None
    getattr_31 = encoder_block_4_layer_1_layer_norm_weight.dtype
    to_22 = mul_41.to(getattr_31);  mul_41 = getattr_31 = None
    mul_42 = encoder_block_4_layer_1_layer_norm_weight * to_22;  encoder_block_4_layer_1_layer_norm_weight = to_22 = None
    encoder_block_4_layer_1_dense_relu_dense_wi_0 = getattr(getattr(self.encoder.block, "4").layer, "1").DenseReluDense.wi_0(mul_42)
    mul_43 = 0.5 * encoder_block_4_layer_1_dense_relu_dense_wi_0
    pow_15 = torch.pow(encoder_block_4_layer_1_dense_relu_dense_wi_0, 3.0)
    mul_44 = 0.044715 * pow_15;  pow_15 = None
    add_36 = encoder_block_4_layer_1_dense_relu_dense_wi_0 + mul_44;  encoder_block_4_layer_1_dense_relu_dense_wi_0 = mul_44 = None
    mul_45 = 0.7978845608028654 * add_36;  add_36 = None
    tanh_4 = torch.tanh(mul_45);  mul_45 = None
    add_37 = 1.0 + tanh_4;  tanh_4 = None
    mul_46 = mul_43 * add_37;  mul_43 = add_37 = None
    encoder_block_4_layer_1_dense_relu_dense_wi_1 = getattr(getattr(self.encoder.block, "4").layer, "1").DenseReluDense.wi_1(mul_42);  mul_42 = None
    mul_47 = mul_46 * encoder_block_4_layer_1_dense_relu_dense_wi_1;  mul_46 = encoder_block_4_layer_1_dense_relu_dense_wi_1 = None
    encoder_block_4_layer_1_dense_relu_dense_dropout = getattr(getattr(self.encoder.block, "4").layer, "1").DenseReluDense.dropout(mul_47);  mul_47 = None
    encoder_block_4_layer_1_dense_relu_dense_wo_weight = getattr(getattr(self.encoder.block, "4").layer, "1").DenseReluDense.wo.weight
    linear_9 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_9 = patch_linear_layer_linear_layer_triton_wrapper(encoder_block_4_layer_1_dense_relu_dense_dropout, linear_9);  encoder_block_4_layer_1_dense_relu_dense_dropout = linear_9 = None
    encoder_block_4_layer_1_dropout = getattr(getattr(self.encoder.block, "4").layer, "1").dropout(linear_layer_triton_wrapper_9);  linear_layer_triton_wrapper_9 = None
    add_38 = add_34 + encoder_block_4_layer_1_dropout;  add_34 = encoder_block_4_layer_1_dropout = None
    getattr_32 = add_38.dtype
    eq_22 = getattr_32 == torch.float16;  getattr_32 = None
    to_23 = add_38.to(torch.float32)
    pow_16 = to_23.pow(2);  to_23 = None
    mean_10 = pow_16.mean(-1, keepdim = True);  pow_16 = None
    add_39 = mean_10 + 1e-06;  mean_10 = None
    rsqrt_10 = torch.rsqrt(add_39);  add_39 = None
    mul_48 = add_38 * rsqrt_10;  rsqrt_10 = None
    encoder_block_5_layer_0_layer_norm_weight = getattr(getattr(self.encoder.block, "5").layer, "0").layer_norm.weight
    getattr_33 = encoder_block_5_layer_0_layer_norm_weight.dtype
    eq_23 = getattr_33 == torch.float16;  getattr_33 = None
    getattr_34 = encoder_block_5_layer_0_layer_norm_weight.dtype
    to_24 = mul_48.to(getattr_34);  mul_48 = getattr_34 = None
    mul_49 = encoder_block_5_layer_0_layer_norm_weight * to_24;  encoder_block_5_layer_0_layer_norm_weight = to_24 = None
    size_6 = mul_49.size()
    getitem_21 = size_6[slice(None, 2, None)];  size_6 = None
    getitem_22 = getitem_21[0]
    getitem_23 = getitem_21[1];  getitem_21 = None
    encoder_block_5_layer_0_self_attention_q = getattr(getattr(self.encoder.block, "5").layer, "0").SelfAttention.q(mul_49)
    view_21 = encoder_block_5_layer_0_self_attention_q.view(getitem_22, -1, 12, 64);  encoder_block_5_layer_0_self_attention_q = None
    transpose_25 = view_21.transpose(1, 2);  view_21 = None
    encoder_block_5_layer_0_self_attention_k = getattr(getattr(self.encoder.block, "5").layer, "0").SelfAttention.k(mul_49)
    view_22 = encoder_block_5_layer_0_self_attention_k.view(getitem_22, -1, 12, 64);  encoder_block_5_layer_0_self_attention_k = None
    transpose_26 = view_22.transpose(1, 2);  view_22 = None
    encoder_block_5_layer_0_self_attention_v = getattr(getattr(self.encoder.block, "5").layer, "0").SelfAttention.v(mul_49);  mul_49 = None
    view_23 = encoder_block_5_layer_0_self_attention_v.view(getitem_22, -1, 12, 64);  encoder_block_5_layer_0_self_attention_v = None
    transpose_27 = view_23.transpose(1, 2);  view_23 = None
    transpose_28 = transpose_26.transpose(3, 2);  transpose_26 = None
    matmul_10 = torch.matmul(transpose_25, transpose_28);  transpose_25 = transpose_28 = None
    add_40 = matmul_10 + add_4;  matmul_10 = None
    float_7 = add_40.float()
    softmax_5 = torch.nn.functional.softmax(float_7, dim = -1, _stacklevel = 3, dtype = None);  float_7 = None
    type_as_5 = softmax_5.type_as(add_40);  softmax_5 = add_40 = None
    dropout_5 = torch.nn.functional.dropout(type_as_5, p = 0.1, training = False, inplace = False);  type_as_5 = None
    matmul_11 = torch.matmul(dropout_5, transpose_27);  dropout_5 = transpose_27 = None
    transpose_29 = matmul_11.transpose(1, 2);  matmul_11 = None
    contiguous_5 = transpose_29.contiguous();  transpose_29 = None
    view_24 = contiguous_5.view(getitem_22, -1, 768);  contiguous_5 = getitem_22 = None
    linear_10 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_10 = patch_linear_layer_linear_layer_triton_wrapper(view_24, linear_10);  view_24 = linear_10 = None
    encoder_block_5_layer_0_dropout = getattr(getattr(self.encoder.block, "5").layer, "0").dropout(linear_layer_triton_wrapper_10);  linear_layer_triton_wrapper_10 = None
    add_41 = add_38 + encoder_block_5_layer_0_dropout;  add_38 = encoder_block_5_layer_0_dropout = None
    getattr_35 = add_41.dtype
    eq_24 = getattr_35 == torch.float16;  getattr_35 = None
    to_25 = add_41.to(torch.float32)
    pow_17 = to_25.pow(2);  to_25 = None
    mean_11 = pow_17.mean(-1, keepdim = True);  pow_17 = None
    add_42 = mean_11 + 1e-06;  mean_11 = None
    rsqrt_11 = torch.rsqrt(add_42);  add_42 = None
    mul_50 = add_41 * rsqrt_11;  rsqrt_11 = None
    encoder_block_5_layer_1_layer_norm_weight = getattr(getattr(self.encoder.block, "5").layer, "1").layer_norm.weight
    getattr_36 = encoder_block_5_layer_1_layer_norm_weight.dtype
    eq_25 = getattr_36 == torch.float16;  getattr_36 = None
    getattr_37 = encoder_block_5_layer_1_layer_norm_weight.dtype
    to_26 = mul_50.to(getattr_37);  mul_50 = getattr_37 = None
    mul_51 = encoder_block_5_layer_1_layer_norm_weight * to_26;  encoder_block_5_layer_1_layer_norm_weight = to_26 = None
    encoder_block_5_layer_1_dense_relu_dense_wi_0 = getattr(getattr(self.encoder.block, "5").layer, "1").DenseReluDense.wi_0(mul_51)
    mul_52 = 0.5 * encoder_block_5_layer_1_dense_relu_dense_wi_0
    pow_18 = torch.pow(encoder_block_5_layer_1_dense_relu_dense_wi_0, 3.0)
    mul_53 = 0.044715 * pow_18;  pow_18 = None
    add_43 = encoder_block_5_layer_1_dense_relu_dense_wi_0 + mul_53;  encoder_block_5_layer_1_dense_relu_dense_wi_0 = mul_53 = None
    mul_54 = 0.7978845608028654 * add_43;  add_43 = None
    tanh_5 = torch.tanh(mul_54);  mul_54 = None
    add_44 = 1.0 + tanh_5;  tanh_5 = None
    mul_55 = mul_52 * add_44;  mul_52 = add_44 = None
    encoder_block_5_layer_1_dense_relu_dense_wi_1 = getattr(getattr(self.encoder.block, "5").layer, "1").DenseReluDense.wi_1(mul_51);  mul_51 = None
    mul_56 = mul_55 * encoder_block_5_layer_1_dense_relu_dense_wi_1;  mul_55 = encoder_block_5_layer_1_dense_relu_dense_wi_1 = None
    encoder_block_5_layer_1_dense_relu_dense_dropout = getattr(getattr(self.encoder.block, "5").layer, "1").DenseReluDense.dropout(mul_56);  mul_56 = None
    encoder_block_5_layer_1_dense_relu_dense_wo_weight = getattr(getattr(self.encoder.block, "5").layer, "1").DenseReluDense.wo.weight
    linear_11 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_11 = patch_linear_layer_linear_layer_triton_wrapper(encoder_block_5_layer_1_dense_relu_dense_dropout, linear_11);  encoder_block_5_layer_1_dense_relu_dense_dropout = linear_11 = None
    encoder_block_5_layer_1_dropout = getattr(getattr(self.encoder.block, "5").layer, "1").dropout(linear_layer_triton_wrapper_11);  linear_layer_triton_wrapper_11 = None
    add_45 = add_41 + encoder_block_5_layer_1_dropout;  add_41 = encoder_block_5_layer_1_dropout = None
    getattr_38 = add_45.dtype
    eq_26 = getattr_38 == torch.float16;  getattr_38 = None
    to_27 = add_45.to(torch.float32)
    pow_19 = to_27.pow(2);  to_27 = None
    mean_12 = pow_19.mean(-1, keepdim = True);  pow_19 = None
    add_46 = mean_12 + 1e-06;  mean_12 = None
    rsqrt_12 = torch.rsqrt(add_46);  add_46 = None
    mul_57 = add_45 * rsqrt_12;  rsqrt_12 = None
    encoder_block_6_layer_0_layer_norm_weight = getattr(getattr(self.encoder.block, "6").layer, "0").layer_norm.weight
    getattr_39 = encoder_block_6_layer_0_layer_norm_weight.dtype
    eq_27 = getattr_39 == torch.float16;  getattr_39 = None
    getattr_40 = encoder_block_6_layer_0_layer_norm_weight.dtype
    to_28 = mul_57.to(getattr_40);  mul_57 = getattr_40 = None
    mul_58 = encoder_block_6_layer_0_layer_norm_weight * to_28;  encoder_block_6_layer_0_layer_norm_weight = to_28 = None
    size_7 = mul_58.size()
    getitem_24 = size_7[slice(None, 2, None)];  size_7 = None
    getitem_25 = getitem_24[0]
    getitem_26 = getitem_24[1];  getitem_24 = None
    encoder_block_6_layer_0_self_attention_q = getattr(getattr(self.encoder.block, "6").layer, "0").SelfAttention.q(mul_58)
    view_25 = encoder_block_6_layer_0_self_attention_q.view(getitem_25, -1, 12, 64);  encoder_block_6_layer_0_self_attention_q = None
    transpose_30 = view_25.transpose(1, 2);  view_25 = None
    encoder_block_6_layer_0_self_attention_k = getattr(getattr(self.encoder.block, "6").layer, "0").SelfAttention.k(mul_58)
    view_26 = encoder_block_6_layer_0_self_attention_k.view(getitem_25, -1, 12, 64);  encoder_block_6_layer_0_self_attention_k = None
    transpose_31 = view_26.transpose(1, 2);  view_26 = None
    encoder_block_6_layer_0_self_attention_v = getattr(getattr(self.encoder.block, "6").layer, "0").SelfAttention.v(mul_58);  mul_58 = None
    view_27 = encoder_block_6_layer_0_self_attention_v.view(getitem_25, -1, 12, 64);  encoder_block_6_layer_0_self_attention_v = None
    transpose_32 = view_27.transpose(1, 2);  view_27 = None
    transpose_33 = transpose_31.transpose(3, 2);  transpose_31 = None
    matmul_12 = torch.matmul(transpose_30, transpose_33);  transpose_30 = transpose_33 = None
    add_47 = matmul_12 + add_4;  matmul_12 = None
    float_8 = add_47.float()
    softmax_6 = torch.nn.functional.softmax(float_8, dim = -1, _stacklevel = 3, dtype = None);  float_8 = None
    type_as_6 = softmax_6.type_as(add_47);  softmax_6 = add_47 = None
    dropout_6 = torch.nn.functional.dropout(type_as_6, p = 0.1, training = False, inplace = False);  type_as_6 = None
    matmul_13 = torch.matmul(dropout_6, transpose_32);  dropout_6 = transpose_32 = None
    transpose_34 = matmul_13.transpose(1, 2);  matmul_13 = None
    contiguous_6 = transpose_34.contiguous();  transpose_34 = None
    view_28 = contiguous_6.view(getitem_25, -1, 768);  contiguous_6 = getitem_25 = None
    linear_12 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_12 = patch_linear_layer_linear_layer_triton_wrapper(view_28, linear_12);  view_28 = linear_12 = None
    encoder_block_6_layer_0_dropout = getattr(getattr(self.encoder.block, "6").layer, "0").dropout(linear_layer_triton_wrapper_12);  linear_layer_triton_wrapper_12 = None
    add_48 = add_45 + encoder_block_6_layer_0_dropout;  add_45 = encoder_block_6_layer_0_dropout = None
    getattr_41 = add_48.dtype
    eq_28 = getattr_41 == torch.float16;  getattr_41 = None
    to_29 = add_48.to(torch.float32)
    pow_20 = to_29.pow(2);  to_29 = None
    mean_13 = pow_20.mean(-1, keepdim = True);  pow_20 = None
    add_49 = mean_13 + 1e-06;  mean_13 = None
    rsqrt_13 = torch.rsqrt(add_49);  add_49 = None
    mul_59 = add_48 * rsqrt_13;  rsqrt_13 = None
    encoder_block_6_layer_1_layer_norm_weight = getattr(getattr(self.encoder.block, "6").layer, "1").layer_norm.weight
    getattr_42 = encoder_block_6_layer_1_layer_norm_weight.dtype
    eq_29 = getattr_42 == torch.float16;  getattr_42 = None
    getattr_43 = encoder_block_6_layer_1_layer_norm_weight.dtype
    to_30 = mul_59.to(getattr_43);  mul_59 = getattr_43 = None
    mul_60 = encoder_block_6_layer_1_layer_norm_weight * to_30;  encoder_block_6_layer_1_layer_norm_weight = to_30 = None
    encoder_block_6_layer_1_dense_relu_dense_wi_0 = getattr(getattr(self.encoder.block, "6").layer, "1").DenseReluDense.wi_0(mul_60)
    mul_61 = 0.5 * encoder_block_6_layer_1_dense_relu_dense_wi_0
    pow_21 = torch.pow(encoder_block_6_layer_1_dense_relu_dense_wi_0, 3.0)
    mul_62 = 0.044715 * pow_21;  pow_21 = None
    add_50 = encoder_block_6_layer_1_dense_relu_dense_wi_0 + mul_62;  encoder_block_6_layer_1_dense_relu_dense_wi_0 = mul_62 = None
    mul_63 = 0.7978845608028654 * add_50;  add_50 = None
    tanh_6 = torch.tanh(mul_63);  mul_63 = None
    add_51 = 1.0 + tanh_6;  tanh_6 = None
    mul_64 = mul_61 * add_51;  mul_61 = add_51 = None
    encoder_block_6_layer_1_dense_relu_dense_wi_1 = getattr(getattr(self.encoder.block, "6").layer, "1").DenseReluDense.wi_1(mul_60);  mul_60 = None
    mul_65 = mul_64 * encoder_block_6_layer_1_dense_relu_dense_wi_1;  mul_64 = encoder_block_6_layer_1_dense_relu_dense_wi_1 = None
    encoder_block_6_layer_1_dense_relu_dense_dropout = getattr(getattr(self.encoder.block, "6").layer, "1").DenseReluDense.dropout(mul_65);  mul_65 = None
    encoder_block_6_layer_1_dense_relu_dense_wo_weight = getattr(getattr(self.encoder.block, "6").layer, "1").DenseReluDense.wo.weight
    linear_13 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_13 = patch_linear_layer_linear_layer_triton_wrapper(encoder_block_6_layer_1_dense_relu_dense_dropout, linear_13);  encoder_block_6_layer_1_dense_relu_dense_dropout = linear_13 = None
    encoder_block_6_layer_1_dropout = getattr(getattr(self.encoder.block, "6").layer, "1").dropout(linear_layer_triton_wrapper_13);  linear_layer_triton_wrapper_13 = None
    add_52 = add_48 + encoder_block_6_layer_1_dropout;  add_48 = encoder_block_6_layer_1_dropout = None
    getattr_44 = add_52.dtype
    eq_30 = getattr_44 == torch.float16;  getattr_44 = None
    to_31 = add_52.to(torch.float32)
    pow_22 = to_31.pow(2);  to_31 = None
    mean_14 = pow_22.mean(-1, keepdim = True);  pow_22 = None
    add_53 = mean_14 + 1e-06;  mean_14 = None
    rsqrt_14 = torch.rsqrt(add_53);  add_53 = None
    mul_66 = add_52 * rsqrt_14;  rsqrt_14 = None
    encoder_block_7_layer_0_layer_norm_weight = getattr(getattr(self.encoder.block, "7").layer, "0").layer_norm.weight
    getattr_45 = encoder_block_7_layer_0_layer_norm_weight.dtype
    eq_31 = getattr_45 == torch.float16;  getattr_45 = None
    getattr_46 = encoder_block_7_layer_0_layer_norm_weight.dtype
    to_32 = mul_66.to(getattr_46);  mul_66 = getattr_46 = None
    mul_67 = encoder_block_7_layer_0_layer_norm_weight * to_32;  encoder_block_7_layer_0_layer_norm_weight = to_32 = None
    size_8 = mul_67.size()
    getitem_27 = size_8[slice(None, 2, None)];  size_8 = None
    getitem_28 = getitem_27[0]
    getitem_29 = getitem_27[1];  getitem_27 = None
    encoder_block_7_layer_0_self_attention_q = getattr(getattr(self.encoder.block, "7").layer, "0").SelfAttention.q(mul_67)
    view_29 = encoder_block_7_layer_0_self_attention_q.view(getitem_28, -1, 12, 64);  encoder_block_7_layer_0_self_attention_q = None
    transpose_35 = view_29.transpose(1, 2);  view_29 = None
    encoder_block_7_layer_0_self_attention_k = getattr(getattr(self.encoder.block, "7").layer, "0").SelfAttention.k(mul_67)
    view_30 = encoder_block_7_layer_0_self_attention_k.view(getitem_28, -1, 12, 64);  encoder_block_7_layer_0_self_attention_k = None
    transpose_36 = view_30.transpose(1, 2);  view_30 = None
    encoder_block_7_layer_0_self_attention_v = getattr(getattr(self.encoder.block, "7").layer, "0").SelfAttention.v(mul_67);  mul_67 = None
    view_31 = encoder_block_7_layer_0_self_attention_v.view(getitem_28, -1, 12, 64);  encoder_block_7_layer_0_self_attention_v = None
    transpose_37 = view_31.transpose(1, 2);  view_31 = None
    transpose_38 = transpose_36.transpose(3, 2);  transpose_36 = None
    matmul_14 = torch.matmul(transpose_35, transpose_38);  transpose_35 = transpose_38 = None
    add_54 = matmul_14 + add_4;  matmul_14 = None
    float_9 = add_54.float()
    softmax_7 = torch.nn.functional.softmax(float_9, dim = -1, _stacklevel = 3, dtype = None);  float_9 = None
    type_as_7 = softmax_7.type_as(add_54);  softmax_7 = add_54 = None
    dropout_7 = torch.nn.functional.dropout(type_as_7, p = 0.1, training = False, inplace = False);  type_as_7 = None
    matmul_15 = torch.matmul(dropout_7, transpose_37);  dropout_7 = transpose_37 = None
    transpose_39 = matmul_15.transpose(1, 2);  matmul_15 = None
    contiguous_7 = transpose_39.contiguous();  transpose_39 = None
    view_32 = contiguous_7.view(getitem_28, -1, 768);  contiguous_7 = getitem_28 = None
    linear_14 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_14 = patch_linear_layer_linear_layer_triton_wrapper(view_32, linear_14);  view_32 = linear_14 = None
    encoder_block_7_layer_0_dropout = getattr(getattr(self.encoder.block, "7").layer, "0").dropout(linear_layer_triton_wrapper_14);  linear_layer_triton_wrapper_14 = None
    add_55 = add_52 + encoder_block_7_layer_0_dropout;  add_52 = encoder_block_7_layer_0_dropout = None
    getattr_47 = add_55.dtype
    eq_32 = getattr_47 == torch.float16;  getattr_47 = None
    to_33 = add_55.to(torch.float32)
    pow_23 = to_33.pow(2);  to_33 = None
    mean_15 = pow_23.mean(-1, keepdim = True);  pow_23 = None
    add_56 = mean_15 + 1e-06;  mean_15 = None
    rsqrt_15 = torch.rsqrt(add_56);  add_56 = None
    mul_68 = add_55 * rsqrt_15;  rsqrt_15 = None
    encoder_block_7_layer_1_layer_norm_weight = getattr(getattr(self.encoder.block, "7").layer, "1").layer_norm.weight
    getattr_48 = encoder_block_7_layer_1_layer_norm_weight.dtype
    eq_33 = getattr_48 == torch.float16;  getattr_48 = None
    getattr_49 = encoder_block_7_layer_1_layer_norm_weight.dtype
    to_34 = mul_68.to(getattr_49);  mul_68 = getattr_49 = None
    mul_69 = encoder_block_7_layer_1_layer_norm_weight * to_34;  encoder_block_7_layer_1_layer_norm_weight = to_34 = None
    encoder_block_7_layer_1_dense_relu_dense_wi_0 = getattr(getattr(self.encoder.block, "7").layer, "1").DenseReluDense.wi_0(mul_69)
    mul_70 = 0.5 * encoder_block_7_layer_1_dense_relu_dense_wi_0
    pow_24 = torch.pow(encoder_block_7_layer_1_dense_relu_dense_wi_0, 3.0)
    mul_71 = 0.044715 * pow_24;  pow_24 = None
    add_57 = encoder_block_7_layer_1_dense_relu_dense_wi_0 + mul_71;  encoder_block_7_layer_1_dense_relu_dense_wi_0 = mul_71 = None
    mul_72 = 0.7978845608028654 * add_57;  add_57 = None
    tanh_7 = torch.tanh(mul_72);  mul_72 = None
    add_58 = 1.0 + tanh_7;  tanh_7 = None
    mul_73 = mul_70 * add_58;  mul_70 = add_58 = None
    encoder_block_7_layer_1_dense_relu_dense_wi_1 = getattr(getattr(self.encoder.block, "7").layer, "1").DenseReluDense.wi_1(mul_69);  mul_69 = None
    mul_74 = mul_73 * encoder_block_7_layer_1_dense_relu_dense_wi_1;  mul_73 = encoder_block_7_layer_1_dense_relu_dense_wi_1 = None
    encoder_block_7_layer_1_dense_relu_dense_dropout = getattr(getattr(self.encoder.block, "7").layer, "1").DenseReluDense.dropout(mul_74);  mul_74 = None
    encoder_block_7_layer_1_dense_relu_dense_wo_weight = getattr(getattr(self.encoder.block, "7").layer, "1").DenseReluDense.wo.weight
    linear_15 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_15 = patch_linear_layer_linear_layer_triton_wrapper(encoder_block_7_layer_1_dense_relu_dense_dropout, linear_15);  encoder_block_7_layer_1_dense_relu_dense_dropout = linear_15 = None
    encoder_block_7_layer_1_dropout = getattr(getattr(self.encoder.block, "7").layer, "1").dropout(linear_layer_triton_wrapper_15);  linear_layer_triton_wrapper_15 = None
    add_59 = add_55 + encoder_block_7_layer_1_dropout;  add_55 = encoder_block_7_layer_1_dropout = None
    getattr_50 = add_59.dtype
    eq_34 = getattr_50 == torch.float16;  getattr_50 = None
    to_35 = add_59.to(torch.float32)
    pow_25 = to_35.pow(2);  to_35 = None
    mean_16 = pow_25.mean(-1, keepdim = True);  pow_25 = None
    add_60 = mean_16 + 1e-06;  mean_16 = None
    rsqrt_16 = torch.rsqrt(add_60);  add_60 = None
    mul_75 = add_59 * rsqrt_16;  rsqrt_16 = None
    encoder_block_8_layer_0_layer_norm_weight = getattr(getattr(self.encoder.block, "8").layer, "0").layer_norm.weight
    getattr_51 = encoder_block_8_layer_0_layer_norm_weight.dtype
    eq_35 = getattr_51 == torch.float16;  getattr_51 = None
    getattr_52 = encoder_block_8_layer_0_layer_norm_weight.dtype
    to_36 = mul_75.to(getattr_52);  mul_75 = getattr_52 = None
    mul_76 = encoder_block_8_layer_0_layer_norm_weight * to_36;  encoder_block_8_layer_0_layer_norm_weight = to_36 = None
    size_9 = mul_76.size()
    getitem_30 = size_9[slice(None, 2, None)];  size_9 = None
    getitem_31 = getitem_30[0]
    getitem_32 = getitem_30[1];  getitem_30 = None
    encoder_block_8_layer_0_self_attention_q = getattr(getattr(self.encoder.block, "8").layer, "0").SelfAttention.q(mul_76)
    view_33 = encoder_block_8_layer_0_self_attention_q.view(getitem_31, -1, 12, 64);  encoder_block_8_layer_0_self_attention_q = None
    transpose_40 = view_33.transpose(1, 2);  view_33 = None
    encoder_block_8_layer_0_self_attention_k = getattr(getattr(self.encoder.block, "8").layer, "0").SelfAttention.k(mul_76)
    view_34 = encoder_block_8_layer_0_self_attention_k.view(getitem_31, -1, 12, 64);  encoder_block_8_layer_0_self_attention_k = None
    transpose_41 = view_34.transpose(1, 2);  view_34 = None
    encoder_block_8_layer_0_self_attention_v = getattr(getattr(self.encoder.block, "8").layer, "0").SelfAttention.v(mul_76);  mul_76 = None
    view_35 = encoder_block_8_layer_0_self_attention_v.view(getitem_31, -1, 12, 64);  encoder_block_8_layer_0_self_attention_v = None
    transpose_42 = view_35.transpose(1, 2);  view_35 = None
    transpose_43 = transpose_41.transpose(3, 2);  transpose_41 = None
    matmul_16 = torch.matmul(transpose_40, transpose_43);  transpose_40 = transpose_43 = None
    add_61 = matmul_16 + add_4;  matmul_16 = None
    float_10 = add_61.float()
    softmax_8 = torch.nn.functional.softmax(float_10, dim = -1, _stacklevel = 3, dtype = None);  float_10 = None
    type_as_8 = softmax_8.type_as(add_61);  softmax_8 = add_61 = None
    dropout_8 = torch.nn.functional.dropout(type_as_8, p = 0.1, training = False, inplace = False);  type_as_8 = None
    matmul_17 = torch.matmul(dropout_8, transpose_42);  dropout_8 = transpose_42 = None
    transpose_44 = matmul_17.transpose(1, 2);  matmul_17 = None
    contiguous_8 = transpose_44.contiguous();  transpose_44 = None
    view_36 = contiguous_8.view(getitem_31, -1, 768);  contiguous_8 = getitem_31 = None
    linear_16 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_16 = patch_linear_layer_linear_layer_triton_wrapper(view_36, linear_16);  view_36 = linear_16 = None
    encoder_block_8_layer_0_dropout = getattr(getattr(self.encoder.block, "8").layer, "0").dropout(linear_layer_triton_wrapper_16);  linear_layer_triton_wrapper_16 = None
    add_62 = add_59 + encoder_block_8_layer_0_dropout;  add_59 = encoder_block_8_layer_0_dropout = None
    getattr_53 = add_62.dtype
    eq_36 = getattr_53 == torch.float16;  getattr_53 = None
    to_37 = add_62.to(torch.float32)
    pow_26 = to_37.pow(2);  to_37 = None
    mean_17 = pow_26.mean(-1, keepdim = True);  pow_26 = None
    add_63 = mean_17 + 1e-06;  mean_17 = None
    rsqrt_17 = torch.rsqrt(add_63);  add_63 = None
    mul_77 = add_62 * rsqrt_17;  rsqrt_17 = None
    encoder_block_8_layer_1_layer_norm_weight = getattr(getattr(self.encoder.block, "8").layer, "1").layer_norm.weight
    getattr_54 = encoder_block_8_layer_1_layer_norm_weight.dtype
    eq_37 = getattr_54 == torch.float16;  getattr_54 = None
    getattr_55 = encoder_block_8_layer_1_layer_norm_weight.dtype
    to_38 = mul_77.to(getattr_55);  mul_77 = getattr_55 = None
    mul_78 = encoder_block_8_layer_1_layer_norm_weight * to_38;  encoder_block_8_layer_1_layer_norm_weight = to_38 = None
    encoder_block_8_layer_1_dense_relu_dense_wi_0 = getattr(getattr(self.encoder.block, "8").layer, "1").DenseReluDense.wi_0(mul_78)
    mul_79 = 0.5 * encoder_block_8_layer_1_dense_relu_dense_wi_0
    pow_27 = torch.pow(encoder_block_8_layer_1_dense_relu_dense_wi_0, 3.0)
    mul_80 = 0.044715 * pow_27;  pow_27 = None
    add_64 = encoder_block_8_layer_1_dense_relu_dense_wi_0 + mul_80;  encoder_block_8_layer_1_dense_relu_dense_wi_0 = mul_80 = None
    mul_81 = 0.7978845608028654 * add_64;  add_64 = None
    tanh_8 = torch.tanh(mul_81);  mul_81 = None
    add_65 = 1.0 + tanh_8;  tanh_8 = None
    mul_82 = mul_79 * add_65;  mul_79 = add_65 = None
    encoder_block_8_layer_1_dense_relu_dense_wi_1 = getattr(getattr(self.encoder.block, "8").layer, "1").DenseReluDense.wi_1(mul_78);  mul_78 = None
    mul_83 = mul_82 * encoder_block_8_layer_1_dense_relu_dense_wi_1;  mul_82 = encoder_block_8_layer_1_dense_relu_dense_wi_1 = None
    encoder_block_8_layer_1_dense_relu_dense_dropout = getattr(getattr(self.encoder.block, "8").layer, "1").DenseReluDense.dropout(mul_83);  mul_83 = None
    encoder_block_8_layer_1_dense_relu_dense_wo_weight = getattr(getattr(self.encoder.block, "8").layer, "1").DenseReluDense.wo.weight
    linear_17 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_17 = patch_linear_layer_linear_layer_triton_wrapper(encoder_block_8_layer_1_dense_relu_dense_dropout, linear_17);  encoder_block_8_layer_1_dense_relu_dense_dropout = linear_17 = None
    encoder_block_8_layer_1_dropout = getattr(getattr(self.encoder.block, "8").layer, "1").dropout(linear_layer_triton_wrapper_17);  linear_layer_triton_wrapper_17 = None
    add_66 = add_62 + encoder_block_8_layer_1_dropout;  add_62 = encoder_block_8_layer_1_dropout = None
    getattr_56 = add_66.dtype
    eq_38 = getattr_56 == torch.float16;  getattr_56 = None
    to_39 = add_66.to(torch.float32)
    pow_28 = to_39.pow(2);  to_39 = None
    mean_18 = pow_28.mean(-1, keepdim = True);  pow_28 = None
    add_67 = mean_18 + 1e-06;  mean_18 = None
    rsqrt_18 = torch.rsqrt(add_67);  add_67 = None
    mul_84 = add_66 * rsqrt_18;  rsqrt_18 = None
    encoder_block_9_layer_0_layer_norm_weight = getattr(getattr(self.encoder.block, "9").layer, "0").layer_norm.weight
    getattr_57 = encoder_block_9_layer_0_layer_norm_weight.dtype
    eq_39 = getattr_57 == torch.float16;  getattr_57 = None
    getattr_58 = encoder_block_9_layer_0_layer_norm_weight.dtype
    to_40 = mul_84.to(getattr_58);  mul_84 = getattr_58 = None
    mul_85 = encoder_block_9_layer_0_layer_norm_weight * to_40;  encoder_block_9_layer_0_layer_norm_weight = to_40 = None
    size_10 = mul_85.size()
    getitem_33 = size_10[slice(None, 2, None)];  size_10 = None
    getitem_34 = getitem_33[0]
    getitem_35 = getitem_33[1];  getitem_33 = None
    encoder_block_9_layer_0_self_attention_q = getattr(getattr(self.encoder.block, "9").layer, "0").SelfAttention.q(mul_85)
    view_37 = encoder_block_9_layer_0_self_attention_q.view(getitem_34, -1, 12, 64);  encoder_block_9_layer_0_self_attention_q = None
    transpose_45 = view_37.transpose(1, 2);  view_37 = None
    encoder_block_9_layer_0_self_attention_k = getattr(getattr(self.encoder.block, "9").layer, "0").SelfAttention.k(mul_85)
    view_38 = encoder_block_9_layer_0_self_attention_k.view(getitem_34, -1, 12, 64);  encoder_block_9_layer_0_self_attention_k = None
    transpose_46 = view_38.transpose(1, 2);  view_38 = None
    encoder_block_9_layer_0_self_attention_v = getattr(getattr(self.encoder.block, "9").layer, "0").SelfAttention.v(mul_85);  mul_85 = None
    view_39 = encoder_block_9_layer_0_self_attention_v.view(getitem_34, -1, 12, 64);  encoder_block_9_layer_0_self_attention_v = None
    transpose_47 = view_39.transpose(1, 2);  view_39 = None
    transpose_48 = transpose_46.transpose(3, 2);  transpose_46 = None
    matmul_18 = torch.matmul(transpose_45, transpose_48);  transpose_45 = transpose_48 = None
    add_68 = matmul_18 + add_4;  matmul_18 = None
    float_11 = add_68.float()
    softmax_9 = torch.nn.functional.softmax(float_11, dim = -1, _stacklevel = 3, dtype = None);  float_11 = None
    type_as_9 = softmax_9.type_as(add_68);  softmax_9 = add_68 = None
    dropout_9 = torch.nn.functional.dropout(type_as_9, p = 0.1, training = False, inplace = False);  type_as_9 = None
    matmul_19 = torch.matmul(dropout_9, transpose_47);  dropout_9 = transpose_47 = None
    transpose_49 = matmul_19.transpose(1, 2);  matmul_19 = None
    contiguous_9 = transpose_49.contiguous();  transpose_49 = None
    view_40 = contiguous_9.view(getitem_34, -1, 768);  contiguous_9 = getitem_34 = None
    linear_18 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_18 = patch_linear_layer_linear_layer_triton_wrapper(view_40, linear_18);  view_40 = linear_18 = None
    encoder_block_9_layer_0_dropout = getattr(getattr(self.encoder.block, "9").layer, "0").dropout(linear_layer_triton_wrapper_18);  linear_layer_triton_wrapper_18 = None
    add_69 = add_66 + encoder_block_9_layer_0_dropout;  add_66 = encoder_block_9_layer_0_dropout = None
    getattr_59 = add_69.dtype
    eq_40 = getattr_59 == torch.float16;  getattr_59 = None
    to_41 = add_69.to(torch.float32)
    pow_29 = to_41.pow(2);  to_41 = None
    mean_19 = pow_29.mean(-1, keepdim = True);  pow_29 = None
    add_70 = mean_19 + 1e-06;  mean_19 = None
    rsqrt_19 = torch.rsqrt(add_70);  add_70 = None
    mul_86 = add_69 * rsqrt_19;  rsqrt_19 = None
    encoder_block_9_layer_1_layer_norm_weight = getattr(getattr(self.encoder.block, "9").layer, "1").layer_norm.weight
    getattr_60 = encoder_block_9_layer_1_layer_norm_weight.dtype
    eq_41 = getattr_60 == torch.float16;  getattr_60 = None
    getattr_61 = encoder_block_9_layer_1_layer_norm_weight.dtype
    to_42 = mul_86.to(getattr_61);  mul_86 = getattr_61 = None
    mul_87 = encoder_block_9_layer_1_layer_norm_weight * to_42;  encoder_block_9_layer_1_layer_norm_weight = to_42 = None
    encoder_block_9_layer_1_dense_relu_dense_wi_0 = getattr(getattr(self.encoder.block, "9").layer, "1").DenseReluDense.wi_0(mul_87)
    mul_88 = 0.5 * encoder_block_9_layer_1_dense_relu_dense_wi_0
    pow_30 = torch.pow(encoder_block_9_layer_1_dense_relu_dense_wi_0, 3.0)
    mul_89 = 0.044715 * pow_30;  pow_30 = None
    add_71 = encoder_block_9_layer_1_dense_relu_dense_wi_0 + mul_89;  encoder_block_9_layer_1_dense_relu_dense_wi_0 = mul_89 = None
    mul_90 = 0.7978845608028654 * add_71;  add_71 = None
    tanh_9 = torch.tanh(mul_90);  mul_90 = None
    add_72 = 1.0 + tanh_9;  tanh_9 = None
    mul_91 = mul_88 * add_72;  mul_88 = add_72 = None
    encoder_block_9_layer_1_dense_relu_dense_wi_1 = getattr(getattr(self.encoder.block, "9").layer, "1").DenseReluDense.wi_1(mul_87);  mul_87 = None
    mul_92 = mul_91 * encoder_block_9_layer_1_dense_relu_dense_wi_1;  mul_91 = encoder_block_9_layer_1_dense_relu_dense_wi_1 = None
    encoder_block_9_layer_1_dense_relu_dense_dropout = getattr(getattr(self.encoder.block, "9").layer, "1").DenseReluDense.dropout(mul_92);  mul_92 = None
    encoder_block_9_layer_1_dense_relu_dense_wo_weight = getattr(getattr(self.encoder.block, "9").layer, "1").DenseReluDense.wo.weight
    linear_19 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_19 = patch_linear_layer_linear_layer_triton_wrapper(encoder_block_9_layer_1_dense_relu_dense_dropout, linear_19);  encoder_block_9_layer_1_dense_relu_dense_dropout = linear_19 = None
    encoder_block_9_layer_1_dropout = getattr(getattr(self.encoder.block, "9").layer, "1").dropout(linear_layer_triton_wrapper_19);  linear_layer_triton_wrapper_19 = None
    add_73 = add_69 + encoder_block_9_layer_1_dropout;  add_69 = encoder_block_9_layer_1_dropout = None
    getattr_62 = add_73.dtype
    eq_42 = getattr_62 == torch.float16;  getattr_62 = None
    to_43 = add_73.to(torch.float32)
    pow_31 = to_43.pow(2);  to_43 = None
    mean_20 = pow_31.mean(-1, keepdim = True);  pow_31 = None
    add_74 = mean_20 + 1e-06;  mean_20 = None
    rsqrt_20 = torch.rsqrt(add_74);  add_74 = None
    mul_93 = add_73 * rsqrt_20;  rsqrt_20 = None
    encoder_block_10_layer_0_layer_norm_weight = getattr(getattr(self.encoder.block, "10").layer, "0").layer_norm.weight
    getattr_63 = encoder_block_10_layer_0_layer_norm_weight.dtype
    eq_43 = getattr_63 == torch.float16;  getattr_63 = None
    getattr_64 = encoder_block_10_layer_0_layer_norm_weight.dtype
    to_44 = mul_93.to(getattr_64);  mul_93 = getattr_64 = None
    mul_94 = encoder_block_10_layer_0_layer_norm_weight * to_44;  encoder_block_10_layer_0_layer_norm_weight = to_44 = None
    size_11 = mul_94.size()
    getitem_36 = size_11[slice(None, 2, None)];  size_11 = None
    getitem_37 = getitem_36[0]
    getitem_38 = getitem_36[1];  getitem_36 = None
    encoder_block_10_layer_0_self_attention_q = getattr(getattr(self.encoder.block, "10").layer, "0").SelfAttention.q(mul_94)
    view_41 = encoder_block_10_layer_0_self_attention_q.view(getitem_37, -1, 12, 64);  encoder_block_10_layer_0_self_attention_q = None
    transpose_50 = view_41.transpose(1, 2);  view_41 = None
    encoder_block_10_layer_0_self_attention_k = getattr(getattr(self.encoder.block, "10").layer, "0").SelfAttention.k(mul_94)
    view_42 = encoder_block_10_layer_0_self_attention_k.view(getitem_37, -1, 12, 64);  encoder_block_10_layer_0_self_attention_k = None
    transpose_51 = view_42.transpose(1, 2);  view_42 = None
    encoder_block_10_layer_0_self_attention_v = getattr(getattr(self.encoder.block, "10").layer, "0").SelfAttention.v(mul_94);  mul_94 = None
    view_43 = encoder_block_10_layer_0_self_attention_v.view(getitem_37, -1, 12, 64);  encoder_block_10_layer_0_self_attention_v = None
    transpose_52 = view_43.transpose(1, 2);  view_43 = None
    transpose_53 = transpose_51.transpose(3, 2);  transpose_51 = None
    matmul_20 = torch.matmul(transpose_50, transpose_53);  transpose_50 = transpose_53 = None
    add_75 = matmul_20 + add_4;  matmul_20 = None
    float_12 = add_75.float()
    softmax_10 = torch.nn.functional.softmax(float_12, dim = -1, _stacklevel = 3, dtype = None);  float_12 = None
    type_as_10 = softmax_10.type_as(add_75);  softmax_10 = add_75 = None
    dropout_10 = torch.nn.functional.dropout(type_as_10, p = 0.1, training = False, inplace = False);  type_as_10 = None
    matmul_21 = torch.matmul(dropout_10, transpose_52);  dropout_10 = transpose_52 = None
    transpose_54 = matmul_21.transpose(1, 2);  matmul_21 = None
    contiguous_10 = transpose_54.contiguous();  transpose_54 = None
    view_44 = contiguous_10.view(getitem_37, -1, 768);  contiguous_10 = getitem_37 = None
    linear_20 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_20 = patch_linear_layer_linear_layer_triton_wrapper(view_44, linear_20);  view_44 = linear_20 = None
    encoder_block_10_layer_0_dropout = getattr(getattr(self.encoder.block, "10").layer, "0").dropout(linear_layer_triton_wrapper_20);  linear_layer_triton_wrapper_20 = None
    add_76 = add_73 + encoder_block_10_layer_0_dropout;  add_73 = encoder_block_10_layer_0_dropout = None
    getattr_65 = add_76.dtype
    eq_44 = getattr_65 == torch.float16;  getattr_65 = None
    to_45 = add_76.to(torch.float32)
    pow_32 = to_45.pow(2);  to_45 = None
    mean_21 = pow_32.mean(-1, keepdim = True);  pow_32 = None
    add_77 = mean_21 + 1e-06;  mean_21 = None
    rsqrt_21 = torch.rsqrt(add_77);  add_77 = None
    mul_95 = add_76 * rsqrt_21;  rsqrt_21 = None
    encoder_block_10_layer_1_layer_norm_weight = getattr(getattr(self.encoder.block, "10").layer, "1").layer_norm.weight
    getattr_66 = encoder_block_10_layer_1_layer_norm_weight.dtype
    eq_45 = getattr_66 == torch.float16;  getattr_66 = None
    getattr_67 = encoder_block_10_layer_1_layer_norm_weight.dtype
    to_46 = mul_95.to(getattr_67);  mul_95 = getattr_67 = None
    mul_96 = encoder_block_10_layer_1_layer_norm_weight * to_46;  encoder_block_10_layer_1_layer_norm_weight = to_46 = None
    encoder_block_10_layer_1_dense_relu_dense_wi_0 = getattr(getattr(self.encoder.block, "10").layer, "1").DenseReluDense.wi_0(mul_96)
    mul_97 = 0.5 * encoder_block_10_layer_1_dense_relu_dense_wi_0
    pow_33 = torch.pow(encoder_block_10_layer_1_dense_relu_dense_wi_0, 3.0)
    mul_98 = 0.044715 * pow_33;  pow_33 = None
    add_78 = encoder_block_10_layer_1_dense_relu_dense_wi_0 + mul_98;  encoder_block_10_layer_1_dense_relu_dense_wi_0 = mul_98 = None
    mul_99 = 0.7978845608028654 * add_78;  add_78 = None
    tanh_10 = torch.tanh(mul_99);  mul_99 = None
    add_79 = 1.0 + tanh_10;  tanh_10 = None
    mul_100 = mul_97 * add_79;  mul_97 = add_79 = None
    encoder_block_10_layer_1_dense_relu_dense_wi_1 = getattr(getattr(self.encoder.block, "10").layer, "1").DenseReluDense.wi_1(mul_96);  mul_96 = None
    mul_101 = mul_100 * encoder_block_10_layer_1_dense_relu_dense_wi_1;  mul_100 = encoder_block_10_layer_1_dense_relu_dense_wi_1 = None
    encoder_block_10_layer_1_dense_relu_dense_dropout = getattr(getattr(self.encoder.block, "10").layer, "1").DenseReluDense.dropout(mul_101);  mul_101 = None
    encoder_block_10_layer_1_dense_relu_dense_wo_weight = getattr(getattr(self.encoder.block, "10").layer, "1").DenseReluDense.wo.weight
    linear_21 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_21 = patch_linear_layer_linear_layer_triton_wrapper(encoder_block_10_layer_1_dense_relu_dense_dropout, linear_21);  encoder_block_10_layer_1_dense_relu_dense_dropout = linear_21 = None
    encoder_block_10_layer_1_dropout = getattr(getattr(self.encoder.block, "10").layer, "1").dropout(linear_layer_triton_wrapper_21);  linear_layer_triton_wrapper_21 = None
    add_80 = add_76 + encoder_block_10_layer_1_dropout;  add_76 = encoder_block_10_layer_1_dropout = None
    getattr_68 = add_80.dtype
    eq_46 = getattr_68 == torch.float16;  getattr_68 = None
    to_47 = add_80.to(torch.float32)
    pow_34 = to_47.pow(2);  to_47 = None
    mean_22 = pow_34.mean(-1, keepdim = True);  pow_34 = None
    add_81 = mean_22 + 1e-06;  mean_22 = None
    rsqrt_22 = torch.rsqrt(add_81);  add_81 = None
    mul_102 = add_80 * rsqrt_22;  rsqrt_22 = None
    encoder_block_11_layer_0_layer_norm_weight = getattr(getattr(self.encoder.block, "11").layer, "0").layer_norm.weight
    getattr_69 = encoder_block_11_layer_0_layer_norm_weight.dtype
    eq_47 = getattr_69 == torch.float16;  getattr_69 = None
    getattr_70 = encoder_block_11_layer_0_layer_norm_weight.dtype
    to_48 = mul_102.to(getattr_70);  mul_102 = getattr_70 = None
    mul_103 = encoder_block_11_layer_0_layer_norm_weight * to_48;  encoder_block_11_layer_0_layer_norm_weight = to_48 = None
    size_12 = mul_103.size()
    getitem_39 = size_12[slice(None, 2, None)];  size_12 = None
    getitem_40 = getitem_39[0]
    getitem_41 = getitem_39[1];  getitem_39 = None
    encoder_block_11_layer_0_self_attention_q = getattr(getattr(self.encoder.block, "11").layer, "0").SelfAttention.q(mul_103)
    view_45 = encoder_block_11_layer_0_self_attention_q.view(getitem_40, -1, 12, 64);  encoder_block_11_layer_0_self_attention_q = None
    transpose_55 = view_45.transpose(1, 2);  view_45 = None
    encoder_block_11_layer_0_self_attention_k = getattr(getattr(self.encoder.block, "11").layer, "0").SelfAttention.k(mul_103)
    view_46 = encoder_block_11_layer_0_self_attention_k.view(getitem_40, -1, 12, 64);  encoder_block_11_layer_0_self_attention_k = None
    transpose_56 = view_46.transpose(1, 2);  view_46 = None
    encoder_block_11_layer_0_self_attention_v = getattr(getattr(self.encoder.block, "11").layer, "0").SelfAttention.v(mul_103);  mul_103 = None
    view_47 = encoder_block_11_layer_0_self_attention_v.view(getitem_40, -1, 12, 64);  encoder_block_11_layer_0_self_attention_v = None
    transpose_57 = view_47.transpose(1, 2);  view_47 = None
    transpose_58 = transpose_56.transpose(3, 2);  transpose_56 = None
    matmul_22 = torch.matmul(transpose_55, transpose_58);  transpose_55 = transpose_58 = None
    add_82 = matmul_22 + add_4;  matmul_22 = add_4 = None
    float_13 = add_82.float()
    softmax_11 = torch.nn.functional.softmax(float_13, dim = -1, _stacklevel = 3, dtype = None);  float_13 = None
    type_as_11 = softmax_11.type_as(add_82);  softmax_11 = add_82 = None
    dropout_11 = torch.nn.functional.dropout(type_as_11, p = 0.1, training = False, inplace = False);  type_as_11 = None
    matmul_23 = torch.matmul(dropout_11, transpose_57);  dropout_11 = transpose_57 = None
    transpose_59 = matmul_23.transpose(1, 2);  matmul_23 = None
    contiguous_11 = transpose_59.contiguous();  transpose_59 = None
    view_48 = contiguous_11.view(getitem_40, -1, 768);  contiguous_11 = getitem_40 = None
    linear_22 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_22 = patch_linear_layer_linear_layer_triton_wrapper(view_48, linear_22);  view_48 = linear_22 = None
    encoder_block_11_layer_0_dropout = getattr(getattr(self.encoder.block, "11").layer, "0").dropout(linear_layer_triton_wrapper_22);  linear_layer_triton_wrapper_22 = None
    add_83 = add_80 + encoder_block_11_layer_0_dropout;  add_80 = encoder_block_11_layer_0_dropout = None
    getattr_71 = add_83.dtype
    eq_48 = getattr_71 == torch.float16;  getattr_71 = None
    to_49 = add_83.to(torch.float32)
    pow_35 = to_49.pow(2);  to_49 = None
    mean_23 = pow_35.mean(-1, keepdim = True);  pow_35 = None
    add_84 = mean_23 + 1e-06;  mean_23 = None
    rsqrt_23 = torch.rsqrt(add_84);  add_84 = None
    mul_104 = add_83 * rsqrt_23;  rsqrt_23 = None
    encoder_block_11_layer_1_layer_norm_weight = getattr(getattr(self.encoder.block, "11").layer, "1").layer_norm.weight
    getattr_72 = encoder_block_11_layer_1_layer_norm_weight.dtype
    eq_49 = getattr_72 == torch.float16;  getattr_72 = None
    getattr_73 = encoder_block_11_layer_1_layer_norm_weight.dtype
    to_50 = mul_104.to(getattr_73);  mul_104 = getattr_73 = None
    mul_105 = encoder_block_11_layer_1_layer_norm_weight * to_50;  encoder_block_11_layer_1_layer_norm_weight = to_50 = None
    encoder_block_11_layer_1_dense_relu_dense_wi_0 = getattr(getattr(self.encoder.block, "11").layer, "1").DenseReluDense.wi_0(mul_105)
    mul_106 = 0.5 * encoder_block_11_layer_1_dense_relu_dense_wi_0
    pow_36 = torch.pow(encoder_block_11_layer_1_dense_relu_dense_wi_0, 3.0)
    mul_107 = 0.044715 * pow_36;  pow_36 = None
    add_85 = encoder_block_11_layer_1_dense_relu_dense_wi_0 + mul_107;  encoder_block_11_layer_1_dense_relu_dense_wi_0 = mul_107 = None
    mul_108 = 0.7978845608028654 * add_85;  add_85 = None
    tanh_11 = torch.tanh(mul_108);  mul_108 = None
    add_86 = 1.0 + tanh_11;  tanh_11 = None
    mul_109 = mul_106 * add_86;  mul_106 = add_86 = None
    encoder_block_11_layer_1_dense_relu_dense_wi_1 = getattr(getattr(self.encoder.block, "11").layer, "1").DenseReluDense.wi_1(mul_105);  mul_105 = None
    mul_110 = mul_109 * encoder_block_11_layer_1_dense_relu_dense_wi_1;  mul_109 = encoder_block_11_layer_1_dense_relu_dense_wi_1 = None
    encoder_block_11_layer_1_dense_relu_dense_dropout = getattr(getattr(self.encoder.block, "11").layer, "1").DenseReluDense.dropout(mul_110);  mul_110 = None
    encoder_block_11_layer_1_dense_relu_dense_wo_weight = getattr(getattr(self.encoder.block, "11").layer, "1").DenseReluDense.wo.weight
    linear_23 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_23 = patch_linear_layer_linear_layer_triton_wrapper(encoder_block_11_layer_1_dense_relu_dense_dropout, linear_23);  encoder_block_11_layer_1_dense_relu_dense_dropout = linear_23 = None
    encoder_block_11_layer_1_dropout = getattr(getattr(self.encoder.block, "11").layer, "1").dropout(linear_layer_triton_wrapper_23);  linear_layer_triton_wrapper_23 = None
    add_87 = add_83 + encoder_block_11_layer_1_dropout;  add_83 = encoder_block_11_layer_1_dropout = None
    getattr_74 = add_87.dtype
    eq_50 = getattr_74 == torch.float16;  getattr_74 = None
    to_51 = add_87.to(torch.float32)
    pow_37 = to_51.pow(2);  to_51 = None
    mean_24 = pow_37.mean(-1, keepdim = True);  pow_37 = None
    add_88 = mean_24 + 1e-06;  mean_24 = None
    rsqrt_24 = torch.rsqrt(add_88);  add_88 = None
    mul_111 = add_87 * rsqrt_24;  add_87 = rsqrt_24 = None
    encoder_final_layer_norm_weight = self.encoder.final_layer_norm.weight
    getattr_75 = encoder_final_layer_norm_weight.dtype
    eq_51 = getattr_75 == torch.float16;  getattr_75 = None
    getattr_76 = encoder_final_layer_norm_weight.dtype
    to_52 = mul_111.to(getattr_76);  mul_111 = getattr_76 = None
    mul_112 = encoder_final_layer_norm_weight * to_52;  encoder_final_layer_norm_weight = to_52 = None
    encoder_dropout_1 = self.encoder.dropout(mul_112);  mul_112 = None
    size_13 = labels.size()
    getitem_42 = size_13[slice(None, -1, None)];  size_13 = None
    add_89 = getitem_42 + (1,);  getitem_42 = None
    full = torch.full(add_89, 0);  add_89 = None
    getitem_43 = labels[(Ellipsis, slice(None, -1, None))]
    cat = torch.cat([full, getitem_43], dim = -1);  full = getitem_43 = None
    eq_52 = cat == -100
    masked_fill_ = cat.masked_fill_(eq_52, 0);  eq_52 = None
    size_14 = cat.size()
    getitem_44 = size_14[-1]
    view_49 = cat.view(-1, getitem_44);  cat = getitem_44 = None
    shared_1 = self.shared(view_49);  view_49 = None
    getitem_45 = size_14[0]
    getitem_46 = size_14[1]
    getattr_77 = shared_1.device
    ones_1 = torch.ones(getitem_45, getitem_46, device = getattr_77);  getitem_46 = getattr_77 = None
    size_15 = encoder_dropout_1.size()
    getitem_47 = size_15[1];  size_15 = None
    getattr_78 = shared_1.device
    ones_2 = torch.ones(getitem_45, getitem_47, device = getattr_78, dtype = torch.int64);  getitem_45 = getitem_47 = getattr_78 = None
    dim_3 = ones_1.dim()
    eq_53 = dim_3 == 2;  dim_3 = None
    dim_4 = ones_1.dim()
    eq_54 = dim_4 == 3;  dim_4 = None
    dim_5 = ones_1.dim()
    eq_55 = dim_5 == 2;  dim_5 = None
    getitem_48 = size_14[0]
    getitem_49 = size_14[1];  size_14 = None
    getattr_79 = ones_1.device
    arange_2 = torch.arange(getitem_49, device = getattr_79);  getattr_79 = None
    getitem_50 = arange_2[(None, None, slice(None, None, None))]
    repeat = getitem_50.repeat(getitem_48, getitem_49, 1);  getitem_50 = getitem_48 = getitem_49 = None
    getitem_51 = arange_2[(None, slice(None, None, None), None)];  arange_2 = None
    le = repeat <= getitem_51;  repeat = getitem_51 = None
    getattr_80 = ones_1.dtype
    to_53 = le.to(getattr_80);  le = getattr_80 = None
    size_16 = to_53.size()
    getitem_52 = size_16[1];  size_16 = None
    size_17 = ones_1.size()
    getitem_53 = size_17[1];  size_17 = None
    lt_1 = getitem_52 < getitem_53;  getitem_52 = getitem_53 = None
    getitem_54 = to_53[(slice(None, None, None), None, slice(None, None, None), slice(None, None, None))];  to_53 = None
    getitem_55 = ones_1[(slice(None, None, None), None, None, slice(None, None, None))];  ones_1 = None
    mul_113 = getitem_54 * getitem_55;  getitem_54 = getitem_55 = None
    to_54 = mul_113.to(dtype = torch.float16);  mul_113 = None
    sub_2 = 1.0 - to_54;  to_54 = None
    mul_114 = sub_2 * -65504.0;  sub_2 = None
    size_18 = encoder_dropout_1.size()
    getitem_56 = size_18[0]
    getitem_57 = size_18[1]
    getitem_58 = size_18[2];  size_18 = None
    dim_6 = ones_2.dim()
    eq_56 = dim_6 == 3;  dim_6 = None
    dim_7 = ones_2.dim()
    eq_57 = dim_7 == 2;  dim_7 = None
    getitem_59 = ones_2[(slice(None, None, None), None, None, slice(None, None, None))];  ones_2 = None
    to_55 = getitem_59.to(dtype = torch.float16);  getitem_59 = None
    sub_3 = 1.0 - to_55;  to_55 = None
    mul_115 = sub_3 * -65504.0;  sub_3 = None
    decoder_dropout = self.decoder.dropout(shared_1);  shared_1 = None
    to_56 = decoder_dropout.to(torch.float32)
    pow_38 = to_56.pow(2);  to_56 = None
    mean_25 = pow_38.mean(-1, keepdim = True);  pow_38 = None
    add_90 = mean_25 + 1e-06;  mean_25 = None
    rsqrt_25 = torch.rsqrt(add_90);  add_90 = None
    mul_116 = decoder_dropout * rsqrt_25;  rsqrt_25 = None
    decoder_block_0_layer_0_layer_norm_weight = getattr(getattr(self.decoder.block, "0").layer, "0").layer_norm.weight
    getattr_81 = decoder_block_0_layer_0_layer_norm_weight.dtype
    eq_58 = getattr_81 == torch.float16;  getattr_81 = None
    getattr_82 = decoder_block_0_layer_0_layer_norm_weight.dtype
    to_57 = mul_116.to(getattr_82);  mul_116 = getattr_82 = None
    mul_117 = decoder_block_0_layer_0_layer_norm_weight * to_57;  decoder_block_0_layer_0_layer_norm_weight = to_57 = None
    size_19 = mul_117.size()
    getitem_60 = size_19[slice(None, 2, None)];  size_19 = None
    getitem_61 = getitem_60[0]
    getitem_62 = getitem_60[1];  getitem_60 = None
    decoder_block_0_layer_0_self_attention_q = getattr(getattr(self.decoder.block, "0").layer, "0").SelfAttention.q(mul_117)
    view_50 = decoder_block_0_layer_0_self_attention_q.view(getitem_61, -1, 12, 64);  decoder_block_0_layer_0_self_attention_q = None
    transpose_60 = view_50.transpose(1, 2);  view_50 = None
    decoder_block_0_layer_0_self_attention_k = getattr(getattr(self.decoder.block, "0").layer, "0").SelfAttention.k(mul_117)
    view_51 = decoder_block_0_layer_0_self_attention_k.view(getitem_61, -1, 12, 64);  decoder_block_0_layer_0_self_attention_k = None
    transpose_61 = view_51.transpose(1, 2);  view_51 = None
    decoder_block_0_layer_0_self_attention_v = getattr(getattr(self.decoder.block, "0").layer, "0").SelfAttention.v(mul_117);  mul_117 = None
    view_52 = decoder_block_0_layer_0_self_attention_v.view(getitem_61, -1, 12, 64);  decoder_block_0_layer_0_self_attention_v = None
    transpose_62 = view_52.transpose(1, 2);  view_52 = None
    transpose_63 = transpose_61.transpose(3, 2)
    matmul_24 = torch.matmul(transpose_60, transpose_63);  transpose_60 = transpose_63 = None
    getattr_83 = matmul_24.device
    arange_3 = torch.arange(getitem_62, dtype = torch.int64, device = getattr_83)
    getitem_63 = arange_3[(slice(None, None, None), None)];  arange_3 = None
    arange_4 = torch.arange(getitem_62, dtype = torch.int64, device = getattr_83);  getitem_62 = getattr_83 = None
    getitem_64 = arange_4[(None, slice(None, None, None))];  arange_4 = None
    sub_4 = getitem_64 - getitem_63;  getitem_64 = getitem_63 = None
    zeros_like = torch.zeros_like(sub_4)
    min_2 = torch.min(sub_4, zeros_like);  sub_4 = zeros_like = None
    neg = -min_2;  min_2 = None
    lt_2 = neg < 16
    float_14 = neg.float()
    truediv_2 = float_14 / 16;  float_14 = None
    log_1 = torch.log(truediv_2);  truediv_2 = None
    truediv_3 = log_1 / 2.0794415416798357;  log_1 = None
    mul_118 = truediv_3 * 16;  truediv_3 = None
    to_58 = mul_118.to(torch.int64);  mul_118 = None
    add_91 = 16 + to_58;  to_58 = None
    full_like_1 = torch.full_like(add_91, 31)
    min_3 = torch.min(add_91, full_like_1);  add_91 = full_like_1 = None
    where_1 = torch.where(lt_2, neg, min_3);  lt_2 = neg = min_3 = None
    add_92 = 0 + where_1;  where_1 = None
    decoder_block_0_layer_0_self_attention_relative_attention_bias = getattr(getattr(self.decoder.block, "0").layer, "0").SelfAttention.relative_attention_bias(add_92);  add_92 = None
    permute_1 = decoder_block_0_layer_0_self_attention_relative_attention_bias.permute([2, 0, 1]);  decoder_block_0_layer_0_self_attention_relative_attention_bias = None
    unsqueeze_1 = permute_1.unsqueeze(0);  permute_1 = None
    add_93 = unsqueeze_1 + mul_114;  unsqueeze_1 = mul_114 = None
    add_94 = matmul_24 + add_93;  matmul_24 = None
    float_15 = add_94.float()
    softmax_12 = torch.nn.functional.softmax(float_15, dim = -1, _stacklevel = 3, dtype = None);  float_15 = None
    type_as_12 = softmax_12.type_as(add_94);  softmax_12 = add_94 = None
    dropout_12 = torch.nn.functional.dropout(type_as_12, p = 0.1, training = False, inplace = False);  type_as_12 = None
    matmul_25 = torch.matmul(dropout_12, transpose_62);  dropout_12 = None
    transpose_64 = matmul_25.transpose(1, 2);  matmul_25 = None
    contiguous_12 = transpose_64.contiguous();  transpose_64 = None
    view_53 = contiguous_12.view(getitem_61, -1, 768);  contiguous_12 = getitem_61 = None
    linear_24 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_24 = patch_linear_layer_linear_layer_triton_wrapper(view_53, linear_24);  view_53 = linear_24 = None
    decoder_block_0_layer_0_dropout = getattr(getattr(self.decoder.block, "0").layer, "0").dropout(linear_layer_triton_wrapper_24);  linear_layer_triton_wrapper_24 = None
    add_95 = decoder_dropout + decoder_block_0_layer_0_dropout;  decoder_dropout = decoder_block_0_layer_0_dropout = None
    getattr_84 = add_95.dtype
    eq_59 = getattr_84 == torch.float16;  getattr_84 = None
    size_20 = transpose_61.size()
    getitem_65 = size_20[2];  size_20 = None
    to_59 = add_95.to(torch.float32)
    pow_39 = to_59.pow(2);  to_59 = None
    mean_26 = pow_39.mean(-1, keepdim = True);  pow_39 = None
    add_96 = mean_26 + 1e-06;  mean_26 = None
    rsqrt_26 = torch.rsqrt(add_96);  add_96 = None
    mul_119 = add_95 * rsqrt_26;  rsqrt_26 = None
    decoder_block_0_layer_1_layer_norm_weight = getattr(getattr(self.decoder.block, "0").layer, "1").layer_norm.weight
    getattr_85 = decoder_block_0_layer_1_layer_norm_weight.dtype
    eq_60 = getattr_85 == torch.float16;  getattr_85 = None
    getattr_86 = decoder_block_0_layer_1_layer_norm_weight.dtype
    to_60 = mul_119.to(getattr_86);  mul_119 = getattr_86 = None
    mul_120 = decoder_block_0_layer_1_layer_norm_weight * to_60;  decoder_block_0_layer_1_layer_norm_weight = to_60 = None
    size_21 = mul_120.size()
    getitem_66 = size_21[slice(None, 2, None)];  size_21 = None
    getitem_67 = getitem_66[0]
    getitem_68 = getitem_66[1];  getitem_66 = None
    size_22 = encoder_dropout_1.size()
    getitem_69 = size_22[1];  size_22 = None
    decoder_block_0_layer_1_enc_dec_attention_q = getattr(getattr(self.decoder.block, "0").layer, "1").EncDecAttention.q(mul_120);  mul_120 = None
    view_54 = decoder_block_0_layer_1_enc_dec_attention_q.view(getitem_67, -1, 12, 64);  decoder_block_0_layer_1_enc_dec_attention_q = None
    transpose_65 = view_54.transpose(1, 2);  view_54 = None
    decoder_block_0_layer_1_enc_dec_attention_k = getattr(getattr(self.decoder.block, "0").layer, "1").EncDecAttention.k(encoder_dropout_1)
    view_55 = decoder_block_0_layer_1_enc_dec_attention_k.view(getitem_67, -1, 12, 64);  decoder_block_0_layer_1_enc_dec_attention_k = None
    transpose_66 = view_55.transpose(1, 2);  view_55 = None
    decoder_block_0_layer_1_enc_dec_attention_v = getattr(getattr(self.decoder.block, "0").layer, "1").EncDecAttention.v(encoder_dropout_1)
    view_56 = decoder_block_0_layer_1_enc_dec_attention_v.view(getitem_67, -1, 12, 64);  decoder_block_0_layer_1_enc_dec_attention_v = None
    transpose_67 = view_56.transpose(1, 2);  view_56 = None
    transpose_68 = transpose_66.transpose(3, 2)
    matmul_26 = torch.matmul(transpose_65, transpose_68);  transpose_65 = transpose_68 = None
    getattr_87 = matmul_26.device
    getattr_88 = matmul_26.dtype
    zeros = torch.zeros((1, 12, getitem_68, getitem_69), device = getattr_87, dtype = getattr_88);  getitem_68 = getitem_69 = getattr_87 = getattr_88 = None
    add_97 = zeros + mul_115;  zeros = mul_115 = None
    add_98 = matmul_26 + add_97;  matmul_26 = None
    float_16 = add_98.float()
    softmax_13 = torch.nn.functional.softmax(float_16, dim = -1, _stacklevel = 3, dtype = None);  float_16 = None
    type_as_13 = softmax_13.type_as(add_98);  softmax_13 = add_98 = None
    dropout_13 = torch.nn.functional.dropout(type_as_13, p = 0.1, training = False, inplace = False);  type_as_13 = None
    matmul_27 = torch.matmul(dropout_13, transpose_67);  dropout_13 = None
    transpose_69 = matmul_27.transpose(1, 2);  matmul_27 = None
    contiguous_13 = transpose_69.contiguous();  transpose_69 = None
    view_57 = contiguous_13.view(getitem_67, -1, 768);  contiguous_13 = getitem_67 = None
    linear_25 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_25 = patch_linear_layer_linear_layer_triton_wrapper(view_57, linear_25);  view_57 = linear_25 = None
    decoder_block_0_layer_1_dropout = getattr(getattr(self.decoder.block, "0").layer, "1").dropout(linear_layer_triton_wrapper_25);  linear_layer_triton_wrapper_25 = None
    add_99 = add_95 + decoder_block_0_layer_1_dropout;  add_95 = decoder_block_0_layer_1_dropout = None
    getattr_89 = add_99.dtype
    eq_61 = getattr_89 == torch.float16;  getattr_89 = None
    to_61 = add_99.to(torch.float32)
    pow_40 = to_61.pow(2);  to_61 = None
    mean_27 = pow_40.mean(-1, keepdim = True);  pow_40 = None
    add_100 = mean_27 + 1e-06;  mean_27 = None
    rsqrt_27 = torch.rsqrt(add_100);  add_100 = None
    mul_121 = add_99 * rsqrt_27;  rsqrt_27 = None
    decoder_block_0_layer_2_layer_norm_weight = getattr(getattr(self.decoder.block, "0").layer, "2").layer_norm.weight
    getattr_90 = decoder_block_0_layer_2_layer_norm_weight.dtype
    eq_62 = getattr_90 == torch.float16;  getattr_90 = None
    getattr_91 = decoder_block_0_layer_2_layer_norm_weight.dtype
    to_62 = mul_121.to(getattr_91);  mul_121 = getattr_91 = None
    mul_122 = decoder_block_0_layer_2_layer_norm_weight * to_62;  decoder_block_0_layer_2_layer_norm_weight = to_62 = None
    decoder_block_0_layer_2_dense_relu_dense_wi_0 = getattr(getattr(self.decoder.block, "0").layer, "2").DenseReluDense.wi_0(mul_122)
    mul_123 = 0.5 * decoder_block_0_layer_2_dense_relu_dense_wi_0
    pow_41 = torch.pow(decoder_block_0_layer_2_dense_relu_dense_wi_0, 3.0)
    mul_124 = 0.044715 * pow_41;  pow_41 = None
    add_101 = decoder_block_0_layer_2_dense_relu_dense_wi_0 + mul_124;  decoder_block_0_layer_2_dense_relu_dense_wi_0 = mul_124 = None
    mul_125 = 0.7978845608028654 * add_101;  add_101 = None
    tanh_12 = torch.tanh(mul_125);  mul_125 = None
    add_102 = 1.0 + tanh_12;  tanh_12 = None
    mul_126 = mul_123 * add_102;  mul_123 = add_102 = None
    decoder_block_0_layer_2_dense_relu_dense_wi_1 = getattr(getattr(self.decoder.block, "0").layer, "2").DenseReluDense.wi_1(mul_122);  mul_122 = None
    mul_127 = mul_126 * decoder_block_0_layer_2_dense_relu_dense_wi_1;  mul_126 = decoder_block_0_layer_2_dense_relu_dense_wi_1 = None
    decoder_block_0_layer_2_dense_relu_dense_dropout = getattr(getattr(self.decoder.block, "0").layer, "2").DenseReluDense.dropout(mul_127);  mul_127 = None
    decoder_block_0_layer_2_dense_relu_dense_wo_weight = getattr(getattr(self.decoder.block, "0").layer, "2").DenseReluDense.wo.weight
    linear_26 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_26 = patch_linear_layer_linear_layer_triton_wrapper(decoder_block_0_layer_2_dense_relu_dense_dropout, linear_26);  decoder_block_0_layer_2_dense_relu_dense_dropout = linear_26 = None
    decoder_block_0_layer_2_dropout = getattr(getattr(self.decoder.block, "0").layer, "2").dropout(linear_layer_triton_wrapper_26);  linear_layer_triton_wrapper_26 = None
    add_103 = add_99 + decoder_block_0_layer_2_dropout;  add_99 = decoder_block_0_layer_2_dropout = None
    getattr_92 = add_103.dtype
    eq_63 = getattr_92 == torch.float16;  getattr_92 = None
    to_63 = add_103.to(torch.float32)
    pow_42 = to_63.pow(2);  to_63 = None
    mean_28 = pow_42.mean(-1, keepdim = True);  pow_42 = None
    add_104 = mean_28 + 1e-06;  mean_28 = None
    rsqrt_28 = torch.rsqrt(add_104);  add_104 = None
    mul_128 = add_103 * rsqrt_28;  rsqrt_28 = None
    decoder_block_1_layer_0_layer_norm_weight = getattr(getattr(self.decoder.block, "1").layer, "0").layer_norm.weight
    getattr_93 = decoder_block_1_layer_0_layer_norm_weight.dtype
    eq_64 = getattr_93 == torch.float16;  getattr_93 = None
    getattr_94 = decoder_block_1_layer_0_layer_norm_weight.dtype
    to_64 = mul_128.to(getattr_94);  mul_128 = getattr_94 = None
    mul_129 = decoder_block_1_layer_0_layer_norm_weight * to_64;  decoder_block_1_layer_0_layer_norm_weight = to_64 = None
    size_23 = mul_129.size()
    getitem_70 = size_23[slice(None, 2, None)];  size_23 = None
    getitem_71 = getitem_70[0]
    getitem_72 = getitem_70[1];  getitem_70 = None
    decoder_block_1_layer_0_self_attention_q = getattr(getattr(self.decoder.block, "1").layer, "0").SelfAttention.q(mul_129)
    view_58 = decoder_block_1_layer_0_self_attention_q.view(getitem_71, -1, 12, 64);  decoder_block_1_layer_0_self_attention_q = None
    transpose_70 = view_58.transpose(1, 2);  view_58 = None
    decoder_block_1_layer_0_self_attention_k = getattr(getattr(self.decoder.block, "1").layer, "0").SelfAttention.k(mul_129)
    view_59 = decoder_block_1_layer_0_self_attention_k.view(getitem_71, -1, 12, 64);  decoder_block_1_layer_0_self_attention_k = None
    transpose_71 = view_59.transpose(1, 2);  view_59 = None
    decoder_block_1_layer_0_self_attention_v = getattr(getattr(self.decoder.block, "1").layer, "0").SelfAttention.v(mul_129);  mul_129 = None
    view_60 = decoder_block_1_layer_0_self_attention_v.view(getitem_71, -1, 12, 64);  decoder_block_1_layer_0_self_attention_v = None
    transpose_72 = view_60.transpose(1, 2);  view_60 = None
    transpose_73 = transpose_71.transpose(3, 2)
    matmul_28 = torch.matmul(transpose_70, transpose_73);  transpose_70 = transpose_73 = None
    add_105 = matmul_28 + add_93;  matmul_28 = None
    float_17 = add_105.float()
    softmax_14 = torch.nn.functional.softmax(float_17, dim = -1, _stacklevel = 3, dtype = None);  float_17 = None
    type_as_14 = softmax_14.type_as(add_105);  softmax_14 = add_105 = None
    dropout_14 = torch.nn.functional.dropout(type_as_14, p = 0.1, training = False, inplace = False);  type_as_14 = None
    matmul_29 = torch.matmul(dropout_14, transpose_72);  dropout_14 = None
    transpose_74 = matmul_29.transpose(1, 2);  matmul_29 = None
    contiguous_14 = transpose_74.contiguous();  transpose_74 = None
    view_61 = contiguous_14.view(getitem_71, -1, 768);  contiguous_14 = getitem_71 = None
    linear_27 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_27 = patch_linear_layer_linear_layer_triton_wrapper(view_61, linear_27);  view_61 = linear_27 = None
    decoder_block_1_layer_0_dropout = getattr(getattr(self.decoder.block, "1").layer, "0").dropout(linear_layer_triton_wrapper_27);  linear_layer_triton_wrapper_27 = None
    add_106 = add_103 + decoder_block_1_layer_0_dropout;  add_103 = decoder_block_1_layer_0_dropout = None
    getattr_95 = add_106.dtype
    eq_65 = getattr_95 == torch.float16;  getattr_95 = None
    size_24 = transpose_71.size()
    getitem_73 = size_24[2];  size_24 = None
    to_65 = add_106.to(torch.float32)
    pow_43 = to_65.pow(2);  to_65 = None
    mean_29 = pow_43.mean(-1, keepdim = True);  pow_43 = None
    add_107 = mean_29 + 1e-06;  mean_29 = None
    rsqrt_29 = torch.rsqrt(add_107);  add_107 = None
    mul_130 = add_106 * rsqrt_29;  rsqrt_29 = None
    decoder_block_1_layer_1_layer_norm_weight = getattr(getattr(self.decoder.block, "1").layer, "1").layer_norm.weight
    getattr_96 = decoder_block_1_layer_1_layer_norm_weight.dtype
    eq_66 = getattr_96 == torch.float16;  getattr_96 = None
    getattr_97 = decoder_block_1_layer_1_layer_norm_weight.dtype
    to_66 = mul_130.to(getattr_97);  mul_130 = getattr_97 = None
    mul_131 = decoder_block_1_layer_1_layer_norm_weight * to_66;  decoder_block_1_layer_1_layer_norm_weight = to_66 = None
    size_25 = mul_131.size()
    getitem_74 = size_25[slice(None, 2, None)];  size_25 = None
    getitem_75 = getitem_74[0]
    getitem_76 = getitem_74[1];  getitem_74 = None
    size_26 = encoder_dropout_1.size()
    getitem_77 = size_26[1];  size_26 = None
    decoder_block_1_layer_1_enc_dec_attention_q = getattr(getattr(self.decoder.block, "1").layer, "1").EncDecAttention.q(mul_131);  mul_131 = None
    view_62 = decoder_block_1_layer_1_enc_dec_attention_q.view(getitem_75, -1, 12, 64);  decoder_block_1_layer_1_enc_dec_attention_q = None
    transpose_75 = view_62.transpose(1, 2);  view_62 = None
    decoder_block_1_layer_1_enc_dec_attention_k = getattr(getattr(self.decoder.block, "1").layer, "1").EncDecAttention.k(encoder_dropout_1)
    view_63 = decoder_block_1_layer_1_enc_dec_attention_k.view(getitem_75, -1, 12, 64);  decoder_block_1_layer_1_enc_dec_attention_k = None
    transpose_76 = view_63.transpose(1, 2);  view_63 = None
    decoder_block_1_layer_1_enc_dec_attention_v = getattr(getattr(self.decoder.block, "1").layer, "1").EncDecAttention.v(encoder_dropout_1)
    view_64 = decoder_block_1_layer_1_enc_dec_attention_v.view(getitem_75, -1, 12, 64);  decoder_block_1_layer_1_enc_dec_attention_v = None
    transpose_77 = view_64.transpose(1, 2);  view_64 = None
    transpose_78 = transpose_76.transpose(3, 2)
    matmul_30 = torch.matmul(transpose_75, transpose_78);  transpose_75 = transpose_78 = None
    add_108 = matmul_30 + add_97;  matmul_30 = None
    float_18 = add_108.float()
    softmax_15 = torch.nn.functional.softmax(float_18, dim = -1, _stacklevel = 3, dtype = None);  float_18 = None
    type_as_15 = softmax_15.type_as(add_108);  softmax_15 = add_108 = None
    dropout_15 = torch.nn.functional.dropout(type_as_15, p = 0.1, training = False, inplace = False);  type_as_15 = None
    matmul_31 = torch.matmul(dropout_15, transpose_77);  dropout_15 = None
    transpose_79 = matmul_31.transpose(1, 2);  matmul_31 = None
    contiguous_15 = transpose_79.contiguous();  transpose_79 = None
    view_65 = contiguous_15.view(getitem_75, -1, 768);  contiguous_15 = getitem_75 = None
    linear_28 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_28 = patch_linear_layer_linear_layer_triton_wrapper(view_65, linear_28);  view_65 = linear_28 = None
    decoder_block_1_layer_1_dropout = getattr(getattr(self.decoder.block, "1").layer, "1").dropout(linear_layer_triton_wrapper_28);  linear_layer_triton_wrapper_28 = None
    add_109 = add_106 + decoder_block_1_layer_1_dropout;  add_106 = decoder_block_1_layer_1_dropout = None
    getattr_98 = add_109.dtype
    eq_67 = getattr_98 == torch.float16;  getattr_98 = None
    to_67 = add_109.to(torch.float32)
    pow_44 = to_67.pow(2);  to_67 = None
    mean_30 = pow_44.mean(-1, keepdim = True);  pow_44 = None
    add_110 = mean_30 + 1e-06;  mean_30 = None
    rsqrt_30 = torch.rsqrt(add_110);  add_110 = None
    mul_132 = add_109 * rsqrt_30;  rsqrt_30 = None
    decoder_block_1_layer_2_layer_norm_weight = getattr(getattr(self.decoder.block, "1").layer, "2").layer_norm.weight
    getattr_99 = decoder_block_1_layer_2_layer_norm_weight.dtype
    eq_68 = getattr_99 == torch.float16;  getattr_99 = None
    getattr_100 = decoder_block_1_layer_2_layer_norm_weight.dtype
    to_68 = mul_132.to(getattr_100);  mul_132 = getattr_100 = None
    mul_133 = decoder_block_1_layer_2_layer_norm_weight * to_68;  decoder_block_1_layer_2_layer_norm_weight = to_68 = None
    decoder_block_1_layer_2_dense_relu_dense_wi_0 = getattr(getattr(self.decoder.block, "1").layer, "2").DenseReluDense.wi_0(mul_133)
    mul_134 = 0.5 * decoder_block_1_layer_2_dense_relu_dense_wi_0
    pow_45 = torch.pow(decoder_block_1_layer_2_dense_relu_dense_wi_0, 3.0)
    mul_135 = 0.044715 * pow_45;  pow_45 = None
    add_111 = decoder_block_1_layer_2_dense_relu_dense_wi_0 + mul_135;  decoder_block_1_layer_2_dense_relu_dense_wi_0 = mul_135 = None
    mul_136 = 0.7978845608028654 * add_111;  add_111 = None
    tanh_13 = torch.tanh(mul_136);  mul_136 = None
    add_112 = 1.0 + tanh_13;  tanh_13 = None
    mul_137 = mul_134 * add_112;  mul_134 = add_112 = None
    decoder_block_1_layer_2_dense_relu_dense_wi_1 = getattr(getattr(self.decoder.block, "1").layer, "2").DenseReluDense.wi_1(mul_133);  mul_133 = None
    mul_138 = mul_137 * decoder_block_1_layer_2_dense_relu_dense_wi_1;  mul_137 = decoder_block_1_layer_2_dense_relu_dense_wi_1 = None
    decoder_block_1_layer_2_dense_relu_dense_dropout = getattr(getattr(self.decoder.block, "1").layer, "2").DenseReluDense.dropout(mul_138);  mul_138 = None
    decoder_block_1_layer_2_dense_relu_dense_wo_weight = getattr(getattr(self.decoder.block, "1").layer, "2").DenseReluDense.wo.weight
    linear_29 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_29 = patch_linear_layer_linear_layer_triton_wrapper(decoder_block_1_layer_2_dense_relu_dense_dropout, linear_29);  decoder_block_1_layer_2_dense_relu_dense_dropout = linear_29 = None
    decoder_block_1_layer_2_dropout = getattr(getattr(self.decoder.block, "1").layer, "2").dropout(linear_layer_triton_wrapper_29);  linear_layer_triton_wrapper_29 = None
    add_113 = add_109 + decoder_block_1_layer_2_dropout;  add_109 = decoder_block_1_layer_2_dropout = None
    getattr_101 = add_113.dtype
    eq_69 = getattr_101 == torch.float16;  getattr_101 = None
    to_69 = add_113.to(torch.float32)
    pow_46 = to_69.pow(2);  to_69 = None
    mean_31 = pow_46.mean(-1, keepdim = True);  pow_46 = None
    add_114 = mean_31 + 1e-06;  mean_31 = None
    rsqrt_31 = torch.rsqrt(add_114);  add_114 = None
    mul_139 = add_113 * rsqrt_31;  rsqrt_31 = None
    decoder_block_2_layer_0_layer_norm_weight = getattr(getattr(self.decoder.block, "2").layer, "0").layer_norm.weight
    getattr_102 = decoder_block_2_layer_0_layer_norm_weight.dtype
    eq_70 = getattr_102 == torch.float16;  getattr_102 = None
    getattr_103 = decoder_block_2_layer_0_layer_norm_weight.dtype
    to_70 = mul_139.to(getattr_103);  mul_139 = getattr_103 = None
    mul_140 = decoder_block_2_layer_0_layer_norm_weight * to_70;  decoder_block_2_layer_0_layer_norm_weight = to_70 = None
    size_27 = mul_140.size()
    getitem_78 = size_27[slice(None, 2, None)];  size_27 = None
    getitem_79 = getitem_78[0]
    getitem_80 = getitem_78[1];  getitem_78 = None
    decoder_block_2_layer_0_self_attention_q = getattr(getattr(self.decoder.block, "2").layer, "0").SelfAttention.q(mul_140)
    view_66 = decoder_block_2_layer_0_self_attention_q.view(getitem_79, -1, 12, 64);  decoder_block_2_layer_0_self_attention_q = None
    transpose_80 = view_66.transpose(1, 2);  view_66 = None
    decoder_block_2_layer_0_self_attention_k = getattr(getattr(self.decoder.block, "2").layer, "0").SelfAttention.k(mul_140)
    view_67 = decoder_block_2_layer_0_self_attention_k.view(getitem_79, -1, 12, 64);  decoder_block_2_layer_0_self_attention_k = None
    transpose_81 = view_67.transpose(1, 2);  view_67 = None
    decoder_block_2_layer_0_self_attention_v = getattr(getattr(self.decoder.block, "2").layer, "0").SelfAttention.v(mul_140);  mul_140 = None
    view_68 = decoder_block_2_layer_0_self_attention_v.view(getitem_79, -1, 12, 64);  decoder_block_2_layer_0_self_attention_v = None
    transpose_82 = view_68.transpose(1, 2);  view_68 = None
    transpose_83 = transpose_81.transpose(3, 2)
    matmul_32 = torch.matmul(transpose_80, transpose_83);  transpose_80 = transpose_83 = None
    add_115 = matmul_32 + add_93;  matmul_32 = None
    float_19 = add_115.float()
    softmax_16 = torch.nn.functional.softmax(float_19, dim = -1, _stacklevel = 3, dtype = None);  float_19 = None
    type_as_16 = softmax_16.type_as(add_115);  softmax_16 = add_115 = None
    dropout_16 = torch.nn.functional.dropout(type_as_16, p = 0.1, training = False, inplace = False);  type_as_16 = None
    matmul_33 = torch.matmul(dropout_16, transpose_82);  dropout_16 = None
    transpose_84 = matmul_33.transpose(1, 2);  matmul_33 = None
    contiguous_16 = transpose_84.contiguous();  transpose_84 = None
    view_69 = contiguous_16.view(getitem_79, -1, 768);  contiguous_16 = getitem_79 = None
    linear_30 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_30 = patch_linear_layer_linear_layer_triton_wrapper(view_69, linear_30);  view_69 = linear_30 = None
    decoder_block_2_layer_0_dropout = getattr(getattr(self.decoder.block, "2").layer, "0").dropout(linear_layer_triton_wrapper_30);  linear_layer_triton_wrapper_30 = None
    add_116 = add_113 + decoder_block_2_layer_0_dropout;  add_113 = decoder_block_2_layer_0_dropout = None
    getattr_104 = add_116.dtype
    eq_71 = getattr_104 == torch.float16;  getattr_104 = None
    size_28 = transpose_81.size()
    getitem_81 = size_28[2];  size_28 = None
    to_71 = add_116.to(torch.float32)
    pow_47 = to_71.pow(2);  to_71 = None
    mean_32 = pow_47.mean(-1, keepdim = True);  pow_47 = None
    add_117 = mean_32 + 1e-06;  mean_32 = None
    rsqrt_32 = torch.rsqrt(add_117);  add_117 = None
    mul_141 = add_116 * rsqrt_32;  rsqrt_32 = None
    decoder_block_2_layer_1_layer_norm_weight = getattr(getattr(self.decoder.block, "2").layer, "1").layer_norm.weight
    getattr_105 = decoder_block_2_layer_1_layer_norm_weight.dtype
    eq_72 = getattr_105 == torch.float16;  getattr_105 = None
    getattr_106 = decoder_block_2_layer_1_layer_norm_weight.dtype
    to_72 = mul_141.to(getattr_106);  mul_141 = getattr_106 = None
    mul_142 = decoder_block_2_layer_1_layer_norm_weight * to_72;  decoder_block_2_layer_1_layer_norm_weight = to_72 = None
    size_29 = mul_142.size()
    getitem_82 = size_29[slice(None, 2, None)];  size_29 = None
    getitem_83 = getitem_82[0]
    getitem_84 = getitem_82[1];  getitem_82 = None
    size_30 = encoder_dropout_1.size()
    getitem_85 = size_30[1];  size_30 = None
    decoder_block_2_layer_1_enc_dec_attention_q = getattr(getattr(self.decoder.block, "2").layer, "1").EncDecAttention.q(mul_142);  mul_142 = None
    view_70 = decoder_block_2_layer_1_enc_dec_attention_q.view(getitem_83, -1, 12, 64);  decoder_block_2_layer_1_enc_dec_attention_q = None
    transpose_85 = view_70.transpose(1, 2);  view_70 = None
    decoder_block_2_layer_1_enc_dec_attention_k = getattr(getattr(self.decoder.block, "2").layer, "1").EncDecAttention.k(encoder_dropout_1)
    view_71 = decoder_block_2_layer_1_enc_dec_attention_k.view(getitem_83, -1, 12, 64);  decoder_block_2_layer_1_enc_dec_attention_k = None
    transpose_86 = view_71.transpose(1, 2);  view_71 = None
    decoder_block_2_layer_1_enc_dec_attention_v = getattr(getattr(self.decoder.block, "2").layer, "1").EncDecAttention.v(encoder_dropout_1)
    view_72 = decoder_block_2_layer_1_enc_dec_attention_v.view(getitem_83, -1, 12, 64);  decoder_block_2_layer_1_enc_dec_attention_v = None
    transpose_87 = view_72.transpose(1, 2);  view_72 = None
    transpose_88 = transpose_86.transpose(3, 2)
    matmul_34 = torch.matmul(transpose_85, transpose_88);  transpose_85 = transpose_88 = None
    add_118 = matmul_34 + add_97;  matmul_34 = None
    float_20 = add_118.float()
    softmax_17 = torch.nn.functional.softmax(float_20, dim = -1, _stacklevel = 3, dtype = None);  float_20 = None
    type_as_17 = softmax_17.type_as(add_118);  softmax_17 = add_118 = None
    dropout_17 = torch.nn.functional.dropout(type_as_17, p = 0.1, training = False, inplace = False);  type_as_17 = None
    matmul_35 = torch.matmul(dropout_17, transpose_87);  dropout_17 = None
    transpose_89 = matmul_35.transpose(1, 2);  matmul_35 = None
    contiguous_17 = transpose_89.contiguous();  transpose_89 = None
    view_73 = contiguous_17.view(getitem_83, -1, 768);  contiguous_17 = getitem_83 = None
    linear_31 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_31 = patch_linear_layer_linear_layer_triton_wrapper(view_73, linear_31);  view_73 = linear_31 = None
    decoder_block_2_layer_1_dropout = getattr(getattr(self.decoder.block, "2").layer, "1").dropout(linear_layer_triton_wrapper_31);  linear_layer_triton_wrapper_31 = None
    add_119 = add_116 + decoder_block_2_layer_1_dropout;  add_116 = decoder_block_2_layer_1_dropout = None
    getattr_107 = add_119.dtype
    eq_73 = getattr_107 == torch.float16;  getattr_107 = None
    to_73 = add_119.to(torch.float32)
    pow_48 = to_73.pow(2);  to_73 = None
    mean_33 = pow_48.mean(-1, keepdim = True);  pow_48 = None
    add_120 = mean_33 + 1e-06;  mean_33 = None
    rsqrt_33 = torch.rsqrt(add_120);  add_120 = None
    mul_143 = add_119 * rsqrt_33;  rsqrt_33 = None
    decoder_block_2_layer_2_layer_norm_weight = getattr(getattr(self.decoder.block, "2").layer, "2").layer_norm.weight
    getattr_108 = decoder_block_2_layer_2_layer_norm_weight.dtype
    eq_74 = getattr_108 == torch.float16;  getattr_108 = None
    getattr_109 = decoder_block_2_layer_2_layer_norm_weight.dtype
    to_74 = mul_143.to(getattr_109);  mul_143 = getattr_109 = None
    mul_144 = decoder_block_2_layer_2_layer_norm_weight * to_74;  decoder_block_2_layer_2_layer_norm_weight = to_74 = None
    decoder_block_2_layer_2_dense_relu_dense_wi_0 = getattr(getattr(self.decoder.block, "2").layer, "2").DenseReluDense.wi_0(mul_144)
    mul_145 = 0.5 * decoder_block_2_layer_2_dense_relu_dense_wi_0
    pow_49 = torch.pow(decoder_block_2_layer_2_dense_relu_dense_wi_0, 3.0)
    mul_146 = 0.044715 * pow_49;  pow_49 = None
    add_121 = decoder_block_2_layer_2_dense_relu_dense_wi_0 + mul_146;  decoder_block_2_layer_2_dense_relu_dense_wi_0 = mul_146 = None
    mul_147 = 0.7978845608028654 * add_121;  add_121 = None
    tanh_14 = torch.tanh(mul_147);  mul_147 = None
    add_122 = 1.0 + tanh_14;  tanh_14 = None
    mul_148 = mul_145 * add_122;  mul_145 = add_122 = None
    decoder_block_2_layer_2_dense_relu_dense_wi_1 = getattr(getattr(self.decoder.block, "2").layer, "2").DenseReluDense.wi_1(mul_144);  mul_144 = None
    mul_149 = mul_148 * decoder_block_2_layer_2_dense_relu_dense_wi_1;  mul_148 = decoder_block_2_layer_2_dense_relu_dense_wi_1 = None
    decoder_block_2_layer_2_dense_relu_dense_dropout = getattr(getattr(self.decoder.block, "2").layer, "2").DenseReluDense.dropout(mul_149);  mul_149 = None
    decoder_block_2_layer_2_dense_relu_dense_wo_weight = getattr(getattr(self.decoder.block, "2").layer, "2").DenseReluDense.wo.weight
    linear_32 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_32 = patch_linear_layer_linear_layer_triton_wrapper(decoder_block_2_layer_2_dense_relu_dense_dropout, linear_32);  decoder_block_2_layer_2_dense_relu_dense_dropout = linear_32 = None
    decoder_block_2_layer_2_dropout = getattr(getattr(self.decoder.block, "2").layer, "2").dropout(linear_layer_triton_wrapper_32);  linear_layer_triton_wrapper_32 = None
    add_123 = add_119 + decoder_block_2_layer_2_dropout;  add_119 = decoder_block_2_layer_2_dropout = None
    getattr_110 = add_123.dtype
    eq_75 = getattr_110 == torch.float16;  getattr_110 = None
    to_75 = add_123.to(torch.float32)
    pow_50 = to_75.pow(2);  to_75 = None
    mean_34 = pow_50.mean(-1, keepdim = True);  pow_50 = None
    add_124 = mean_34 + 1e-06;  mean_34 = None
    rsqrt_34 = torch.rsqrt(add_124);  add_124 = None
    mul_150 = add_123 * rsqrt_34;  rsqrt_34 = None
    decoder_block_3_layer_0_layer_norm_weight = getattr(getattr(self.decoder.block, "3").layer, "0").layer_norm.weight
    getattr_111 = decoder_block_3_layer_0_layer_norm_weight.dtype
    eq_76 = getattr_111 == torch.float16;  getattr_111 = None
    getattr_112 = decoder_block_3_layer_0_layer_norm_weight.dtype
    to_76 = mul_150.to(getattr_112);  mul_150 = getattr_112 = None
    mul_151 = decoder_block_3_layer_0_layer_norm_weight * to_76;  decoder_block_3_layer_0_layer_norm_weight = to_76 = None
    size_31 = mul_151.size()
    getitem_86 = size_31[slice(None, 2, None)];  size_31 = None
    getitem_87 = getitem_86[0]
    getitem_88 = getitem_86[1];  getitem_86 = None
    decoder_block_3_layer_0_self_attention_q = getattr(getattr(self.decoder.block, "3").layer, "0").SelfAttention.q(mul_151)
    view_74 = decoder_block_3_layer_0_self_attention_q.view(getitem_87, -1, 12, 64);  decoder_block_3_layer_0_self_attention_q = None
    transpose_90 = view_74.transpose(1, 2);  view_74 = None
    decoder_block_3_layer_0_self_attention_k = getattr(getattr(self.decoder.block, "3").layer, "0").SelfAttention.k(mul_151)
    view_75 = decoder_block_3_layer_0_self_attention_k.view(getitem_87, -1, 12, 64);  decoder_block_3_layer_0_self_attention_k = None
    transpose_91 = view_75.transpose(1, 2);  view_75 = None
    decoder_block_3_layer_0_self_attention_v = getattr(getattr(self.decoder.block, "3").layer, "0").SelfAttention.v(mul_151);  mul_151 = None
    view_76 = decoder_block_3_layer_0_self_attention_v.view(getitem_87, -1, 12, 64);  decoder_block_3_layer_0_self_attention_v = None
    transpose_92 = view_76.transpose(1, 2);  view_76 = None
    transpose_93 = transpose_91.transpose(3, 2)
    matmul_36 = torch.matmul(transpose_90, transpose_93);  transpose_90 = transpose_93 = None
    add_125 = matmul_36 + add_93;  matmul_36 = None
    float_21 = add_125.float()
    softmax_18 = torch.nn.functional.softmax(float_21, dim = -1, _stacklevel = 3, dtype = None);  float_21 = None
    type_as_18 = softmax_18.type_as(add_125);  softmax_18 = add_125 = None
    dropout_18 = torch.nn.functional.dropout(type_as_18, p = 0.1, training = False, inplace = False);  type_as_18 = None
    matmul_37 = torch.matmul(dropout_18, transpose_92);  dropout_18 = None
    transpose_94 = matmul_37.transpose(1, 2);  matmul_37 = None
    contiguous_18 = transpose_94.contiguous();  transpose_94 = None
    view_77 = contiguous_18.view(getitem_87, -1, 768);  contiguous_18 = getitem_87 = None
    linear_33 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_33 = patch_linear_layer_linear_layer_triton_wrapper(view_77, linear_33);  view_77 = linear_33 = None
    decoder_block_3_layer_0_dropout = getattr(getattr(self.decoder.block, "3").layer, "0").dropout(linear_layer_triton_wrapper_33);  linear_layer_triton_wrapper_33 = None
    add_126 = add_123 + decoder_block_3_layer_0_dropout;  add_123 = decoder_block_3_layer_0_dropout = None
    getattr_113 = add_126.dtype
    eq_77 = getattr_113 == torch.float16;  getattr_113 = None
    size_32 = transpose_91.size()
    getitem_89 = size_32[2];  size_32 = None
    to_77 = add_126.to(torch.float32)
    pow_51 = to_77.pow(2);  to_77 = None
    mean_35 = pow_51.mean(-1, keepdim = True);  pow_51 = None
    add_127 = mean_35 + 1e-06;  mean_35 = None
    rsqrt_35 = torch.rsqrt(add_127);  add_127 = None
    mul_152 = add_126 * rsqrt_35;  rsqrt_35 = None
    decoder_block_3_layer_1_layer_norm_weight = getattr(getattr(self.decoder.block, "3").layer, "1").layer_norm.weight
    getattr_114 = decoder_block_3_layer_1_layer_norm_weight.dtype
    eq_78 = getattr_114 == torch.float16;  getattr_114 = None
    getattr_115 = decoder_block_3_layer_1_layer_norm_weight.dtype
    to_78 = mul_152.to(getattr_115);  mul_152 = getattr_115 = None
    mul_153 = decoder_block_3_layer_1_layer_norm_weight * to_78;  decoder_block_3_layer_1_layer_norm_weight = to_78 = None
    size_33 = mul_153.size()
    getitem_90 = size_33[slice(None, 2, None)];  size_33 = None
    getitem_91 = getitem_90[0]
    getitem_92 = getitem_90[1];  getitem_90 = None
    size_34 = encoder_dropout_1.size()
    getitem_93 = size_34[1];  size_34 = None
    decoder_block_3_layer_1_enc_dec_attention_q = getattr(getattr(self.decoder.block, "3").layer, "1").EncDecAttention.q(mul_153);  mul_153 = None
    view_78 = decoder_block_3_layer_1_enc_dec_attention_q.view(getitem_91, -1, 12, 64);  decoder_block_3_layer_1_enc_dec_attention_q = None
    transpose_95 = view_78.transpose(1, 2);  view_78 = None
    decoder_block_3_layer_1_enc_dec_attention_k = getattr(getattr(self.decoder.block, "3").layer, "1").EncDecAttention.k(encoder_dropout_1)
    view_79 = decoder_block_3_layer_1_enc_dec_attention_k.view(getitem_91, -1, 12, 64);  decoder_block_3_layer_1_enc_dec_attention_k = None
    transpose_96 = view_79.transpose(1, 2);  view_79 = None
    decoder_block_3_layer_1_enc_dec_attention_v = getattr(getattr(self.decoder.block, "3").layer, "1").EncDecAttention.v(encoder_dropout_1)
    view_80 = decoder_block_3_layer_1_enc_dec_attention_v.view(getitem_91, -1, 12, 64);  decoder_block_3_layer_1_enc_dec_attention_v = None
    transpose_97 = view_80.transpose(1, 2);  view_80 = None
    transpose_98 = transpose_96.transpose(3, 2)
    matmul_38 = torch.matmul(transpose_95, transpose_98);  transpose_95 = transpose_98 = None
    add_128 = matmul_38 + add_97;  matmul_38 = None
    float_22 = add_128.float()
    softmax_19 = torch.nn.functional.softmax(float_22, dim = -1, _stacklevel = 3, dtype = None);  float_22 = None
    type_as_19 = softmax_19.type_as(add_128);  softmax_19 = add_128 = None
    dropout_19 = torch.nn.functional.dropout(type_as_19, p = 0.1, training = False, inplace = False);  type_as_19 = None
    matmul_39 = torch.matmul(dropout_19, transpose_97);  dropout_19 = None
    transpose_99 = matmul_39.transpose(1, 2);  matmul_39 = None
    contiguous_19 = transpose_99.contiguous();  transpose_99 = None
    view_81 = contiguous_19.view(getitem_91, -1, 768);  contiguous_19 = getitem_91 = None
    linear_34 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_34 = patch_linear_layer_linear_layer_triton_wrapper(view_81, linear_34);  view_81 = linear_34 = None
    decoder_block_3_layer_1_dropout = getattr(getattr(self.decoder.block, "3").layer, "1").dropout(linear_layer_triton_wrapper_34);  linear_layer_triton_wrapper_34 = None
    add_129 = add_126 + decoder_block_3_layer_1_dropout;  add_126 = decoder_block_3_layer_1_dropout = None
    getattr_116 = add_129.dtype
    eq_79 = getattr_116 == torch.float16;  getattr_116 = None
    to_79 = add_129.to(torch.float32)
    pow_52 = to_79.pow(2);  to_79 = None
    mean_36 = pow_52.mean(-1, keepdim = True);  pow_52 = None
    add_130 = mean_36 + 1e-06;  mean_36 = None
    rsqrt_36 = torch.rsqrt(add_130);  add_130 = None
    mul_154 = add_129 * rsqrt_36;  rsqrt_36 = None
    decoder_block_3_layer_2_layer_norm_weight = getattr(getattr(self.decoder.block, "3").layer, "2").layer_norm.weight
    getattr_117 = decoder_block_3_layer_2_layer_norm_weight.dtype
    eq_80 = getattr_117 == torch.float16;  getattr_117 = None
    getattr_118 = decoder_block_3_layer_2_layer_norm_weight.dtype
    to_80 = mul_154.to(getattr_118);  mul_154 = getattr_118 = None
    mul_155 = decoder_block_3_layer_2_layer_norm_weight * to_80;  decoder_block_3_layer_2_layer_norm_weight = to_80 = None
    decoder_block_3_layer_2_dense_relu_dense_wi_0 = getattr(getattr(self.decoder.block, "3").layer, "2").DenseReluDense.wi_0(mul_155)
    mul_156 = 0.5 * decoder_block_3_layer_2_dense_relu_dense_wi_0
    pow_53 = torch.pow(decoder_block_3_layer_2_dense_relu_dense_wi_0, 3.0)
    mul_157 = 0.044715 * pow_53;  pow_53 = None
    add_131 = decoder_block_3_layer_2_dense_relu_dense_wi_0 + mul_157;  decoder_block_3_layer_2_dense_relu_dense_wi_0 = mul_157 = None
    mul_158 = 0.7978845608028654 * add_131;  add_131 = None
    tanh_15 = torch.tanh(mul_158);  mul_158 = None
    add_132 = 1.0 + tanh_15;  tanh_15 = None
    mul_159 = mul_156 * add_132;  mul_156 = add_132 = None
    decoder_block_3_layer_2_dense_relu_dense_wi_1 = getattr(getattr(self.decoder.block, "3").layer, "2").DenseReluDense.wi_1(mul_155);  mul_155 = None
    mul_160 = mul_159 * decoder_block_3_layer_2_dense_relu_dense_wi_1;  mul_159 = decoder_block_3_layer_2_dense_relu_dense_wi_1 = None
    decoder_block_3_layer_2_dense_relu_dense_dropout = getattr(getattr(self.decoder.block, "3").layer, "2").DenseReluDense.dropout(mul_160);  mul_160 = None
    decoder_block_3_layer_2_dense_relu_dense_wo_weight = getattr(getattr(self.decoder.block, "3").layer, "2").DenseReluDense.wo.weight
    linear_35 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_35 = patch_linear_layer_linear_layer_triton_wrapper(decoder_block_3_layer_2_dense_relu_dense_dropout, linear_35);  decoder_block_3_layer_2_dense_relu_dense_dropout = linear_35 = None
    decoder_block_3_layer_2_dropout = getattr(getattr(self.decoder.block, "3").layer, "2").dropout(linear_layer_triton_wrapper_35);  linear_layer_triton_wrapper_35 = None
    add_133 = add_129 + decoder_block_3_layer_2_dropout;  add_129 = decoder_block_3_layer_2_dropout = None
    getattr_119 = add_133.dtype
    eq_81 = getattr_119 == torch.float16;  getattr_119 = None
    to_81 = add_133.to(torch.float32)
    pow_54 = to_81.pow(2);  to_81 = None
    mean_37 = pow_54.mean(-1, keepdim = True);  pow_54 = None
    add_134 = mean_37 + 1e-06;  mean_37 = None
    rsqrt_37 = torch.rsqrt(add_134);  add_134 = None
    mul_161 = add_133 * rsqrt_37;  rsqrt_37 = None
    decoder_block_4_layer_0_layer_norm_weight = getattr(getattr(self.decoder.block, "4").layer, "0").layer_norm.weight
    getattr_120 = decoder_block_4_layer_0_layer_norm_weight.dtype
    eq_82 = getattr_120 == torch.float16;  getattr_120 = None
    getattr_121 = decoder_block_4_layer_0_layer_norm_weight.dtype
    to_82 = mul_161.to(getattr_121);  mul_161 = getattr_121 = None
    mul_162 = decoder_block_4_layer_0_layer_norm_weight * to_82;  decoder_block_4_layer_0_layer_norm_weight = to_82 = None
    size_35 = mul_162.size()
    getitem_94 = size_35[slice(None, 2, None)];  size_35 = None
    getitem_95 = getitem_94[0]
    getitem_96 = getitem_94[1];  getitem_94 = None
    decoder_block_4_layer_0_self_attention_q = getattr(getattr(self.decoder.block, "4").layer, "0").SelfAttention.q(mul_162)
    view_82 = decoder_block_4_layer_0_self_attention_q.view(getitem_95, -1, 12, 64);  decoder_block_4_layer_0_self_attention_q = None
    transpose_100 = view_82.transpose(1, 2);  view_82 = None
    decoder_block_4_layer_0_self_attention_k = getattr(getattr(self.decoder.block, "4").layer, "0").SelfAttention.k(mul_162)
    view_83 = decoder_block_4_layer_0_self_attention_k.view(getitem_95, -1, 12, 64);  decoder_block_4_layer_0_self_attention_k = None
    transpose_101 = view_83.transpose(1, 2);  view_83 = None
    decoder_block_4_layer_0_self_attention_v = getattr(getattr(self.decoder.block, "4").layer, "0").SelfAttention.v(mul_162);  mul_162 = None
    view_84 = decoder_block_4_layer_0_self_attention_v.view(getitem_95, -1, 12, 64);  decoder_block_4_layer_0_self_attention_v = None
    transpose_102 = view_84.transpose(1, 2);  view_84 = None
    transpose_103 = transpose_101.transpose(3, 2)
    matmul_40 = torch.matmul(transpose_100, transpose_103);  transpose_100 = transpose_103 = None
    add_135 = matmul_40 + add_93;  matmul_40 = None
    float_23 = add_135.float()
    softmax_20 = torch.nn.functional.softmax(float_23, dim = -1, _stacklevel = 3, dtype = None);  float_23 = None
    type_as_20 = softmax_20.type_as(add_135);  softmax_20 = add_135 = None
    dropout_20 = torch.nn.functional.dropout(type_as_20, p = 0.1, training = False, inplace = False);  type_as_20 = None
    matmul_41 = torch.matmul(dropout_20, transpose_102);  dropout_20 = None
    transpose_104 = matmul_41.transpose(1, 2);  matmul_41 = None
    contiguous_20 = transpose_104.contiguous();  transpose_104 = None
    view_85 = contiguous_20.view(getitem_95, -1, 768);  contiguous_20 = getitem_95 = None
    linear_36 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_36 = patch_linear_layer_linear_layer_triton_wrapper(view_85, linear_36);  view_85 = linear_36 = None
    decoder_block_4_layer_0_dropout = getattr(getattr(self.decoder.block, "4").layer, "0").dropout(linear_layer_triton_wrapper_36);  linear_layer_triton_wrapper_36 = None
    add_136 = add_133 + decoder_block_4_layer_0_dropout;  add_133 = decoder_block_4_layer_0_dropout = None
    getattr_122 = add_136.dtype
    eq_83 = getattr_122 == torch.float16;  getattr_122 = None
    size_36 = transpose_101.size()
    getitem_97 = size_36[2];  size_36 = None
    to_83 = add_136.to(torch.float32)
    pow_55 = to_83.pow(2);  to_83 = None
    mean_38 = pow_55.mean(-1, keepdim = True);  pow_55 = None
    add_137 = mean_38 + 1e-06;  mean_38 = None
    rsqrt_38 = torch.rsqrt(add_137);  add_137 = None
    mul_163 = add_136 * rsqrt_38;  rsqrt_38 = None
    decoder_block_4_layer_1_layer_norm_weight = getattr(getattr(self.decoder.block, "4").layer, "1").layer_norm.weight
    getattr_123 = decoder_block_4_layer_1_layer_norm_weight.dtype
    eq_84 = getattr_123 == torch.float16;  getattr_123 = None
    getattr_124 = decoder_block_4_layer_1_layer_norm_weight.dtype
    to_84 = mul_163.to(getattr_124);  mul_163 = getattr_124 = None
    mul_164 = decoder_block_4_layer_1_layer_norm_weight * to_84;  decoder_block_4_layer_1_layer_norm_weight = to_84 = None
    size_37 = mul_164.size()
    getitem_98 = size_37[slice(None, 2, None)];  size_37 = None
    getitem_99 = getitem_98[0]
    getitem_100 = getitem_98[1];  getitem_98 = None
    size_38 = encoder_dropout_1.size()
    getitem_101 = size_38[1];  size_38 = None
    decoder_block_4_layer_1_enc_dec_attention_q = getattr(getattr(self.decoder.block, "4").layer, "1").EncDecAttention.q(mul_164);  mul_164 = None
    view_86 = decoder_block_4_layer_1_enc_dec_attention_q.view(getitem_99, -1, 12, 64);  decoder_block_4_layer_1_enc_dec_attention_q = None
    transpose_105 = view_86.transpose(1, 2);  view_86 = None
    decoder_block_4_layer_1_enc_dec_attention_k = getattr(getattr(self.decoder.block, "4").layer, "1").EncDecAttention.k(encoder_dropout_1)
    view_87 = decoder_block_4_layer_1_enc_dec_attention_k.view(getitem_99, -1, 12, 64);  decoder_block_4_layer_1_enc_dec_attention_k = None
    transpose_106 = view_87.transpose(1, 2);  view_87 = None
    decoder_block_4_layer_1_enc_dec_attention_v = getattr(getattr(self.decoder.block, "4").layer, "1").EncDecAttention.v(encoder_dropout_1)
    view_88 = decoder_block_4_layer_1_enc_dec_attention_v.view(getitem_99, -1, 12, 64);  decoder_block_4_layer_1_enc_dec_attention_v = None
    transpose_107 = view_88.transpose(1, 2);  view_88 = None
    transpose_108 = transpose_106.transpose(3, 2)
    matmul_42 = torch.matmul(transpose_105, transpose_108);  transpose_105 = transpose_108 = None
    add_138 = matmul_42 + add_97;  matmul_42 = None
    float_24 = add_138.float()
    softmax_21 = torch.nn.functional.softmax(float_24, dim = -1, _stacklevel = 3, dtype = None);  float_24 = None
    type_as_21 = softmax_21.type_as(add_138);  softmax_21 = add_138 = None
    dropout_21 = torch.nn.functional.dropout(type_as_21, p = 0.1, training = False, inplace = False);  type_as_21 = None
    matmul_43 = torch.matmul(dropout_21, transpose_107);  dropout_21 = None
    transpose_109 = matmul_43.transpose(1, 2);  matmul_43 = None
    contiguous_21 = transpose_109.contiguous();  transpose_109 = None
    view_89 = contiguous_21.view(getitem_99, -1, 768);  contiguous_21 = getitem_99 = None
    linear_37 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_37 = patch_linear_layer_linear_layer_triton_wrapper(view_89, linear_37);  view_89 = linear_37 = None
    decoder_block_4_layer_1_dropout = getattr(getattr(self.decoder.block, "4").layer, "1").dropout(linear_layer_triton_wrapper_37);  linear_layer_triton_wrapper_37 = None
    add_139 = add_136 + decoder_block_4_layer_1_dropout;  add_136 = decoder_block_4_layer_1_dropout = None
    getattr_125 = add_139.dtype
    eq_85 = getattr_125 == torch.float16;  getattr_125 = None
    to_85 = add_139.to(torch.float32)
    pow_56 = to_85.pow(2);  to_85 = None
    mean_39 = pow_56.mean(-1, keepdim = True);  pow_56 = None
    add_140 = mean_39 + 1e-06;  mean_39 = None
    rsqrt_39 = torch.rsqrt(add_140);  add_140 = None
    mul_165 = add_139 * rsqrt_39;  rsqrt_39 = None
    decoder_block_4_layer_2_layer_norm_weight = getattr(getattr(self.decoder.block, "4").layer, "2").layer_norm.weight
    getattr_126 = decoder_block_4_layer_2_layer_norm_weight.dtype
    eq_86 = getattr_126 == torch.float16;  getattr_126 = None
    getattr_127 = decoder_block_4_layer_2_layer_norm_weight.dtype
    to_86 = mul_165.to(getattr_127);  mul_165 = getattr_127 = None
    mul_166 = decoder_block_4_layer_2_layer_norm_weight * to_86;  decoder_block_4_layer_2_layer_norm_weight = to_86 = None
    decoder_block_4_layer_2_dense_relu_dense_wi_0 = getattr(getattr(self.decoder.block, "4").layer, "2").DenseReluDense.wi_0(mul_166)
    mul_167 = 0.5 * decoder_block_4_layer_2_dense_relu_dense_wi_0
    pow_57 = torch.pow(decoder_block_4_layer_2_dense_relu_dense_wi_0, 3.0)
    mul_168 = 0.044715 * pow_57;  pow_57 = None
    add_141 = decoder_block_4_layer_2_dense_relu_dense_wi_0 + mul_168;  decoder_block_4_layer_2_dense_relu_dense_wi_0 = mul_168 = None
    mul_169 = 0.7978845608028654 * add_141;  add_141 = None
    tanh_16 = torch.tanh(mul_169);  mul_169 = None
    add_142 = 1.0 + tanh_16;  tanh_16 = None
    mul_170 = mul_167 * add_142;  mul_167 = add_142 = None
    decoder_block_4_layer_2_dense_relu_dense_wi_1 = getattr(getattr(self.decoder.block, "4").layer, "2").DenseReluDense.wi_1(mul_166);  mul_166 = None
    mul_171 = mul_170 * decoder_block_4_layer_2_dense_relu_dense_wi_1;  mul_170 = decoder_block_4_layer_2_dense_relu_dense_wi_1 = None
    decoder_block_4_layer_2_dense_relu_dense_dropout = getattr(getattr(self.decoder.block, "4").layer, "2").DenseReluDense.dropout(mul_171);  mul_171 = None
    decoder_block_4_layer_2_dense_relu_dense_wo_weight = getattr(getattr(self.decoder.block, "4").layer, "2").DenseReluDense.wo.weight
    linear_38 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_38 = patch_linear_layer_linear_layer_triton_wrapper(decoder_block_4_layer_2_dense_relu_dense_dropout, linear_38);  decoder_block_4_layer_2_dense_relu_dense_dropout = linear_38 = None
    decoder_block_4_layer_2_dropout = getattr(getattr(self.decoder.block, "4").layer, "2").dropout(linear_layer_triton_wrapper_38);  linear_layer_triton_wrapper_38 = None
    add_143 = add_139 + decoder_block_4_layer_2_dropout;  add_139 = decoder_block_4_layer_2_dropout = None
    getattr_128 = add_143.dtype
    eq_87 = getattr_128 == torch.float16;  getattr_128 = None
    to_87 = add_143.to(torch.float32)
    pow_58 = to_87.pow(2);  to_87 = None
    mean_40 = pow_58.mean(-1, keepdim = True);  pow_58 = None
    add_144 = mean_40 + 1e-06;  mean_40 = None
    rsqrt_40 = torch.rsqrt(add_144);  add_144 = None
    mul_172 = add_143 * rsqrt_40;  rsqrt_40 = None
    decoder_block_5_layer_0_layer_norm_weight = getattr(getattr(self.decoder.block, "5").layer, "0").layer_norm.weight
    getattr_129 = decoder_block_5_layer_0_layer_norm_weight.dtype
    eq_88 = getattr_129 == torch.float16;  getattr_129 = None
    getattr_130 = decoder_block_5_layer_0_layer_norm_weight.dtype
    to_88 = mul_172.to(getattr_130);  mul_172 = getattr_130 = None
    mul_173 = decoder_block_5_layer_0_layer_norm_weight * to_88;  decoder_block_5_layer_0_layer_norm_weight = to_88 = None
    size_39 = mul_173.size()
    getitem_102 = size_39[slice(None, 2, None)];  size_39 = None
    getitem_103 = getitem_102[0]
    getitem_104 = getitem_102[1];  getitem_102 = None
    decoder_block_5_layer_0_self_attention_q = getattr(getattr(self.decoder.block, "5").layer, "0").SelfAttention.q(mul_173)
    view_90 = decoder_block_5_layer_0_self_attention_q.view(getitem_103, -1, 12, 64);  decoder_block_5_layer_0_self_attention_q = None
    transpose_110 = view_90.transpose(1, 2);  view_90 = None
    decoder_block_5_layer_0_self_attention_k = getattr(getattr(self.decoder.block, "5").layer, "0").SelfAttention.k(mul_173)
    view_91 = decoder_block_5_layer_0_self_attention_k.view(getitem_103, -1, 12, 64);  decoder_block_5_layer_0_self_attention_k = None
    transpose_111 = view_91.transpose(1, 2);  view_91 = None
    decoder_block_5_layer_0_self_attention_v = getattr(getattr(self.decoder.block, "5").layer, "0").SelfAttention.v(mul_173);  mul_173 = None
    view_92 = decoder_block_5_layer_0_self_attention_v.view(getitem_103, -1, 12, 64);  decoder_block_5_layer_0_self_attention_v = None
    transpose_112 = view_92.transpose(1, 2);  view_92 = None
    transpose_113 = transpose_111.transpose(3, 2)
    matmul_44 = torch.matmul(transpose_110, transpose_113);  transpose_110 = transpose_113 = None
    add_145 = matmul_44 + add_93;  matmul_44 = None
    float_25 = add_145.float()
    softmax_22 = torch.nn.functional.softmax(float_25, dim = -1, _stacklevel = 3, dtype = None);  float_25 = None
    type_as_22 = softmax_22.type_as(add_145);  softmax_22 = add_145 = None
    dropout_22 = torch.nn.functional.dropout(type_as_22, p = 0.1, training = False, inplace = False);  type_as_22 = None
    matmul_45 = torch.matmul(dropout_22, transpose_112);  dropout_22 = None
    transpose_114 = matmul_45.transpose(1, 2);  matmul_45 = None
    contiguous_22 = transpose_114.contiguous();  transpose_114 = None
    view_93 = contiguous_22.view(getitem_103, -1, 768);  contiguous_22 = getitem_103 = None
    linear_39 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_39 = patch_linear_layer_linear_layer_triton_wrapper(view_93, linear_39);  view_93 = linear_39 = None
    decoder_block_5_layer_0_dropout = getattr(getattr(self.decoder.block, "5").layer, "0").dropout(linear_layer_triton_wrapper_39);  linear_layer_triton_wrapper_39 = None
    add_146 = add_143 + decoder_block_5_layer_0_dropout;  add_143 = decoder_block_5_layer_0_dropout = None
    getattr_131 = add_146.dtype
    eq_89 = getattr_131 == torch.float16;  getattr_131 = None
    size_40 = transpose_111.size()
    getitem_105 = size_40[2];  size_40 = None
    to_89 = add_146.to(torch.float32)
    pow_59 = to_89.pow(2);  to_89 = None
    mean_41 = pow_59.mean(-1, keepdim = True);  pow_59 = None
    add_147 = mean_41 + 1e-06;  mean_41 = None
    rsqrt_41 = torch.rsqrt(add_147);  add_147 = None
    mul_174 = add_146 * rsqrt_41;  rsqrt_41 = None
    decoder_block_5_layer_1_layer_norm_weight = getattr(getattr(self.decoder.block, "5").layer, "1").layer_norm.weight
    getattr_132 = decoder_block_5_layer_1_layer_norm_weight.dtype
    eq_90 = getattr_132 == torch.float16;  getattr_132 = None
    getattr_133 = decoder_block_5_layer_1_layer_norm_weight.dtype
    to_90 = mul_174.to(getattr_133);  mul_174 = getattr_133 = None
    mul_175 = decoder_block_5_layer_1_layer_norm_weight * to_90;  decoder_block_5_layer_1_layer_norm_weight = to_90 = None
    size_41 = mul_175.size()
    getitem_106 = size_41[slice(None, 2, None)];  size_41 = None
    getitem_107 = getitem_106[0]
    getitem_108 = getitem_106[1];  getitem_106 = None
    size_42 = encoder_dropout_1.size()
    getitem_109 = size_42[1];  size_42 = None
    decoder_block_5_layer_1_enc_dec_attention_q = getattr(getattr(self.decoder.block, "5").layer, "1").EncDecAttention.q(mul_175);  mul_175 = None
    view_94 = decoder_block_5_layer_1_enc_dec_attention_q.view(getitem_107, -1, 12, 64);  decoder_block_5_layer_1_enc_dec_attention_q = None
    transpose_115 = view_94.transpose(1, 2);  view_94 = None
    decoder_block_5_layer_1_enc_dec_attention_k = getattr(getattr(self.decoder.block, "5").layer, "1").EncDecAttention.k(encoder_dropout_1)
    view_95 = decoder_block_5_layer_1_enc_dec_attention_k.view(getitem_107, -1, 12, 64);  decoder_block_5_layer_1_enc_dec_attention_k = None
    transpose_116 = view_95.transpose(1, 2);  view_95 = None
    decoder_block_5_layer_1_enc_dec_attention_v = getattr(getattr(self.decoder.block, "5").layer, "1").EncDecAttention.v(encoder_dropout_1)
    view_96 = decoder_block_5_layer_1_enc_dec_attention_v.view(getitem_107, -1, 12, 64);  decoder_block_5_layer_1_enc_dec_attention_v = None
    transpose_117 = view_96.transpose(1, 2);  view_96 = None
    transpose_118 = transpose_116.transpose(3, 2)
    matmul_46 = torch.matmul(transpose_115, transpose_118);  transpose_115 = transpose_118 = None
    add_148 = matmul_46 + add_97;  matmul_46 = None
    float_26 = add_148.float()
    softmax_23 = torch.nn.functional.softmax(float_26, dim = -1, _stacklevel = 3, dtype = None);  float_26 = None
    type_as_23 = softmax_23.type_as(add_148);  softmax_23 = add_148 = None
    dropout_23 = torch.nn.functional.dropout(type_as_23, p = 0.1, training = False, inplace = False);  type_as_23 = None
    matmul_47 = torch.matmul(dropout_23, transpose_117);  dropout_23 = None
    transpose_119 = matmul_47.transpose(1, 2);  matmul_47 = None
    contiguous_23 = transpose_119.contiguous();  transpose_119 = None
    view_97 = contiguous_23.view(getitem_107, -1, 768);  contiguous_23 = getitem_107 = None
    linear_40 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_40 = patch_linear_layer_linear_layer_triton_wrapper(view_97, linear_40);  view_97 = linear_40 = None
    decoder_block_5_layer_1_dropout = getattr(getattr(self.decoder.block, "5").layer, "1").dropout(linear_layer_triton_wrapper_40);  linear_layer_triton_wrapper_40 = None
    add_149 = add_146 + decoder_block_5_layer_1_dropout;  add_146 = decoder_block_5_layer_1_dropout = None
    getattr_134 = add_149.dtype
    eq_91 = getattr_134 == torch.float16;  getattr_134 = None
    to_91 = add_149.to(torch.float32)
    pow_60 = to_91.pow(2);  to_91 = None
    mean_42 = pow_60.mean(-1, keepdim = True);  pow_60 = None
    add_150 = mean_42 + 1e-06;  mean_42 = None
    rsqrt_42 = torch.rsqrt(add_150);  add_150 = None
    mul_176 = add_149 * rsqrt_42;  rsqrt_42 = None
    decoder_block_5_layer_2_layer_norm_weight = getattr(getattr(self.decoder.block, "5").layer, "2").layer_norm.weight
    getattr_135 = decoder_block_5_layer_2_layer_norm_weight.dtype
    eq_92 = getattr_135 == torch.float16;  getattr_135 = None
    getattr_136 = decoder_block_5_layer_2_layer_norm_weight.dtype
    to_92 = mul_176.to(getattr_136);  mul_176 = getattr_136 = None
    mul_177 = decoder_block_5_layer_2_layer_norm_weight * to_92;  decoder_block_5_layer_2_layer_norm_weight = to_92 = None
    decoder_block_5_layer_2_dense_relu_dense_wi_0 = getattr(getattr(self.decoder.block, "5").layer, "2").DenseReluDense.wi_0(mul_177)
    mul_178 = 0.5 * decoder_block_5_layer_2_dense_relu_dense_wi_0
    pow_61 = torch.pow(decoder_block_5_layer_2_dense_relu_dense_wi_0, 3.0)
    mul_179 = 0.044715 * pow_61;  pow_61 = None
    add_151 = decoder_block_5_layer_2_dense_relu_dense_wi_0 + mul_179;  decoder_block_5_layer_2_dense_relu_dense_wi_0 = mul_179 = None
    mul_180 = 0.7978845608028654 * add_151;  add_151 = None
    tanh_17 = torch.tanh(mul_180);  mul_180 = None
    add_152 = 1.0 + tanh_17;  tanh_17 = None
    mul_181 = mul_178 * add_152;  mul_178 = add_152 = None
    decoder_block_5_layer_2_dense_relu_dense_wi_1 = getattr(getattr(self.decoder.block, "5").layer, "2").DenseReluDense.wi_1(mul_177);  mul_177 = None
    mul_182 = mul_181 * decoder_block_5_layer_2_dense_relu_dense_wi_1;  mul_181 = decoder_block_5_layer_2_dense_relu_dense_wi_1 = None
    decoder_block_5_layer_2_dense_relu_dense_dropout = getattr(getattr(self.decoder.block, "5").layer, "2").DenseReluDense.dropout(mul_182);  mul_182 = None
    decoder_block_5_layer_2_dense_relu_dense_wo_weight = getattr(getattr(self.decoder.block, "5").layer, "2").DenseReluDense.wo.weight
    linear_41 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_41 = patch_linear_layer_linear_layer_triton_wrapper(decoder_block_5_layer_2_dense_relu_dense_dropout, linear_41);  decoder_block_5_layer_2_dense_relu_dense_dropout = linear_41 = None
    decoder_block_5_layer_2_dropout = getattr(getattr(self.decoder.block, "5").layer, "2").dropout(linear_layer_triton_wrapper_41);  linear_layer_triton_wrapper_41 = None
    add_153 = add_149 + decoder_block_5_layer_2_dropout;  add_149 = decoder_block_5_layer_2_dropout = None
    getattr_137 = add_153.dtype
    eq_93 = getattr_137 == torch.float16;  getattr_137 = None
    to_93 = add_153.to(torch.float32)
    pow_62 = to_93.pow(2);  to_93 = None
    mean_43 = pow_62.mean(-1, keepdim = True);  pow_62 = None
    add_154 = mean_43 + 1e-06;  mean_43 = None
    rsqrt_43 = torch.rsqrt(add_154);  add_154 = None
    mul_183 = add_153 * rsqrt_43;  rsqrt_43 = None
    decoder_block_6_layer_0_layer_norm_weight = getattr(getattr(self.decoder.block, "6").layer, "0").layer_norm.weight
    getattr_138 = decoder_block_6_layer_0_layer_norm_weight.dtype
    eq_94 = getattr_138 == torch.float16;  getattr_138 = None
    getattr_139 = decoder_block_6_layer_0_layer_norm_weight.dtype
    to_94 = mul_183.to(getattr_139);  mul_183 = getattr_139 = None
    mul_184 = decoder_block_6_layer_0_layer_norm_weight * to_94;  decoder_block_6_layer_0_layer_norm_weight = to_94 = None
    size_43 = mul_184.size()
    getitem_110 = size_43[slice(None, 2, None)];  size_43 = None
    getitem_111 = getitem_110[0]
    getitem_112 = getitem_110[1];  getitem_110 = None
    decoder_block_6_layer_0_self_attention_q = getattr(getattr(self.decoder.block, "6").layer, "0").SelfAttention.q(mul_184)
    view_98 = decoder_block_6_layer_0_self_attention_q.view(getitem_111, -1, 12, 64);  decoder_block_6_layer_0_self_attention_q = None
    transpose_120 = view_98.transpose(1, 2);  view_98 = None
    decoder_block_6_layer_0_self_attention_k = getattr(getattr(self.decoder.block, "6").layer, "0").SelfAttention.k(mul_184)
    view_99 = decoder_block_6_layer_0_self_attention_k.view(getitem_111, -1, 12, 64);  decoder_block_6_layer_0_self_attention_k = None
    transpose_121 = view_99.transpose(1, 2);  view_99 = None
    decoder_block_6_layer_0_self_attention_v = getattr(getattr(self.decoder.block, "6").layer, "0").SelfAttention.v(mul_184);  mul_184 = None
    view_100 = decoder_block_6_layer_0_self_attention_v.view(getitem_111, -1, 12, 64);  decoder_block_6_layer_0_self_attention_v = None
    transpose_122 = view_100.transpose(1, 2);  view_100 = None
    transpose_123 = transpose_121.transpose(3, 2)
    matmul_48 = torch.matmul(transpose_120, transpose_123);  transpose_120 = transpose_123 = None
    add_155 = matmul_48 + add_93;  matmul_48 = None
    float_27 = add_155.float()
    softmax_24 = torch.nn.functional.softmax(float_27, dim = -1, _stacklevel = 3, dtype = None);  float_27 = None
    type_as_24 = softmax_24.type_as(add_155);  softmax_24 = add_155 = None
    dropout_24 = torch.nn.functional.dropout(type_as_24, p = 0.1, training = False, inplace = False);  type_as_24 = None
    matmul_49 = torch.matmul(dropout_24, transpose_122);  dropout_24 = None
    transpose_124 = matmul_49.transpose(1, 2);  matmul_49 = None
    contiguous_24 = transpose_124.contiguous();  transpose_124 = None
    view_101 = contiguous_24.view(getitem_111, -1, 768);  contiguous_24 = getitem_111 = None
    linear_42 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_42 = patch_linear_layer_linear_layer_triton_wrapper(view_101, linear_42);  view_101 = linear_42 = None
    decoder_block_6_layer_0_dropout = getattr(getattr(self.decoder.block, "6").layer, "0").dropout(linear_layer_triton_wrapper_42);  linear_layer_triton_wrapper_42 = None
    add_156 = add_153 + decoder_block_6_layer_0_dropout;  add_153 = decoder_block_6_layer_0_dropout = None
    getattr_140 = add_156.dtype
    eq_95 = getattr_140 == torch.float16;  getattr_140 = None
    size_44 = transpose_121.size()
    getitem_113 = size_44[2];  size_44 = None
    to_95 = add_156.to(torch.float32)
    pow_63 = to_95.pow(2);  to_95 = None
    mean_44 = pow_63.mean(-1, keepdim = True);  pow_63 = None
    add_157 = mean_44 + 1e-06;  mean_44 = None
    rsqrt_44 = torch.rsqrt(add_157);  add_157 = None
    mul_185 = add_156 * rsqrt_44;  rsqrt_44 = None
    decoder_block_6_layer_1_layer_norm_weight = getattr(getattr(self.decoder.block, "6").layer, "1").layer_norm.weight
    getattr_141 = decoder_block_6_layer_1_layer_norm_weight.dtype
    eq_96 = getattr_141 == torch.float16;  getattr_141 = None
    getattr_142 = decoder_block_6_layer_1_layer_norm_weight.dtype
    to_96 = mul_185.to(getattr_142);  mul_185 = getattr_142 = None
    mul_186 = decoder_block_6_layer_1_layer_norm_weight * to_96;  decoder_block_6_layer_1_layer_norm_weight = to_96 = None
    size_45 = mul_186.size()
    getitem_114 = size_45[slice(None, 2, None)];  size_45 = None
    getitem_115 = getitem_114[0]
    getitem_116 = getitem_114[1];  getitem_114 = None
    size_46 = encoder_dropout_1.size()
    getitem_117 = size_46[1];  size_46 = None
    decoder_block_6_layer_1_enc_dec_attention_q = getattr(getattr(self.decoder.block, "6").layer, "1").EncDecAttention.q(mul_186);  mul_186 = None
    view_102 = decoder_block_6_layer_1_enc_dec_attention_q.view(getitem_115, -1, 12, 64);  decoder_block_6_layer_1_enc_dec_attention_q = None
    transpose_125 = view_102.transpose(1, 2);  view_102 = None
    decoder_block_6_layer_1_enc_dec_attention_k = getattr(getattr(self.decoder.block, "6").layer, "1").EncDecAttention.k(encoder_dropout_1)
    view_103 = decoder_block_6_layer_1_enc_dec_attention_k.view(getitem_115, -1, 12, 64);  decoder_block_6_layer_1_enc_dec_attention_k = None
    transpose_126 = view_103.transpose(1, 2);  view_103 = None
    decoder_block_6_layer_1_enc_dec_attention_v = getattr(getattr(self.decoder.block, "6").layer, "1").EncDecAttention.v(encoder_dropout_1)
    view_104 = decoder_block_6_layer_1_enc_dec_attention_v.view(getitem_115, -1, 12, 64);  decoder_block_6_layer_1_enc_dec_attention_v = None
    transpose_127 = view_104.transpose(1, 2);  view_104 = None
    transpose_128 = transpose_126.transpose(3, 2)
    matmul_50 = torch.matmul(transpose_125, transpose_128);  transpose_125 = transpose_128 = None
    add_158 = matmul_50 + add_97;  matmul_50 = None
    float_28 = add_158.float()
    softmax_25 = torch.nn.functional.softmax(float_28, dim = -1, _stacklevel = 3, dtype = None);  float_28 = None
    type_as_25 = softmax_25.type_as(add_158);  softmax_25 = add_158 = None
    dropout_25 = torch.nn.functional.dropout(type_as_25, p = 0.1, training = False, inplace = False);  type_as_25 = None
    matmul_51 = torch.matmul(dropout_25, transpose_127);  dropout_25 = None
    transpose_129 = matmul_51.transpose(1, 2);  matmul_51 = None
    contiguous_25 = transpose_129.contiguous();  transpose_129 = None
    view_105 = contiguous_25.view(getitem_115, -1, 768);  contiguous_25 = getitem_115 = None
    linear_43 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_43 = patch_linear_layer_linear_layer_triton_wrapper(view_105, linear_43);  view_105 = linear_43 = None
    decoder_block_6_layer_1_dropout = getattr(getattr(self.decoder.block, "6").layer, "1").dropout(linear_layer_triton_wrapper_43);  linear_layer_triton_wrapper_43 = None
    add_159 = add_156 + decoder_block_6_layer_1_dropout;  add_156 = decoder_block_6_layer_1_dropout = None
    getattr_143 = add_159.dtype
    eq_97 = getattr_143 == torch.float16;  getattr_143 = None
    to_97 = add_159.to(torch.float32)
    pow_64 = to_97.pow(2);  to_97 = None
    mean_45 = pow_64.mean(-1, keepdim = True);  pow_64 = None
    add_160 = mean_45 + 1e-06;  mean_45 = None
    rsqrt_45 = torch.rsqrt(add_160);  add_160 = None
    mul_187 = add_159 * rsqrt_45;  rsqrt_45 = None
    decoder_block_6_layer_2_layer_norm_weight = getattr(getattr(self.decoder.block, "6").layer, "2").layer_norm.weight
    getattr_144 = decoder_block_6_layer_2_layer_norm_weight.dtype
    eq_98 = getattr_144 == torch.float16;  getattr_144 = None
    getattr_145 = decoder_block_6_layer_2_layer_norm_weight.dtype
    to_98 = mul_187.to(getattr_145);  mul_187 = getattr_145 = None
    mul_188 = decoder_block_6_layer_2_layer_norm_weight * to_98;  decoder_block_6_layer_2_layer_norm_weight = to_98 = None
    decoder_block_6_layer_2_dense_relu_dense_wi_0 = getattr(getattr(self.decoder.block, "6").layer, "2").DenseReluDense.wi_0(mul_188)
    mul_189 = 0.5 * decoder_block_6_layer_2_dense_relu_dense_wi_0
    pow_65 = torch.pow(decoder_block_6_layer_2_dense_relu_dense_wi_0, 3.0)
    mul_190 = 0.044715 * pow_65;  pow_65 = None
    add_161 = decoder_block_6_layer_2_dense_relu_dense_wi_0 + mul_190;  decoder_block_6_layer_2_dense_relu_dense_wi_0 = mul_190 = None
    mul_191 = 0.7978845608028654 * add_161;  add_161 = None
    tanh_18 = torch.tanh(mul_191);  mul_191 = None
    add_162 = 1.0 + tanh_18;  tanh_18 = None
    mul_192 = mul_189 * add_162;  mul_189 = add_162 = None
    decoder_block_6_layer_2_dense_relu_dense_wi_1 = getattr(getattr(self.decoder.block, "6").layer, "2").DenseReluDense.wi_1(mul_188);  mul_188 = None
    mul_193 = mul_192 * decoder_block_6_layer_2_dense_relu_dense_wi_1;  mul_192 = decoder_block_6_layer_2_dense_relu_dense_wi_1 = None
    decoder_block_6_layer_2_dense_relu_dense_dropout = getattr(getattr(self.decoder.block, "6").layer, "2").DenseReluDense.dropout(mul_193);  mul_193 = None
    decoder_block_6_layer_2_dense_relu_dense_wo_weight = getattr(getattr(self.decoder.block, "6").layer, "2").DenseReluDense.wo.weight
    linear_44 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_44 = patch_linear_layer_linear_layer_triton_wrapper(decoder_block_6_layer_2_dense_relu_dense_dropout, linear_44);  decoder_block_6_layer_2_dense_relu_dense_dropout = linear_44 = None
    decoder_block_6_layer_2_dropout = getattr(getattr(self.decoder.block, "6").layer, "2").dropout(linear_layer_triton_wrapper_44);  linear_layer_triton_wrapper_44 = None
    add_163 = add_159 + decoder_block_6_layer_2_dropout;  add_159 = decoder_block_6_layer_2_dropout = None
    getattr_146 = add_163.dtype
    eq_99 = getattr_146 == torch.float16;  getattr_146 = None
    to_99 = add_163.to(torch.float32)
    pow_66 = to_99.pow(2);  to_99 = None
    mean_46 = pow_66.mean(-1, keepdim = True);  pow_66 = None
    add_164 = mean_46 + 1e-06;  mean_46 = None
    rsqrt_46 = torch.rsqrt(add_164);  add_164 = None
    mul_194 = add_163 * rsqrt_46;  rsqrt_46 = None
    decoder_block_7_layer_0_layer_norm_weight = getattr(getattr(self.decoder.block, "7").layer, "0").layer_norm.weight
    getattr_147 = decoder_block_7_layer_0_layer_norm_weight.dtype
    eq_100 = getattr_147 == torch.float16;  getattr_147 = None
    getattr_148 = decoder_block_7_layer_0_layer_norm_weight.dtype
    to_100 = mul_194.to(getattr_148);  mul_194 = getattr_148 = None
    mul_195 = decoder_block_7_layer_0_layer_norm_weight * to_100;  decoder_block_7_layer_0_layer_norm_weight = to_100 = None
    size_47 = mul_195.size()
    getitem_118 = size_47[slice(None, 2, None)];  size_47 = None
    getitem_119 = getitem_118[0]
    getitem_120 = getitem_118[1];  getitem_118 = None
    decoder_block_7_layer_0_self_attention_q = getattr(getattr(self.decoder.block, "7").layer, "0").SelfAttention.q(mul_195)
    view_106 = decoder_block_7_layer_0_self_attention_q.view(getitem_119, -1, 12, 64);  decoder_block_7_layer_0_self_attention_q = None
    transpose_130 = view_106.transpose(1, 2);  view_106 = None
    decoder_block_7_layer_0_self_attention_k = getattr(getattr(self.decoder.block, "7").layer, "0").SelfAttention.k(mul_195)
    view_107 = decoder_block_7_layer_0_self_attention_k.view(getitem_119, -1, 12, 64);  decoder_block_7_layer_0_self_attention_k = None
    transpose_131 = view_107.transpose(1, 2);  view_107 = None
    decoder_block_7_layer_0_self_attention_v = getattr(getattr(self.decoder.block, "7").layer, "0").SelfAttention.v(mul_195);  mul_195 = None
    view_108 = decoder_block_7_layer_0_self_attention_v.view(getitem_119, -1, 12, 64);  decoder_block_7_layer_0_self_attention_v = None
    transpose_132 = view_108.transpose(1, 2);  view_108 = None
    transpose_133 = transpose_131.transpose(3, 2)
    matmul_52 = torch.matmul(transpose_130, transpose_133);  transpose_130 = transpose_133 = None
    add_165 = matmul_52 + add_93;  matmul_52 = None
    float_29 = add_165.float()
    softmax_26 = torch.nn.functional.softmax(float_29, dim = -1, _stacklevel = 3, dtype = None);  float_29 = None
    type_as_26 = softmax_26.type_as(add_165);  softmax_26 = add_165 = None
    dropout_26 = torch.nn.functional.dropout(type_as_26, p = 0.1, training = False, inplace = False);  type_as_26 = None
    matmul_53 = torch.matmul(dropout_26, transpose_132);  dropout_26 = None
    transpose_134 = matmul_53.transpose(1, 2);  matmul_53 = None
    contiguous_26 = transpose_134.contiguous();  transpose_134 = None
    view_109 = contiguous_26.view(getitem_119, -1, 768);  contiguous_26 = getitem_119 = None
    linear_45 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_45 = patch_linear_layer_linear_layer_triton_wrapper(view_109, linear_45);  view_109 = linear_45 = None
    decoder_block_7_layer_0_dropout = getattr(getattr(self.decoder.block, "7").layer, "0").dropout(linear_layer_triton_wrapper_45);  linear_layer_triton_wrapper_45 = None
    add_166 = add_163 + decoder_block_7_layer_0_dropout;  add_163 = decoder_block_7_layer_0_dropout = None
    getattr_149 = add_166.dtype
    eq_101 = getattr_149 == torch.float16;  getattr_149 = None
    size_48 = transpose_131.size()
    getitem_121 = size_48[2];  size_48 = None
    to_101 = add_166.to(torch.float32)
    pow_67 = to_101.pow(2);  to_101 = None
    mean_47 = pow_67.mean(-1, keepdim = True);  pow_67 = None
    add_167 = mean_47 + 1e-06;  mean_47 = None
    rsqrt_47 = torch.rsqrt(add_167);  add_167 = None
    mul_196 = add_166 * rsqrt_47;  rsqrt_47 = None
    decoder_block_7_layer_1_layer_norm_weight = getattr(getattr(self.decoder.block, "7").layer, "1").layer_norm.weight
    getattr_150 = decoder_block_7_layer_1_layer_norm_weight.dtype
    eq_102 = getattr_150 == torch.float16;  getattr_150 = None
    getattr_151 = decoder_block_7_layer_1_layer_norm_weight.dtype
    to_102 = mul_196.to(getattr_151);  mul_196 = getattr_151 = None
    mul_197 = decoder_block_7_layer_1_layer_norm_weight * to_102;  decoder_block_7_layer_1_layer_norm_weight = to_102 = None
    size_49 = mul_197.size()
    getitem_122 = size_49[slice(None, 2, None)];  size_49 = None
    getitem_123 = getitem_122[0]
    getitem_124 = getitem_122[1];  getitem_122 = None
    size_50 = encoder_dropout_1.size()
    getitem_125 = size_50[1];  size_50 = None
    decoder_block_7_layer_1_enc_dec_attention_q = getattr(getattr(self.decoder.block, "7").layer, "1").EncDecAttention.q(mul_197);  mul_197 = None
    view_110 = decoder_block_7_layer_1_enc_dec_attention_q.view(getitem_123, -1, 12, 64);  decoder_block_7_layer_1_enc_dec_attention_q = None
    transpose_135 = view_110.transpose(1, 2);  view_110 = None
    decoder_block_7_layer_1_enc_dec_attention_k = getattr(getattr(self.decoder.block, "7").layer, "1").EncDecAttention.k(encoder_dropout_1)
    view_111 = decoder_block_7_layer_1_enc_dec_attention_k.view(getitem_123, -1, 12, 64);  decoder_block_7_layer_1_enc_dec_attention_k = None
    transpose_136 = view_111.transpose(1, 2);  view_111 = None
    decoder_block_7_layer_1_enc_dec_attention_v = getattr(getattr(self.decoder.block, "7").layer, "1").EncDecAttention.v(encoder_dropout_1)
    view_112 = decoder_block_7_layer_1_enc_dec_attention_v.view(getitem_123, -1, 12, 64);  decoder_block_7_layer_1_enc_dec_attention_v = None
    transpose_137 = view_112.transpose(1, 2);  view_112 = None
    transpose_138 = transpose_136.transpose(3, 2)
    matmul_54 = torch.matmul(transpose_135, transpose_138);  transpose_135 = transpose_138 = None
    add_168 = matmul_54 + add_97;  matmul_54 = None
    float_30 = add_168.float()
    softmax_27 = torch.nn.functional.softmax(float_30, dim = -1, _stacklevel = 3, dtype = None);  float_30 = None
    type_as_27 = softmax_27.type_as(add_168);  softmax_27 = add_168 = None
    dropout_27 = torch.nn.functional.dropout(type_as_27, p = 0.1, training = False, inplace = False);  type_as_27 = None
    matmul_55 = torch.matmul(dropout_27, transpose_137);  dropout_27 = None
    transpose_139 = matmul_55.transpose(1, 2);  matmul_55 = None
    contiguous_27 = transpose_139.contiguous();  transpose_139 = None
    view_113 = contiguous_27.view(getitem_123, -1, 768);  contiguous_27 = getitem_123 = None
    linear_46 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_46 = patch_linear_layer_linear_layer_triton_wrapper(view_113, linear_46);  view_113 = linear_46 = None
    decoder_block_7_layer_1_dropout = getattr(getattr(self.decoder.block, "7").layer, "1").dropout(linear_layer_triton_wrapper_46);  linear_layer_triton_wrapper_46 = None
    add_169 = add_166 + decoder_block_7_layer_1_dropout;  add_166 = decoder_block_7_layer_1_dropout = None
    getattr_152 = add_169.dtype
    eq_103 = getattr_152 == torch.float16;  getattr_152 = None
    to_103 = add_169.to(torch.float32)
    pow_68 = to_103.pow(2);  to_103 = None
    mean_48 = pow_68.mean(-1, keepdim = True);  pow_68 = None
    add_170 = mean_48 + 1e-06;  mean_48 = None
    rsqrt_48 = torch.rsqrt(add_170);  add_170 = None
    mul_198 = add_169 * rsqrt_48;  rsqrt_48 = None
    decoder_block_7_layer_2_layer_norm_weight = getattr(getattr(self.decoder.block, "7").layer, "2").layer_norm.weight
    getattr_153 = decoder_block_7_layer_2_layer_norm_weight.dtype
    eq_104 = getattr_153 == torch.float16;  getattr_153 = None
    getattr_154 = decoder_block_7_layer_2_layer_norm_weight.dtype
    to_104 = mul_198.to(getattr_154);  mul_198 = getattr_154 = None
    mul_199 = decoder_block_7_layer_2_layer_norm_weight * to_104;  decoder_block_7_layer_2_layer_norm_weight = to_104 = None
    decoder_block_7_layer_2_dense_relu_dense_wi_0 = getattr(getattr(self.decoder.block, "7").layer, "2").DenseReluDense.wi_0(mul_199)
    mul_200 = 0.5 * decoder_block_7_layer_2_dense_relu_dense_wi_0
    pow_69 = torch.pow(decoder_block_7_layer_2_dense_relu_dense_wi_0, 3.0)
    mul_201 = 0.044715 * pow_69;  pow_69 = None
    add_171 = decoder_block_7_layer_2_dense_relu_dense_wi_0 + mul_201;  decoder_block_7_layer_2_dense_relu_dense_wi_0 = mul_201 = None
    mul_202 = 0.7978845608028654 * add_171;  add_171 = None
    tanh_19 = torch.tanh(mul_202);  mul_202 = None
    add_172 = 1.0 + tanh_19;  tanh_19 = None
    mul_203 = mul_200 * add_172;  mul_200 = add_172 = None
    decoder_block_7_layer_2_dense_relu_dense_wi_1 = getattr(getattr(self.decoder.block, "7").layer, "2").DenseReluDense.wi_1(mul_199);  mul_199 = None
    mul_204 = mul_203 * decoder_block_7_layer_2_dense_relu_dense_wi_1;  mul_203 = decoder_block_7_layer_2_dense_relu_dense_wi_1 = None
    decoder_block_7_layer_2_dense_relu_dense_dropout = getattr(getattr(self.decoder.block, "7").layer, "2").DenseReluDense.dropout(mul_204);  mul_204 = None
    decoder_block_7_layer_2_dense_relu_dense_wo_weight = getattr(getattr(self.decoder.block, "7").layer, "2").DenseReluDense.wo.weight
    linear_47 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_47 = patch_linear_layer_linear_layer_triton_wrapper(decoder_block_7_layer_2_dense_relu_dense_dropout, linear_47);  decoder_block_7_layer_2_dense_relu_dense_dropout = linear_47 = None
    decoder_block_7_layer_2_dropout = getattr(getattr(self.decoder.block, "7").layer, "2").dropout(linear_layer_triton_wrapper_47);  linear_layer_triton_wrapper_47 = None
    add_173 = add_169 + decoder_block_7_layer_2_dropout;  add_169 = decoder_block_7_layer_2_dropout = None
    getattr_155 = add_173.dtype
    eq_105 = getattr_155 == torch.float16;  getattr_155 = None
    to_105 = add_173.to(torch.float32)
    pow_70 = to_105.pow(2);  to_105 = None
    mean_49 = pow_70.mean(-1, keepdim = True);  pow_70 = None
    add_174 = mean_49 + 1e-06;  mean_49 = None
    rsqrt_49 = torch.rsqrt(add_174);  add_174 = None
    mul_205 = add_173 * rsqrt_49;  rsqrt_49 = None
    decoder_block_8_layer_0_layer_norm_weight = getattr(getattr(self.decoder.block, "8").layer, "0").layer_norm.weight
    getattr_156 = decoder_block_8_layer_0_layer_norm_weight.dtype
    eq_106 = getattr_156 == torch.float16;  getattr_156 = None
    getattr_157 = decoder_block_8_layer_0_layer_norm_weight.dtype
    to_106 = mul_205.to(getattr_157);  mul_205 = getattr_157 = None
    mul_206 = decoder_block_8_layer_0_layer_norm_weight * to_106;  decoder_block_8_layer_0_layer_norm_weight = to_106 = None
    size_51 = mul_206.size()
    getitem_126 = size_51[slice(None, 2, None)];  size_51 = None
    getitem_127 = getitem_126[0]
    getitem_128 = getitem_126[1];  getitem_126 = None
    decoder_block_8_layer_0_self_attention_q = getattr(getattr(self.decoder.block, "8").layer, "0").SelfAttention.q(mul_206)
    view_114 = decoder_block_8_layer_0_self_attention_q.view(getitem_127, -1, 12, 64);  decoder_block_8_layer_0_self_attention_q = None
    transpose_140 = view_114.transpose(1, 2);  view_114 = None
    decoder_block_8_layer_0_self_attention_k = getattr(getattr(self.decoder.block, "8").layer, "0").SelfAttention.k(mul_206)
    view_115 = decoder_block_8_layer_0_self_attention_k.view(getitem_127, -1, 12, 64);  decoder_block_8_layer_0_self_attention_k = None
    transpose_141 = view_115.transpose(1, 2);  view_115 = None
    decoder_block_8_layer_0_self_attention_v = getattr(getattr(self.decoder.block, "8").layer, "0").SelfAttention.v(mul_206);  mul_206 = None
    view_116 = decoder_block_8_layer_0_self_attention_v.view(getitem_127, -1, 12, 64);  decoder_block_8_layer_0_self_attention_v = None
    transpose_142 = view_116.transpose(1, 2);  view_116 = None
    transpose_143 = transpose_141.transpose(3, 2)
    matmul_56 = torch.matmul(transpose_140, transpose_143);  transpose_140 = transpose_143 = None
    add_175 = matmul_56 + add_93;  matmul_56 = None
    float_31 = add_175.float()
    softmax_28 = torch.nn.functional.softmax(float_31, dim = -1, _stacklevel = 3, dtype = None);  float_31 = None
    type_as_28 = softmax_28.type_as(add_175);  softmax_28 = add_175 = None
    dropout_28 = torch.nn.functional.dropout(type_as_28, p = 0.1, training = False, inplace = False);  type_as_28 = None
    matmul_57 = torch.matmul(dropout_28, transpose_142);  dropout_28 = None
    transpose_144 = matmul_57.transpose(1, 2);  matmul_57 = None
    contiguous_28 = transpose_144.contiguous();  transpose_144 = None
    view_117 = contiguous_28.view(getitem_127, -1, 768);  contiguous_28 = getitem_127 = None
    linear_48 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_48 = patch_linear_layer_linear_layer_triton_wrapper(view_117, linear_48);  view_117 = linear_48 = None
    decoder_block_8_layer_0_dropout = getattr(getattr(self.decoder.block, "8").layer, "0").dropout(linear_layer_triton_wrapper_48);  linear_layer_triton_wrapper_48 = None
    add_176 = add_173 + decoder_block_8_layer_0_dropout;  add_173 = decoder_block_8_layer_0_dropout = None
    getattr_158 = add_176.dtype
    eq_107 = getattr_158 == torch.float16;  getattr_158 = None
    size_52 = transpose_141.size()
    getitem_129 = size_52[2];  size_52 = None
    to_107 = add_176.to(torch.float32)
    pow_71 = to_107.pow(2);  to_107 = None
    mean_50 = pow_71.mean(-1, keepdim = True);  pow_71 = None
    add_177 = mean_50 + 1e-06;  mean_50 = None
    rsqrt_50 = torch.rsqrt(add_177);  add_177 = None
    mul_207 = add_176 * rsqrt_50;  rsqrt_50 = None
    decoder_block_8_layer_1_layer_norm_weight = getattr(getattr(self.decoder.block, "8").layer, "1").layer_norm.weight
    getattr_159 = decoder_block_8_layer_1_layer_norm_weight.dtype
    eq_108 = getattr_159 == torch.float16;  getattr_159 = None
    getattr_160 = decoder_block_8_layer_1_layer_norm_weight.dtype
    to_108 = mul_207.to(getattr_160);  mul_207 = getattr_160 = None
    mul_208 = decoder_block_8_layer_1_layer_norm_weight * to_108;  decoder_block_8_layer_1_layer_norm_weight = to_108 = None
    size_53 = mul_208.size()
    getitem_130 = size_53[slice(None, 2, None)];  size_53 = None
    getitem_131 = getitem_130[0]
    getitem_132 = getitem_130[1];  getitem_130 = None
    size_54 = encoder_dropout_1.size()
    getitem_133 = size_54[1];  size_54 = None
    decoder_block_8_layer_1_enc_dec_attention_q = getattr(getattr(self.decoder.block, "8").layer, "1").EncDecAttention.q(mul_208);  mul_208 = None
    view_118 = decoder_block_8_layer_1_enc_dec_attention_q.view(getitem_131, -1, 12, 64);  decoder_block_8_layer_1_enc_dec_attention_q = None
    transpose_145 = view_118.transpose(1, 2);  view_118 = None
    decoder_block_8_layer_1_enc_dec_attention_k = getattr(getattr(self.decoder.block, "8").layer, "1").EncDecAttention.k(encoder_dropout_1)
    view_119 = decoder_block_8_layer_1_enc_dec_attention_k.view(getitem_131, -1, 12, 64);  decoder_block_8_layer_1_enc_dec_attention_k = None
    transpose_146 = view_119.transpose(1, 2);  view_119 = None
    decoder_block_8_layer_1_enc_dec_attention_v = getattr(getattr(self.decoder.block, "8").layer, "1").EncDecAttention.v(encoder_dropout_1)
    view_120 = decoder_block_8_layer_1_enc_dec_attention_v.view(getitem_131, -1, 12, 64);  decoder_block_8_layer_1_enc_dec_attention_v = None
    transpose_147 = view_120.transpose(1, 2);  view_120 = None
    transpose_148 = transpose_146.transpose(3, 2)
    matmul_58 = torch.matmul(transpose_145, transpose_148);  transpose_145 = transpose_148 = None
    add_178 = matmul_58 + add_97;  matmul_58 = None
    float_32 = add_178.float()
    softmax_29 = torch.nn.functional.softmax(float_32, dim = -1, _stacklevel = 3, dtype = None);  float_32 = None
    type_as_29 = softmax_29.type_as(add_178);  softmax_29 = add_178 = None
    dropout_29 = torch.nn.functional.dropout(type_as_29, p = 0.1, training = False, inplace = False);  type_as_29 = None
    matmul_59 = torch.matmul(dropout_29, transpose_147);  dropout_29 = None
    transpose_149 = matmul_59.transpose(1, 2);  matmul_59 = None
    contiguous_29 = transpose_149.contiguous();  transpose_149 = None
    view_121 = contiguous_29.view(getitem_131, -1, 768);  contiguous_29 = getitem_131 = None
    linear_49 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_49 = patch_linear_layer_linear_layer_triton_wrapper(view_121, linear_49);  view_121 = linear_49 = None
    decoder_block_8_layer_1_dropout = getattr(getattr(self.decoder.block, "8").layer, "1").dropout(linear_layer_triton_wrapper_49);  linear_layer_triton_wrapper_49 = None
    add_179 = add_176 + decoder_block_8_layer_1_dropout;  add_176 = decoder_block_8_layer_1_dropout = None
    getattr_161 = add_179.dtype
    eq_109 = getattr_161 == torch.float16;  getattr_161 = None
    to_109 = add_179.to(torch.float32)
    pow_72 = to_109.pow(2);  to_109 = None
    mean_51 = pow_72.mean(-1, keepdim = True);  pow_72 = None
    add_180 = mean_51 + 1e-06;  mean_51 = None
    rsqrt_51 = torch.rsqrt(add_180);  add_180 = None
    mul_209 = add_179 * rsqrt_51;  rsqrt_51 = None
    decoder_block_8_layer_2_layer_norm_weight = getattr(getattr(self.decoder.block, "8").layer, "2").layer_norm.weight
    getattr_162 = decoder_block_8_layer_2_layer_norm_weight.dtype
    eq_110 = getattr_162 == torch.float16;  getattr_162 = None
    getattr_163 = decoder_block_8_layer_2_layer_norm_weight.dtype
    to_110 = mul_209.to(getattr_163);  mul_209 = getattr_163 = None
    mul_210 = decoder_block_8_layer_2_layer_norm_weight * to_110;  decoder_block_8_layer_2_layer_norm_weight = to_110 = None
    decoder_block_8_layer_2_dense_relu_dense_wi_0 = getattr(getattr(self.decoder.block, "8").layer, "2").DenseReluDense.wi_0(mul_210)
    mul_211 = 0.5 * decoder_block_8_layer_2_dense_relu_dense_wi_0
    pow_73 = torch.pow(decoder_block_8_layer_2_dense_relu_dense_wi_0, 3.0)
    mul_212 = 0.044715 * pow_73;  pow_73 = None
    add_181 = decoder_block_8_layer_2_dense_relu_dense_wi_0 + mul_212;  decoder_block_8_layer_2_dense_relu_dense_wi_0 = mul_212 = None
    mul_213 = 0.7978845608028654 * add_181;  add_181 = None
    tanh_20 = torch.tanh(mul_213);  mul_213 = None
    add_182 = 1.0 + tanh_20;  tanh_20 = None
    mul_214 = mul_211 * add_182;  mul_211 = add_182 = None
    decoder_block_8_layer_2_dense_relu_dense_wi_1 = getattr(getattr(self.decoder.block, "8").layer, "2").DenseReluDense.wi_1(mul_210);  mul_210 = None
    mul_215 = mul_214 * decoder_block_8_layer_2_dense_relu_dense_wi_1;  mul_214 = decoder_block_8_layer_2_dense_relu_dense_wi_1 = None
    decoder_block_8_layer_2_dense_relu_dense_dropout = getattr(getattr(self.decoder.block, "8").layer, "2").DenseReluDense.dropout(mul_215);  mul_215 = None
    decoder_block_8_layer_2_dense_relu_dense_wo_weight = getattr(getattr(self.decoder.block, "8").layer, "2").DenseReluDense.wo.weight
    linear_50 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_50 = patch_linear_layer_linear_layer_triton_wrapper(decoder_block_8_layer_2_dense_relu_dense_dropout, linear_50);  decoder_block_8_layer_2_dense_relu_dense_dropout = linear_50 = None
    decoder_block_8_layer_2_dropout = getattr(getattr(self.decoder.block, "8").layer, "2").dropout(linear_layer_triton_wrapper_50);  linear_layer_triton_wrapper_50 = None
    add_183 = add_179 + decoder_block_8_layer_2_dropout;  add_179 = decoder_block_8_layer_2_dropout = None
    getattr_164 = add_183.dtype
    eq_111 = getattr_164 == torch.float16;  getattr_164 = None
    to_111 = add_183.to(torch.float32)
    pow_74 = to_111.pow(2);  to_111 = None
    mean_52 = pow_74.mean(-1, keepdim = True);  pow_74 = None
    add_184 = mean_52 + 1e-06;  mean_52 = None
    rsqrt_52 = torch.rsqrt(add_184);  add_184 = None
    mul_216 = add_183 * rsqrt_52;  rsqrt_52 = None
    decoder_block_9_layer_0_layer_norm_weight = getattr(getattr(self.decoder.block, "9").layer, "0").layer_norm.weight
    getattr_165 = decoder_block_9_layer_0_layer_norm_weight.dtype
    eq_112 = getattr_165 == torch.float16;  getattr_165 = None
    getattr_166 = decoder_block_9_layer_0_layer_norm_weight.dtype
    to_112 = mul_216.to(getattr_166);  mul_216 = getattr_166 = None
    mul_217 = decoder_block_9_layer_0_layer_norm_weight * to_112;  decoder_block_9_layer_0_layer_norm_weight = to_112 = None
    size_55 = mul_217.size()
    getitem_134 = size_55[slice(None, 2, None)];  size_55 = None
    getitem_135 = getitem_134[0]
    getitem_136 = getitem_134[1];  getitem_134 = None
    decoder_block_9_layer_0_self_attention_q = getattr(getattr(self.decoder.block, "9").layer, "0").SelfAttention.q(mul_217)
    view_122 = decoder_block_9_layer_0_self_attention_q.view(getitem_135, -1, 12, 64);  decoder_block_9_layer_0_self_attention_q = None
    transpose_150 = view_122.transpose(1, 2);  view_122 = None
    decoder_block_9_layer_0_self_attention_k = getattr(getattr(self.decoder.block, "9").layer, "0").SelfAttention.k(mul_217)
    view_123 = decoder_block_9_layer_0_self_attention_k.view(getitem_135, -1, 12, 64);  decoder_block_9_layer_0_self_attention_k = None
    transpose_151 = view_123.transpose(1, 2);  view_123 = None
    decoder_block_9_layer_0_self_attention_v = getattr(getattr(self.decoder.block, "9").layer, "0").SelfAttention.v(mul_217);  mul_217 = None
    view_124 = decoder_block_9_layer_0_self_attention_v.view(getitem_135, -1, 12, 64);  decoder_block_9_layer_0_self_attention_v = None
    transpose_152 = view_124.transpose(1, 2);  view_124 = None
    transpose_153 = transpose_151.transpose(3, 2)
    matmul_60 = torch.matmul(transpose_150, transpose_153);  transpose_150 = transpose_153 = None
    add_185 = matmul_60 + add_93;  matmul_60 = None
    float_33 = add_185.float()
    softmax_30 = torch.nn.functional.softmax(float_33, dim = -1, _stacklevel = 3, dtype = None);  float_33 = None
    type_as_30 = softmax_30.type_as(add_185);  softmax_30 = add_185 = None
    dropout_30 = torch.nn.functional.dropout(type_as_30, p = 0.1, training = False, inplace = False);  type_as_30 = None
    matmul_61 = torch.matmul(dropout_30, transpose_152);  dropout_30 = None
    transpose_154 = matmul_61.transpose(1, 2);  matmul_61 = None
    contiguous_30 = transpose_154.contiguous();  transpose_154 = None
    view_125 = contiguous_30.view(getitem_135, -1, 768);  contiguous_30 = getitem_135 = None
    linear_51 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_51 = patch_linear_layer_linear_layer_triton_wrapper(view_125, linear_51);  view_125 = linear_51 = None
    decoder_block_9_layer_0_dropout = getattr(getattr(self.decoder.block, "9").layer, "0").dropout(linear_layer_triton_wrapper_51);  linear_layer_triton_wrapper_51 = None
    add_186 = add_183 + decoder_block_9_layer_0_dropout;  add_183 = decoder_block_9_layer_0_dropout = None
    getattr_167 = add_186.dtype
    eq_113 = getattr_167 == torch.float16;  getattr_167 = None
    size_56 = transpose_151.size()
    getitem_137 = size_56[2];  size_56 = None
    to_113 = add_186.to(torch.float32)
    pow_75 = to_113.pow(2);  to_113 = None
    mean_53 = pow_75.mean(-1, keepdim = True);  pow_75 = None
    add_187 = mean_53 + 1e-06;  mean_53 = None
    rsqrt_53 = torch.rsqrt(add_187);  add_187 = None
    mul_218 = add_186 * rsqrt_53;  rsqrt_53 = None
    decoder_block_9_layer_1_layer_norm_weight = getattr(getattr(self.decoder.block, "9").layer, "1").layer_norm.weight
    getattr_168 = decoder_block_9_layer_1_layer_norm_weight.dtype
    eq_114 = getattr_168 == torch.float16;  getattr_168 = None
    getattr_169 = decoder_block_9_layer_1_layer_norm_weight.dtype
    to_114 = mul_218.to(getattr_169);  mul_218 = getattr_169 = None
    mul_219 = decoder_block_9_layer_1_layer_norm_weight * to_114;  decoder_block_9_layer_1_layer_norm_weight = to_114 = None
    size_57 = mul_219.size()
    getitem_138 = size_57[slice(None, 2, None)];  size_57 = None
    getitem_139 = getitem_138[0]
    getitem_140 = getitem_138[1];  getitem_138 = None
    size_58 = encoder_dropout_1.size()
    getitem_141 = size_58[1];  size_58 = None
    decoder_block_9_layer_1_enc_dec_attention_q = getattr(getattr(self.decoder.block, "9").layer, "1").EncDecAttention.q(mul_219);  mul_219 = None
    view_126 = decoder_block_9_layer_1_enc_dec_attention_q.view(getitem_139, -1, 12, 64);  decoder_block_9_layer_1_enc_dec_attention_q = None
    transpose_155 = view_126.transpose(1, 2);  view_126 = None
    decoder_block_9_layer_1_enc_dec_attention_k = getattr(getattr(self.decoder.block, "9").layer, "1").EncDecAttention.k(encoder_dropout_1)
    view_127 = decoder_block_9_layer_1_enc_dec_attention_k.view(getitem_139, -1, 12, 64);  decoder_block_9_layer_1_enc_dec_attention_k = None
    transpose_156 = view_127.transpose(1, 2);  view_127 = None
    decoder_block_9_layer_1_enc_dec_attention_v = getattr(getattr(self.decoder.block, "9").layer, "1").EncDecAttention.v(encoder_dropout_1)
    view_128 = decoder_block_9_layer_1_enc_dec_attention_v.view(getitem_139, -1, 12, 64);  decoder_block_9_layer_1_enc_dec_attention_v = None
    transpose_157 = view_128.transpose(1, 2);  view_128 = None
    transpose_158 = transpose_156.transpose(3, 2)
    matmul_62 = torch.matmul(transpose_155, transpose_158);  transpose_155 = transpose_158 = None
    add_188 = matmul_62 + add_97;  matmul_62 = None
    float_34 = add_188.float()
    softmax_31 = torch.nn.functional.softmax(float_34, dim = -1, _stacklevel = 3, dtype = None);  float_34 = None
    type_as_31 = softmax_31.type_as(add_188);  softmax_31 = add_188 = None
    dropout_31 = torch.nn.functional.dropout(type_as_31, p = 0.1, training = False, inplace = False);  type_as_31 = None
    matmul_63 = torch.matmul(dropout_31, transpose_157);  dropout_31 = None
    transpose_159 = matmul_63.transpose(1, 2);  matmul_63 = None
    contiguous_31 = transpose_159.contiguous();  transpose_159 = None
    view_129 = contiguous_31.view(getitem_139, -1, 768);  contiguous_31 = getitem_139 = None
    linear_52 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_52 = patch_linear_layer_linear_layer_triton_wrapper(view_129, linear_52);  view_129 = linear_52 = None
    decoder_block_9_layer_1_dropout = getattr(getattr(self.decoder.block, "9").layer, "1").dropout(linear_layer_triton_wrapper_52);  linear_layer_triton_wrapper_52 = None
    add_189 = add_186 + decoder_block_9_layer_1_dropout;  add_186 = decoder_block_9_layer_1_dropout = None
    getattr_170 = add_189.dtype
    eq_115 = getattr_170 == torch.float16;  getattr_170 = None
    to_115 = add_189.to(torch.float32)
    pow_76 = to_115.pow(2);  to_115 = None
    mean_54 = pow_76.mean(-1, keepdim = True);  pow_76 = None
    add_190 = mean_54 + 1e-06;  mean_54 = None
    rsqrt_54 = torch.rsqrt(add_190);  add_190 = None
    mul_220 = add_189 * rsqrt_54;  rsqrt_54 = None
    decoder_block_9_layer_2_layer_norm_weight = getattr(getattr(self.decoder.block, "9").layer, "2").layer_norm.weight
    getattr_171 = decoder_block_9_layer_2_layer_norm_weight.dtype
    eq_116 = getattr_171 == torch.float16;  getattr_171 = None
    getattr_172 = decoder_block_9_layer_2_layer_norm_weight.dtype
    to_116 = mul_220.to(getattr_172);  mul_220 = getattr_172 = None
    mul_221 = decoder_block_9_layer_2_layer_norm_weight * to_116;  decoder_block_9_layer_2_layer_norm_weight = to_116 = None
    decoder_block_9_layer_2_dense_relu_dense_wi_0 = getattr(getattr(self.decoder.block, "9").layer, "2").DenseReluDense.wi_0(mul_221)
    mul_222 = 0.5 * decoder_block_9_layer_2_dense_relu_dense_wi_0
    pow_77 = torch.pow(decoder_block_9_layer_2_dense_relu_dense_wi_0, 3.0)
    mul_223 = 0.044715 * pow_77;  pow_77 = None
    add_191 = decoder_block_9_layer_2_dense_relu_dense_wi_0 + mul_223;  decoder_block_9_layer_2_dense_relu_dense_wi_0 = mul_223 = None
    mul_224 = 0.7978845608028654 * add_191;  add_191 = None
    tanh_21 = torch.tanh(mul_224);  mul_224 = None
    add_192 = 1.0 + tanh_21;  tanh_21 = None
    mul_225 = mul_222 * add_192;  mul_222 = add_192 = None
    decoder_block_9_layer_2_dense_relu_dense_wi_1 = getattr(getattr(self.decoder.block, "9").layer, "2").DenseReluDense.wi_1(mul_221);  mul_221 = None
    mul_226 = mul_225 * decoder_block_9_layer_2_dense_relu_dense_wi_1;  mul_225 = decoder_block_9_layer_2_dense_relu_dense_wi_1 = None
    decoder_block_9_layer_2_dense_relu_dense_dropout = getattr(getattr(self.decoder.block, "9").layer, "2").DenseReluDense.dropout(mul_226);  mul_226 = None
    decoder_block_9_layer_2_dense_relu_dense_wo_weight = getattr(getattr(self.decoder.block, "9").layer, "2").DenseReluDense.wo.weight
    linear_53 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_53 = patch_linear_layer_linear_layer_triton_wrapper(decoder_block_9_layer_2_dense_relu_dense_dropout, linear_53);  decoder_block_9_layer_2_dense_relu_dense_dropout = linear_53 = None
    decoder_block_9_layer_2_dropout = getattr(getattr(self.decoder.block, "9").layer, "2").dropout(linear_layer_triton_wrapper_53);  linear_layer_triton_wrapper_53 = None
    add_193 = add_189 + decoder_block_9_layer_2_dropout;  add_189 = decoder_block_9_layer_2_dropout = None
    getattr_173 = add_193.dtype
    eq_117 = getattr_173 == torch.float16;  getattr_173 = None
    to_117 = add_193.to(torch.float32)
    pow_78 = to_117.pow(2);  to_117 = None
    mean_55 = pow_78.mean(-1, keepdim = True);  pow_78 = None
    add_194 = mean_55 + 1e-06;  mean_55 = None
    rsqrt_55 = torch.rsqrt(add_194);  add_194 = None
    mul_227 = add_193 * rsqrt_55;  rsqrt_55 = None
    decoder_block_10_layer_0_layer_norm_weight = getattr(getattr(self.decoder.block, "10").layer, "0").layer_norm.weight
    getattr_174 = decoder_block_10_layer_0_layer_norm_weight.dtype
    eq_118 = getattr_174 == torch.float16;  getattr_174 = None
    getattr_175 = decoder_block_10_layer_0_layer_norm_weight.dtype
    to_118 = mul_227.to(getattr_175);  mul_227 = getattr_175 = None
    mul_228 = decoder_block_10_layer_0_layer_norm_weight * to_118;  decoder_block_10_layer_0_layer_norm_weight = to_118 = None
    size_59 = mul_228.size()
    getitem_142 = size_59[slice(None, 2, None)];  size_59 = None
    getitem_143 = getitem_142[0]
    getitem_144 = getitem_142[1];  getitem_142 = None
    decoder_block_10_layer_0_self_attention_q = getattr(getattr(self.decoder.block, "10").layer, "0").SelfAttention.q(mul_228)
    view_130 = decoder_block_10_layer_0_self_attention_q.view(getitem_143, -1, 12, 64);  decoder_block_10_layer_0_self_attention_q = None
    transpose_160 = view_130.transpose(1, 2);  view_130 = None
    decoder_block_10_layer_0_self_attention_k = getattr(getattr(self.decoder.block, "10").layer, "0").SelfAttention.k(mul_228)
    view_131 = decoder_block_10_layer_0_self_attention_k.view(getitem_143, -1, 12, 64);  decoder_block_10_layer_0_self_attention_k = None
    transpose_161 = view_131.transpose(1, 2);  view_131 = None
    decoder_block_10_layer_0_self_attention_v = getattr(getattr(self.decoder.block, "10").layer, "0").SelfAttention.v(mul_228);  mul_228 = None
    view_132 = decoder_block_10_layer_0_self_attention_v.view(getitem_143, -1, 12, 64);  decoder_block_10_layer_0_self_attention_v = None
    transpose_162 = view_132.transpose(1, 2);  view_132 = None
    transpose_163 = transpose_161.transpose(3, 2)
    matmul_64 = torch.matmul(transpose_160, transpose_163);  transpose_160 = transpose_163 = None
    add_195 = matmul_64 + add_93;  matmul_64 = None
    float_35 = add_195.float()
    softmax_32 = torch.nn.functional.softmax(float_35, dim = -1, _stacklevel = 3, dtype = None);  float_35 = None
    type_as_32 = softmax_32.type_as(add_195);  softmax_32 = add_195 = None
    dropout_32 = torch.nn.functional.dropout(type_as_32, p = 0.1, training = False, inplace = False);  type_as_32 = None
    matmul_65 = torch.matmul(dropout_32, transpose_162);  dropout_32 = None
    transpose_164 = matmul_65.transpose(1, 2);  matmul_65 = None
    contiguous_32 = transpose_164.contiguous();  transpose_164 = None
    view_133 = contiguous_32.view(getitem_143, -1, 768);  contiguous_32 = getitem_143 = None
    linear_54 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_54 = patch_linear_layer_linear_layer_triton_wrapper(view_133, linear_54);  view_133 = linear_54 = None
    decoder_block_10_layer_0_dropout = getattr(getattr(self.decoder.block, "10").layer, "0").dropout(linear_layer_triton_wrapper_54);  linear_layer_triton_wrapper_54 = None
    add_196 = add_193 + decoder_block_10_layer_0_dropout;  add_193 = decoder_block_10_layer_0_dropout = None
    getattr_176 = add_196.dtype
    eq_119 = getattr_176 == torch.float16;  getattr_176 = None
    size_60 = transpose_161.size()
    getitem_145 = size_60[2];  size_60 = None
    to_119 = add_196.to(torch.float32)
    pow_79 = to_119.pow(2);  to_119 = None
    mean_56 = pow_79.mean(-1, keepdim = True);  pow_79 = None
    add_197 = mean_56 + 1e-06;  mean_56 = None
    rsqrt_56 = torch.rsqrt(add_197);  add_197 = None
    mul_229 = add_196 * rsqrt_56;  rsqrt_56 = None
    decoder_block_10_layer_1_layer_norm_weight = getattr(getattr(self.decoder.block, "10").layer, "1").layer_norm.weight
    getattr_177 = decoder_block_10_layer_1_layer_norm_weight.dtype
    eq_120 = getattr_177 == torch.float16;  getattr_177 = None
    getattr_178 = decoder_block_10_layer_1_layer_norm_weight.dtype
    to_120 = mul_229.to(getattr_178);  mul_229 = getattr_178 = None
    mul_230 = decoder_block_10_layer_1_layer_norm_weight * to_120;  decoder_block_10_layer_1_layer_norm_weight = to_120 = None
    size_61 = mul_230.size()
    getitem_146 = size_61[slice(None, 2, None)];  size_61 = None
    getitem_147 = getitem_146[0]
    getitem_148 = getitem_146[1];  getitem_146 = None
    size_62 = encoder_dropout_1.size()
    getitem_149 = size_62[1];  size_62 = None
    decoder_block_10_layer_1_enc_dec_attention_q = getattr(getattr(self.decoder.block, "10").layer, "1").EncDecAttention.q(mul_230);  mul_230 = None
    view_134 = decoder_block_10_layer_1_enc_dec_attention_q.view(getitem_147, -1, 12, 64);  decoder_block_10_layer_1_enc_dec_attention_q = None
    transpose_165 = view_134.transpose(1, 2);  view_134 = None
    decoder_block_10_layer_1_enc_dec_attention_k = getattr(getattr(self.decoder.block, "10").layer, "1").EncDecAttention.k(encoder_dropout_1)
    view_135 = decoder_block_10_layer_1_enc_dec_attention_k.view(getitem_147, -1, 12, 64);  decoder_block_10_layer_1_enc_dec_attention_k = None
    transpose_166 = view_135.transpose(1, 2);  view_135 = None
    decoder_block_10_layer_1_enc_dec_attention_v = getattr(getattr(self.decoder.block, "10").layer, "1").EncDecAttention.v(encoder_dropout_1)
    view_136 = decoder_block_10_layer_1_enc_dec_attention_v.view(getitem_147, -1, 12, 64);  decoder_block_10_layer_1_enc_dec_attention_v = None
    transpose_167 = view_136.transpose(1, 2);  view_136 = None
    transpose_168 = transpose_166.transpose(3, 2)
    matmul_66 = torch.matmul(transpose_165, transpose_168);  transpose_165 = transpose_168 = None
    add_198 = matmul_66 + add_97;  matmul_66 = None
    float_36 = add_198.float()
    softmax_33 = torch.nn.functional.softmax(float_36, dim = -1, _stacklevel = 3, dtype = None);  float_36 = None
    type_as_33 = softmax_33.type_as(add_198);  softmax_33 = add_198 = None
    dropout_33 = torch.nn.functional.dropout(type_as_33, p = 0.1, training = False, inplace = False);  type_as_33 = None
    matmul_67 = torch.matmul(dropout_33, transpose_167);  dropout_33 = None
    transpose_169 = matmul_67.transpose(1, 2);  matmul_67 = None
    contiguous_33 = transpose_169.contiguous();  transpose_169 = None
    view_137 = contiguous_33.view(getitem_147, -1, 768);  contiguous_33 = getitem_147 = None
    linear_55 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_55 = patch_linear_layer_linear_layer_triton_wrapper(view_137, linear_55);  view_137 = linear_55 = None
    decoder_block_10_layer_1_dropout = getattr(getattr(self.decoder.block, "10").layer, "1").dropout(linear_layer_triton_wrapper_55);  linear_layer_triton_wrapper_55 = None
    add_199 = add_196 + decoder_block_10_layer_1_dropout;  add_196 = decoder_block_10_layer_1_dropout = None
    getattr_179 = add_199.dtype
    eq_121 = getattr_179 == torch.float16;  getattr_179 = None
    to_121 = add_199.to(torch.float32)
    pow_80 = to_121.pow(2);  to_121 = None
    mean_57 = pow_80.mean(-1, keepdim = True);  pow_80 = None
    add_200 = mean_57 + 1e-06;  mean_57 = None
    rsqrt_57 = torch.rsqrt(add_200);  add_200 = None
    mul_231 = add_199 * rsqrt_57;  rsqrt_57 = None
    decoder_block_10_layer_2_layer_norm_weight = getattr(getattr(self.decoder.block, "10").layer, "2").layer_norm.weight
    getattr_180 = decoder_block_10_layer_2_layer_norm_weight.dtype
    eq_122 = getattr_180 == torch.float16;  getattr_180 = None
    getattr_181 = decoder_block_10_layer_2_layer_norm_weight.dtype
    to_122 = mul_231.to(getattr_181);  mul_231 = getattr_181 = None
    mul_232 = decoder_block_10_layer_2_layer_norm_weight * to_122;  decoder_block_10_layer_2_layer_norm_weight = to_122 = None
    decoder_block_10_layer_2_dense_relu_dense_wi_0 = getattr(getattr(self.decoder.block, "10").layer, "2").DenseReluDense.wi_0(mul_232)
    mul_233 = 0.5 * decoder_block_10_layer_2_dense_relu_dense_wi_0
    pow_81 = torch.pow(decoder_block_10_layer_2_dense_relu_dense_wi_0, 3.0)
    mul_234 = 0.044715 * pow_81;  pow_81 = None
    add_201 = decoder_block_10_layer_2_dense_relu_dense_wi_0 + mul_234;  decoder_block_10_layer_2_dense_relu_dense_wi_0 = mul_234 = None
    mul_235 = 0.7978845608028654 * add_201;  add_201 = None
    tanh_22 = torch.tanh(mul_235);  mul_235 = None
    add_202 = 1.0 + tanh_22;  tanh_22 = None
    mul_236 = mul_233 * add_202;  mul_233 = add_202 = None
    decoder_block_10_layer_2_dense_relu_dense_wi_1 = getattr(getattr(self.decoder.block, "10").layer, "2").DenseReluDense.wi_1(mul_232);  mul_232 = None
    mul_237 = mul_236 * decoder_block_10_layer_2_dense_relu_dense_wi_1;  mul_236 = decoder_block_10_layer_2_dense_relu_dense_wi_1 = None
    decoder_block_10_layer_2_dense_relu_dense_dropout = getattr(getattr(self.decoder.block, "10").layer, "2").DenseReluDense.dropout(mul_237);  mul_237 = None
    decoder_block_10_layer_2_dense_relu_dense_wo_weight = getattr(getattr(self.decoder.block, "10").layer, "2").DenseReluDense.wo.weight
    linear_56 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_56 = patch_linear_layer_linear_layer_triton_wrapper(decoder_block_10_layer_2_dense_relu_dense_dropout, linear_56);  decoder_block_10_layer_2_dense_relu_dense_dropout = linear_56 = None
    decoder_block_10_layer_2_dropout = getattr(getattr(self.decoder.block, "10").layer, "2").dropout(linear_layer_triton_wrapper_56);  linear_layer_triton_wrapper_56 = None
    add_203 = add_199 + decoder_block_10_layer_2_dropout;  add_199 = decoder_block_10_layer_2_dropout = None
    getattr_182 = add_203.dtype
    eq_123 = getattr_182 == torch.float16;  getattr_182 = None
    to_123 = add_203.to(torch.float32)
    pow_82 = to_123.pow(2);  to_123 = None
    mean_58 = pow_82.mean(-1, keepdim = True);  pow_82 = None
    add_204 = mean_58 + 1e-06;  mean_58 = None
    rsqrt_58 = torch.rsqrt(add_204);  add_204 = None
    mul_238 = add_203 * rsqrt_58;  rsqrt_58 = None
    decoder_block_11_layer_0_layer_norm_weight = getattr(getattr(self.decoder.block, "11").layer, "0").layer_norm.weight
    getattr_183 = decoder_block_11_layer_0_layer_norm_weight.dtype
    eq_124 = getattr_183 == torch.float16;  getattr_183 = None
    getattr_184 = decoder_block_11_layer_0_layer_norm_weight.dtype
    to_124 = mul_238.to(getattr_184);  mul_238 = getattr_184 = None
    mul_239 = decoder_block_11_layer_0_layer_norm_weight * to_124;  decoder_block_11_layer_0_layer_norm_weight = to_124 = None
    size_63 = mul_239.size()
    getitem_150 = size_63[slice(None, 2, None)];  size_63 = None
    getitem_151 = getitem_150[0]
    getitem_152 = getitem_150[1];  getitem_150 = None
    decoder_block_11_layer_0_self_attention_q = getattr(getattr(self.decoder.block, "11").layer, "0").SelfAttention.q(mul_239)
    view_138 = decoder_block_11_layer_0_self_attention_q.view(getitem_151, -1, 12, 64);  decoder_block_11_layer_0_self_attention_q = None
    transpose_170 = view_138.transpose(1, 2);  view_138 = None
    decoder_block_11_layer_0_self_attention_k = getattr(getattr(self.decoder.block, "11").layer, "0").SelfAttention.k(mul_239)
    view_139 = decoder_block_11_layer_0_self_attention_k.view(getitem_151, -1, 12, 64);  decoder_block_11_layer_0_self_attention_k = None
    transpose_171 = view_139.transpose(1, 2);  view_139 = None
    decoder_block_11_layer_0_self_attention_v = getattr(getattr(self.decoder.block, "11").layer, "0").SelfAttention.v(mul_239);  mul_239 = None
    view_140 = decoder_block_11_layer_0_self_attention_v.view(getitem_151, -1, 12, 64);  decoder_block_11_layer_0_self_attention_v = None
    transpose_172 = view_140.transpose(1, 2);  view_140 = None
    transpose_173 = transpose_171.transpose(3, 2)
    matmul_68 = torch.matmul(transpose_170, transpose_173);  transpose_170 = transpose_173 = None
    add_205 = matmul_68 + add_93;  matmul_68 = add_93 = None
    float_37 = add_205.float()
    softmax_34 = torch.nn.functional.softmax(float_37, dim = -1, _stacklevel = 3, dtype = None);  float_37 = None
    type_as_34 = softmax_34.type_as(add_205);  softmax_34 = add_205 = None
    dropout_34 = torch.nn.functional.dropout(type_as_34, p = 0.1, training = False, inplace = False);  type_as_34 = None
    matmul_69 = torch.matmul(dropout_34, transpose_172);  dropout_34 = None
    transpose_174 = matmul_69.transpose(1, 2);  matmul_69 = None
    contiguous_34 = transpose_174.contiguous();  transpose_174 = None
    view_141 = contiguous_34.view(getitem_151, -1, 768);  contiguous_34 = getitem_151 = None
    linear_57 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_57 = patch_linear_layer_linear_layer_triton_wrapper(view_141, linear_57);  view_141 = linear_57 = None
    decoder_block_11_layer_0_dropout = getattr(getattr(self.decoder.block, "11").layer, "0").dropout(linear_layer_triton_wrapper_57);  linear_layer_triton_wrapper_57 = None
    add_206 = add_203 + decoder_block_11_layer_0_dropout;  add_203 = decoder_block_11_layer_0_dropout = None
    getattr_185 = add_206.dtype
    eq_125 = getattr_185 == torch.float16;  getattr_185 = None
    size_64 = transpose_171.size()
    getitem_153 = size_64[2];  size_64 = None
    to_125 = add_206.to(torch.float32)
    pow_83 = to_125.pow(2);  to_125 = None
    mean_59 = pow_83.mean(-1, keepdim = True);  pow_83 = None
    add_207 = mean_59 + 1e-06;  mean_59 = None
    rsqrt_59 = torch.rsqrt(add_207);  add_207 = None
    mul_240 = add_206 * rsqrt_59;  rsqrt_59 = None
    decoder_block_11_layer_1_layer_norm_weight = getattr(getattr(self.decoder.block, "11").layer, "1").layer_norm.weight
    getattr_186 = decoder_block_11_layer_1_layer_norm_weight.dtype
    eq_126 = getattr_186 == torch.float16;  getattr_186 = None
    getattr_187 = decoder_block_11_layer_1_layer_norm_weight.dtype
    to_126 = mul_240.to(getattr_187);  mul_240 = getattr_187 = None
    mul_241 = decoder_block_11_layer_1_layer_norm_weight * to_126;  decoder_block_11_layer_1_layer_norm_weight = to_126 = None
    size_65 = mul_241.size()
    getitem_154 = size_65[slice(None, 2, None)];  size_65 = None
    getitem_155 = getitem_154[0]
    getitem_156 = getitem_154[1];  getitem_154 = None
    size_66 = encoder_dropout_1.size()
    getitem_157 = size_66[1];  size_66 = None
    decoder_block_11_layer_1_enc_dec_attention_q = getattr(getattr(self.decoder.block, "11").layer, "1").EncDecAttention.q(mul_241);  mul_241 = None
    view_142 = decoder_block_11_layer_1_enc_dec_attention_q.view(getitem_155, -1, 12, 64);  decoder_block_11_layer_1_enc_dec_attention_q = None
    transpose_175 = view_142.transpose(1, 2);  view_142 = None
    decoder_block_11_layer_1_enc_dec_attention_k = getattr(getattr(self.decoder.block, "11").layer, "1").EncDecAttention.k(encoder_dropout_1)
    view_143 = decoder_block_11_layer_1_enc_dec_attention_k.view(getitem_155, -1, 12, 64);  decoder_block_11_layer_1_enc_dec_attention_k = None
    transpose_176 = view_143.transpose(1, 2);  view_143 = None
    decoder_block_11_layer_1_enc_dec_attention_v = getattr(getattr(self.decoder.block, "11").layer, "1").EncDecAttention.v(encoder_dropout_1)
    view_144 = decoder_block_11_layer_1_enc_dec_attention_v.view(getitem_155, -1, 12, 64);  decoder_block_11_layer_1_enc_dec_attention_v = None
    transpose_177 = view_144.transpose(1, 2);  view_144 = None
    transpose_178 = transpose_176.transpose(3, 2)
    matmul_70 = torch.matmul(transpose_175, transpose_178);  transpose_175 = transpose_178 = None
    add_208 = matmul_70 + add_97;  matmul_70 = add_97 = None
    float_38 = add_208.float()
    softmax_35 = torch.nn.functional.softmax(float_38, dim = -1, _stacklevel = 3, dtype = None);  float_38 = None
    type_as_35 = softmax_35.type_as(add_208);  softmax_35 = add_208 = None
    dropout_35 = torch.nn.functional.dropout(type_as_35, p = 0.1, training = False, inplace = False);  type_as_35 = None
    matmul_71 = torch.matmul(dropout_35, transpose_177);  dropout_35 = None
    transpose_179 = matmul_71.transpose(1, 2);  matmul_71 = None
    contiguous_35 = transpose_179.contiguous();  transpose_179 = None
    view_145 = contiguous_35.view(getitem_155, -1, 768);  contiguous_35 = getitem_155 = None
    linear_58 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_58 = patch_linear_layer_linear_layer_triton_wrapper(view_145, linear_58);  view_145 = linear_58 = None
    decoder_block_11_layer_1_dropout = getattr(getattr(self.decoder.block, "11").layer, "1").dropout(linear_layer_triton_wrapper_58);  linear_layer_triton_wrapper_58 = None
    add_209 = add_206 + decoder_block_11_layer_1_dropout;  add_206 = decoder_block_11_layer_1_dropout = None
    getattr_188 = add_209.dtype
    eq_127 = getattr_188 == torch.float16;  getattr_188 = None
    to_127 = add_209.to(torch.float32)
    pow_84 = to_127.pow(2);  to_127 = None
    mean_60 = pow_84.mean(-1, keepdim = True);  pow_84 = None
    add_210 = mean_60 + 1e-06;  mean_60 = None
    rsqrt_60 = torch.rsqrt(add_210);  add_210 = None
    mul_242 = add_209 * rsqrt_60;  rsqrt_60 = None
    decoder_block_11_layer_2_layer_norm_weight = getattr(getattr(self.decoder.block, "11").layer, "2").layer_norm.weight
    getattr_189 = decoder_block_11_layer_2_layer_norm_weight.dtype
    eq_128 = getattr_189 == torch.float16;  getattr_189 = None
    getattr_190 = decoder_block_11_layer_2_layer_norm_weight.dtype
    to_128 = mul_242.to(getattr_190);  mul_242 = getattr_190 = None
    mul_243 = decoder_block_11_layer_2_layer_norm_weight * to_128;  decoder_block_11_layer_2_layer_norm_weight = to_128 = None
    decoder_block_11_layer_2_dense_relu_dense_wi_0 = getattr(getattr(self.decoder.block, "11").layer, "2").DenseReluDense.wi_0(mul_243)
    mul_244 = 0.5 * decoder_block_11_layer_2_dense_relu_dense_wi_0
    pow_85 = torch.pow(decoder_block_11_layer_2_dense_relu_dense_wi_0, 3.0)
    mul_245 = 0.044715 * pow_85;  pow_85 = None
    add_211 = decoder_block_11_layer_2_dense_relu_dense_wi_0 + mul_245;  decoder_block_11_layer_2_dense_relu_dense_wi_0 = mul_245 = None
    mul_246 = 0.7978845608028654 * add_211;  add_211 = None
    tanh_23 = torch.tanh(mul_246);  mul_246 = None
    add_212 = 1.0 + tanh_23;  tanh_23 = None
    mul_247 = mul_244 * add_212;  mul_244 = add_212 = None
    decoder_block_11_layer_2_dense_relu_dense_wi_1 = getattr(getattr(self.decoder.block, "11").layer, "2").DenseReluDense.wi_1(mul_243);  mul_243 = None
    mul_248 = mul_247 * decoder_block_11_layer_2_dense_relu_dense_wi_1;  mul_247 = decoder_block_11_layer_2_dense_relu_dense_wi_1 = None
    decoder_block_11_layer_2_dense_relu_dense_dropout = getattr(getattr(self.decoder.block, "11").layer, "2").DenseReluDense.dropout(mul_248);  mul_248 = None
    decoder_block_11_layer_2_dense_relu_dense_wo_weight = getattr(getattr(self.decoder.block, "11").layer, "2").DenseReluDense.wo.weight
    linear_59 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_59 = patch_linear_layer_linear_layer_triton_wrapper(decoder_block_11_layer_2_dense_relu_dense_dropout, linear_59);  decoder_block_11_layer_2_dense_relu_dense_dropout = linear_59 = None
    decoder_block_11_layer_2_dropout = getattr(getattr(self.decoder.block, "11").layer, "2").dropout(linear_layer_triton_wrapper_59);  linear_layer_triton_wrapper_59 = None
    add_213 = add_209 + decoder_block_11_layer_2_dropout;  add_209 = decoder_block_11_layer_2_dropout = None
    getattr_191 = add_213.dtype
    eq_129 = getattr_191 == torch.float16;  getattr_191 = None
    to_129 = add_213.to(torch.float32)
    pow_86 = to_129.pow(2);  to_129 = None
    mean_61 = pow_86.mean(-1, keepdim = True);  pow_86 = None
    add_214 = mean_61 + 1e-06;  mean_61 = None
    rsqrt_61 = torch.rsqrt(add_214);  add_214 = None
    mul_249 = add_213 * rsqrt_61;  add_213 = rsqrt_61 = None
    decoder_final_layer_norm_weight = self.decoder.final_layer_norm.weight
    getattr_192 = decoder_final_layer_norm_weight.dtype
    eq_130 = getattr_192 == torch.float16;  getattr_192 = None
    getattr_193 = decoder_final_layer_norm_weight.dtype
    to_130 = mul_249.to(getattr_193);  mul_249 = getattr_193 = None
    mul_250 = decoder_final_layer_norm_weight * to_130;  decoder_final_layer_norm_weight = to_130 = None
    decoder_dropout_1 = self.decoder.dropout(mul_250);  mul_250 = None
    linear_60 = getattr(getattr(self.encoder.block, "0").layer, "0").SelfAttention.o
    linear_layer_triton_wrapper_60 = patch_linear_layer_linear_layer_triton_wrapper(decoder_dropout_1, linear_60);  decoder_dropout_1 = linear_60 = None
    getattr_194 = linear_layer_triton_wrapper_60.device
    to_131 = labels.to(getattr_194);  labels = getattr_194 = None
    size_67 = linear_layer_triton_wrapper_60.size(-1)
    view_146 = linear_layer_triton_wrapper_60.view(-1, size_67);  size_67 = None
    view_147 = to_131.view(-1);  to_131 = None
    crossentropyloss_0 = self.crossentropyloss_0(view_146, view_147);  view_146 = view_147 = None
    return {'loss': crossentropyloss_0, 'logits': linear_layer_triton_wrapper_60, 'past_key_values': ((transpose_61, transpose_62, transpose_66, transpose_67), (transpose_71, transpose_72, transpose_76, transpose_77), (transpose_81, transpose_82, transpose_86, transpose_87), (transpose_91, transpose_92, transpose_96, transpose_97), (transpose_101, transpose_102, transpose_106, transpose_107), (transpose_111, transpose_112, transpose_116, transpose_117), (transpose_121, transpose_122, transpose_126, transpose_127), (transpose_131, transpose_132, transpose_136, transpose_137), (transpose_141, transpose_142, transpose_146, transpose_147), (transpose_151, transpose_152, transpose_156, transpose_157), (transpose_161, transpose_162, transpose_166, transpose_167), (transpose_171, transpose_172, transpose_176, transpose_177)), 'encoder_last_hidden_state': encoder_dropout_1}
    
Seq2SeqLMOutput(loss=tensor(16.2812, device='cuda:0', dtype=torch.float16,
       grad_fn=<NllLossBackward0>), logits=tensor([[[-37.4688,  -3.2031,  -8.9531,  ..., -37.6875, -37.1875, -37.5938],
         [-37.2812,  -6.9570,  -3.3477,  ..., -37.2812, -37.2812, -37.3125],
         [-35.2188,  -3.4863,  -5.8906,  ..., -35.2188, -35.0312, -35.1875],
         ...,
         [-39.9688,  -2.3926,  -6.9414,  ..., -39.9688, -39.9688, -39.8438],
         [-45.2188,  -6.4922,  -4.5352,  ..., -45.3125, -45.1875, -45.2188],
         [-46.1875,  -2.2070,  -7.6445,  ..., -46.0938, -45.9688, -46.2188]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<UnsafeViewBackward0>), past_key_values=((tensor([[[[-1.4062, -2.3379, -0.2510,  ..., -0.2925, -0.5488,  0.6602],
          [-1.0010, -1.1963,  0.9966,  ..., -0.7324, -0.1941,  0.4707],
          [ 1.7461,  3.0508, -1.7012,  ..., -0.5527,  0.5078,  0.4995],
          ...,
          [ 1.2363,  0.4592,  0.0193,  ...,  0.8340, -0.0773, -0.5869],
          [ 1.0449, -1.0117,  0.0996,  ...,  0.0462, -0.3101, -0.4370],
          [ 0.0870, -1.2705,  1.1328,  ..., -0.1655, -0.0054, -0.2500]],

         [[ 0.1974, -0.2898,  0.0053,  ..., -0.3538, -0.5542,  0.5547],
          [ 0.7598,  1.0049,  0.4763,  ...,  0.3518,  0.1201,  1.2217],
          [ 1.5332,  0.0213,  0.3379,  ...,  0.1000, -0.9067, -1.6162],
          ...,
          [-0.7930,  0.3130,  0.8833,  ...,  0.2898,  0.5278, -0.8545],
          [ 1.0811, -0.6831, -0.1262,  ..., -0.3228,  0.4741, -0.6387],
          [ 0.0839, -0.2292,  0.2466,  ...,  0.0716, -0.0519,  0.3882]],

         [[-0.3208,  0.1606, -0.0848,  ..., -0.0121, -0.1642, -0.0408],
          [ 0.3987, -0.3516, -0.7212,  ...,  0.6372, -0.4023,  0.2893],
          [-0.4521,  0.3254, -0.2458,  ..., -1.0820, -0.3149,  1.1807],
          ...,
          [-0.1075,  0.5439,  1.1738,  ..., -0.0075, -0.3958, -0.3706],
          [-0.4438,  0.1749,  0.1614,  ...,  0.6660,  0.0447,  0.5601],
          [ 0.5454, -0.2422, -0.2413,  ...,  0.1421,  0.3147, -0.1406]],

         ...,

         [[ 2.2031,  1.6104,  0.0535,  ...,  0.2117, -0.3962, -0.1949],
          [ 1.3193,  3.7930, -0.5215,  ..., -0.9731, -2.9395, -0.5771],
          [-0.6606,  1.0811, -0.0326,  ..., -0.3496, -0.4792,  0.4927],
          ...,
          [-0.2712, -3.2910, -0.3262,  ...,  0.5166,  1.2373, -0.1157],
          [-0.1959,  0.0342, -0.3999,  ...,  0.1508,  1.0146,  0.2915],
          [ 1.2578,  3.6230, -0.4824,  ..., -0.7827, -2.1934, -0.3235]],

         [[ 0.6157,  0.0432,  1.7822,  ..., -0.3418, -1.2383,  0.6108],
          [ 1.6582,  0.3525, -0.6978,  ...,  0.9604,  0.2656,  0.4192],
          [ 0.2905,  1.1240, -0.1523,  ...,  0.9980,  0.0316, -0.4731],
          ...,
          [-0.8066, -0.6045, -1.1670,  ...,  0.1436,  0.4382,  0.3660],
          [ 0.1320,  0.1560, -0.4749,  ...,  0.3896,  0.2488, -0.3186],
          [ 1.2588,  0.4749, -0.1243,  ..., -0.1038,  0.3557,  0.5034]],

         [[-2.4727, -0.3430, -1.4219,  ..., -0.3027,  1.1582, -1.1201],
          [-1.4658, -0.7744, -0.6084,  ..., -0.0999,  0.3872,  0.9111],
          [ 1.9844, -0.8633,  0.4036,  ...,  0.6436,  0.2627, -0.7012],
          ...,
          [-1.1123,  0.3284,  2.5566,  ...,  0.5044,  1.4590,  0.7642],
          [-0.6665,  0.0668,  0.3892,  ..., -0.4934, -1.2363, -0.7974],
          [-0.0187, -0.2227, -0.2703,  ..., -0.0373,  0.0312,  0.1862]]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<TransposeBackward0>), tensor([[[[-0.2350,  0.0858,  0.0180,  ...,  0.2791, -0.0471,  0.0566],
          [-0.2123, -0.1472,  0.1111,  ...,  0.7334,  0.0261, -0.0634],
          [-0.4009, -2.0879, -1.9658,  ..., -0.0675, -0.2184,  0.6357],
          ...,
          [-0.4031,  0.2480, -0.1509,  ...,  0.0189, -0.3918, -0.1020],
          [-2.5234,  0.9282,  0.0359,  ...,  0.4175, -1.0732, -0.1320],
          [-0.1843, -0.0510,  0.4021,  ...,  0.1365,  0.0303, -0.0490]],

         [[-0.2622,  0.1313,  0.0585,  ..., -0.0590,  0.1534,  0.1450],
          [ 0.0676,  0.5151, -0.0687,  ..., -0.6592,  0.4641,  0.1161],
          [ 0.6104, -1.1025,  0.6548,  ..., -0.7866, -1.0117, -1.9258],
          ...,
          [-0.0637, -0.7964,  0.3086,  ..., -0.4019,  0.1027,  0.2084],
          [-0.0410,  0.1215,  0.0742,  ..., -0.1664,  0.0678,  0.0357],
          [ 0.3103,  0.0295,  0.0988,  ..., -0.5220,  0.1520, -0.0640]],

         [[-0.1758, -0.2032, -0.1755,  ...,  0.1936,  0.0551, -0.2974],
          [-0.7759, -0.0919,  1.4678,  ..., -0.1807,  0.2744,  0.5762],
          [-0.6436,  1.7500,  2.7559,  ..., -0.2844, -0.5825,  1.5918],
          ...,
          [-1.0156, -0.1000, -0.4348,  ..., -0.1510, -0.9585,  0.8496],
          [ 0.9644, -0.0513, -0.4780,  ...,  0.5396,  0.7212,  0.5186],
          [-0.1454, -0.0161,  0.3899,  ..., -0.2371,  0.2783,  0.4922]],

         ...,

         [[-0.1120, -0.0649, -0.1211,  ...,  1.1201,  0.0098, -0.0986],
          [-0.0079, -0.5376, -0.5000,  ..., -0.2078, -1.5566, -0.1715],
          [ 0.1715, -0.1389, -0.4348,  ..., -0.9355, -0.0111, -0.0712],
          ...,
          [ 0.0877,  0.1642,  0.4785,  ...,  1.3340,  0.7700,  0.0643],
          [-0.2539, -0.2059, -0.3184,  ...,  0.8374,  0.1897, -0.0667],
          [ 0.8237, -0.2185,  1.6133,  ...,  0.1910, -1.4629, -0.1153]],

         [[-0.2313, -0.0318, -0.5435,  ..., -0.1385, -0.0411, -0.0632],
          [-0.3809,  0.0312,  0.7334,  ..., -0.8120, -0.2949, -0.6357],
          [ 2.1055, -0.6313,  1.0918,  ...,  0.3169,  0.0122, -0.4146],
          ...,
          [-0.0217, -0.9736,  0.3687,  ..., -0.4138, -0.2166, -0.1048],
          [-0.1370, -0.3547, -0.8521,  ..., -0.1443, -0.0613,  0.5513],
          [-0.0980, -0.0844,  0.0120,  ..., -0.4050, -0.5020, -0.1389]],

         [[ 0.0491,  0.0281,  0.0057,  ...,  0.0734,  0.0574, -0.1411],
          [-0.1703, -0.3884,  0.2280,  ...,  0.2206,  0.0757, -0.2355],
          [ 0.8325,  0.9458,  0.6235,  ...,  1.3730,  0.0514,  0.7261],
          ...,
          [ 1.8105, -0.8228,  3.4199,  ..., -0.2576, -0.8198,  0.1500],
          [ 0.3711,  1.5430,  1.0381,  ..., -0.7202,  0.7080, -1.3535],
          [-0.2007, -0.6919,  0.2208,  ..., -0.0757, -0.1365, -0.1611]]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<TransposeBackward0>), tensor([[[[-1.0811e+00, -1.2537e-01, -1.4636e-01,  ...,  1.1279e+00,
            1.8867e+00,  3.1372e-01],
          [-8.9307e-01, -4.2114e-01,  3.5913e-01,  ...,  2.6294e-01,
            6.6992e-01,  6.3171e-02],
          [-1.1934e+00, -5.2832e-01,  5.6494e-01,  ...,  1.1191e+00,
            7.6172e-01,  5.1660e-01],
          ...,
          [-5.6915e-02, -5.0928e-01,  1.4609e+00,  ...,  7.5195e-01,
            1.9619e+00, -3.4912e-02],
          [ 1.8691e+00, -1.0176e+00, -3.0391e+00,  ...,  5.9509e-02,
            8.1543e-01, -5.6787e-01],
          [ 4.2651e-01, -3.2745e-02, -1.3733e-01,  ..., -4.4360e-01,
           -6.2207e-01,  4.6460e-01]],

         [[-3.6182e-01, -4.6460e-01, -4.6069e-01,  ..., -8.7354e-01,
            2.1367e+00, -9.6240e-01],
          [-4.8218e-01,  5.0049e-01,  5.5225e-01,  ..., -2.0801e+00,
            5.0635e-01, -5.4004e-01],
          [-5.8496e-01, -3.2275e-01,  6.2842e-01,  ..., -1.6523e+00,
            4.0259e-01, -8.0811e-02],
          ...,
          [-5.4639e-01,  8.5059e-01, -1.1401e-01,  ..., -4.8169e-01,
           -1.1761e-01,  2.2229e-01],
          [-9.2285e-01, -3.5864e-01,  5.4150e-01,  ..., -4.6313e-01,
            1.0732e+00, -8.2715e-01],
          [ 1.5369e-01,  1.8970e-01,  1.2482e-02,  ...,  1.2871e+00,
           -2.3022e-01,  6.1279e-01]],

         [[ 2.0156e+00, -7.1875e-01,  5.3564e-01,  ..., -2.7599e-03,
           -8.0032e-03, -1.6992e+00],
          [ 1.7100e+00,  1.2079e-01,  8.7061e-01,  ...,  6.7627e-02,
            4.6606e-01, -1.1504e+00],
          [ 1.3018e+00,  3.9722e-01,  5.6299e-01,  ..., -6.3232e-01,
            1.0920e-03, -5.2393e-01],
          ...,
          [ 1.1406e+00,  1.2708e-01,  1.1709e+00,  ..., -3.6987e-01,
            7.4268e-01, -7.2705e-01],
          [ 1.4980e+00, -1.8506e-01,  1.8379e+00,  ...,  3.8232e-01,
            3.7476e-01, -2.0684e+00],
          [-5.4785e-01,  1.4385e+00, -2.5742e+00,  ...,  5.4932e-01,
           -1.2372e-01,  1.3892e-01]],

         ...,

         [[ 1.0059e+00,  8.4229e-01, -6.1279e-01,  ...,  5.8203e-01,
            9.8438e-01, -1.1240e+00],
          [ 4.8047e-01,  1.1895e+00, -7.0410e-01,  ...,  1.0723e+00,
           -3.6743e-01, -1.6133e+00],
          [ 6.2012e-01, -1.9608e-02, -5.2832e-01,  ...,  1.7627e+00,
           -5.1953e-01, -1.8291e+00],
          ...,
          [ 1.2480e+00,  4.4116e-01, -2.0684e+00,  ...,  1.1143e+00,
           -2.9810e-01, -2.1729e-01],
          [ 1.8242e+00,  6.6699e-01,  2.3621e-01,  ...,  9.2969e-01,
           -3.3936e-01, -4.2749e-01],
          [ 1.6211e-01, -1.2764e+00,  6.8481e-02,  ..., -4.7144e-01,
            8.8318e-02, -5.3418e-01]],

         [[ 8.8330e-01,  2.3468e-02,  6.1426e-01,  ...,  1.3330e+00,
            3.8525e-01, -1.7249e-01],
          [ 1.4099e-01, -6.5430e-01,  1.1211e+00,  ...,  1.7051e+00,
            5.5225e-01, -1.0919e-01],
          [ 2.6807e-01, -8.1201e-01,  1.6270e+00,  ...,  1.3408e+00,
            7.1777e-01, -7.4268e-01],
          ...,
          [ 5.0964e-02, -5.1221e-01,  1.1895e+00,  ..., -1.2568e+00,
            1.1113e+00, -1.2578e+00],
          [-4.8730e-01, -8.4033e-01,  9.9609e-01,  ...,  1.7451e+00,
            9.8975e-01, -7.9590e-01],
          [ 5.7275e-01,  2.8003e-01,  9.4336e-01,  ...,  9.0332e-02,
           -3.9429e-02,  9.1699e-01]],

         [[ 1.3291e+00, -8.0469e-01, -2.4451e-01,  ...,  1.9551e+00,
           -9.9792e-02,  1.0117e+00],
          [ 3.8906e+00, -1.0361e+00, -2.9205e-02,  ...,  9.6729e-01,
           -5.4108e-02,  1.0195e+00],
          [ 3.0488e+00, -1.4834e+00,  9.7839e-02,  ...,  1.1143e+00,
           -1.1896e-01,  3.0518e-01],
          ...,
          [ 4.3286e-01, -6.2061e-01, -6.6113e-01,  ...,  1.3301e+00,
            2.1313e-01,  5.7373e-01],
          [ 1.3145e+00,  1.6738e+00,  1.8799e+00,  ..., -2.7090e+00,
            2.3125e+00, -3.1445e-01],
          [ 7.6855e-01,  1.3018e+00,  2.0007e-01,  ..., -5.8936e-01,
            2.4695e-01, -9.5520e-02]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>), tensor([[[[ 1.0244e+00, -8.8965e-01,  9.1992e-01,  ...,  5.3125e-01,
           -1.0859e+00,  4.3823e-01],
          [-2.8174e-01, -1.7832e+00,  1.0693e+00,  ...,  6.5527e-01,
           -8.4229e-01,  7.8516e-01],
          [-5.2930e-01, -6.7529e-01,  6.8262e-01,  ...,  5.3906e-01,
           -1.2695e+00,  5.6885e-01],
          ...,
          [ 8.9404e-01, -8.1934e-01,  8.4521e-01,  ..., -6.0840e-01,
           -5.8105e-01,  5.1611e-01],
          [-2.6685e-01, -2.2539e+00,  4.7021e-01,  ..., -9.4092e-01,
           -1.5400e+00,  8.7354e-01],
          [-3.3379e-03,  5.8057e-01,  2.1826e-01,  ...,  7.8979e-02,
           -3.2074e-02,  3.4204e-01]],

         [[-1.1172e+00,  1.6191e+00, -1.0107e+00,  ..., -2.4841e-01,
           -2.1309e+00,  2.1172e+00],
          [-1.6807e+00,  1.1826e+00, -1.7041e+00,  ...,  3.2562e-02,
           -2.0078e+00,  1.0459e+00],
          [-2.6836e+00,  1.3745e-01, -1.3418e+00,  ..., -1.5076e-01,
           -1.0371e+00,  1.5674e+00],
          ...,
          [-7.0068e-01, -6.3818e-01, -7.9590e-01,  ...,  2.1924e-01,
           -2.7051e+00,  2.5366e-01],
          [-1.3489e-01,  4.2969e-01, -4.2383e-01,  ..., -4.5312e-01,
           -8.5498e-01,  1.4688e+00],
          [ 2.6904e-01,  1.1407e-01, -5.9473e-01,  ...,  8.8818e-01,
            1.9482e-01, -5.5713e-01]],

         [[ 8.5547e-01, -7.4756e-01, -1.9568e-01,  ..., -1.0781e+00,
           -4.8248e-02, -4.6851e-01],
          [ 5.0586e-01,  4.3994e-01, -1.1260e+00,  ..., -9.8145e-01,
           -5.7617e-01,  1.7358e-01],
          [ 2.1851e-01,  4.4360e-01, -8.8477e-01,  ...,  2.3572e-01,
           -7.8369e-01,  2.8149e-01],
          ...,
          [ 6.7810e-02, -1.7383e+00, -6.8481e-02,  ..., -2.9834e-01,
           -1.2285e+00, -2.2290e-01],
          [-1.4268e+00, -9.1357e-01, -1.6650e+00,  ..., -2.5664e+00,
           -5.3516e-01, -1.4014e+00],
          [ 1.2988e-01,  6.9824e-01, -1.2622e-01,  ...,  1.1578e-01,
            2.1277e-01,  6.9763e-02]],

         ...,

         [[-1.1548e-01, -1.6807e+00,  1.9521e+00,  ..., -2.3608e-01,
           -1.1816e+00,  1.4521e+00],
          [ 2.3035e-01, -4.4849e-01,  1.2197e+00,  ...,  6.7432e-01,
           -3.0005e-01,  1.2139e+00],
          [ 5.3516e-01,  5.3516e-01,  1.5049e+00,  ...,  9.5215e-01,
            4.8608e-01,  7.4561e-01],
          ...,
          [ 1.1123e+00, -8.4912e-01,  1.1748e+00,  ...,  9.8438e-01,
            6.1279e-01,  1.9658e+00],
          [ 9.2969e-01, -7.8467e-01,  3.8916e-01,  ...,  7.2266e-01,
            6.4697e-01,  2.0684e+00],
          [ 2.4915e-01,  5.6824e-02,  2.8259e-02,  ...,  2.0618e-01,
           -5.0934e-02, -1.0657e-01]],

         [[-8.5498e-01, -1.2969e+00, -4.3854e-02,  ..., -5.6641e-01,
            1.9702e-01,  1.1729e+00],
          [-3.9990e-01,  7.9632e-05,  2.1716e-01,  ..., -3.4790e-02,
            1.3965e-01,  1.8115e+00],
          [-2.0312e-01, -6.8701e-01, -1.0382e-01,  ..., -5.6824e-02,
            3.9038e-01,  1.3281e+00],
          ...,
          [ 1.8970e-01, -5.2686e-01,  1.7480e+00,  ..., -4.7510e-01,
           -4.4336e-01,  1.0527e+00],
          [-6.8945e-01, -6.0742e-01, -5.2832e-01,  ..., -1.6270e+00,
            3.0786e-01,  7.6318e-01],
          [ 1.8066e-01, -1.9885e-01, -1.8408e-01,  ...,  1.1719e-01,
           -1.5979e-01, -1.0645e+00]],

         [[-4.5557e-01, -5.8789e-01,  7.4756e-01,  ..., -8.3374e-02,
            1.7383e+00, -1.0176e+00],
          [-5.6738e-01,  1.3000e-01,  5.4346e-01,  ..., -4.3115e-01,
            1.3242e+00, -1.7646e+00],
          [-6.9336e-01,  2.2620e-01,  1.4124e-01,  ..., -1.0068e+00,
            3.3032e-01, -1.0020e+00],
          ...,
          [ 8.3203e-01,  2.6001e-01,  7.5562e-02,  ...,  3.8281e-01,
            3.1714e-01, -4.1431e-01],
          [-1.6162e-01, -7.6904e-01, -1.1357e+00,  ...,  4.8853e-01,
           -8.1006e-01,  2.0483e-01],
          [ 1.9641e-01,  2.5488e-01,  3.4595e-01,  ...,  2.4561e-01,
            3.7506e-02,  8.1055e-02]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>)), (tensor([[[[ 1.7126e-01,  1.0391e+00, -1.4148e-01,  ...,  8.8623e-01,
            8.8623e-01,  4.9866e-02],
          [ 1.8818e+00,  8.7451e-01, -1.1211e+00,  ...,  1.1143e+00,
            1.3223e+00,  2.0820e+00],
          [-1.6768e+00,  2.7832e-01, -1.2754e+00,  ..., -2.1426e+00,
           -2.3398e+00, -4.0015e-01],
          ...,
          [-1.6220e-02,  1.1310e-01,  2.2324e+00,  ...,  5.3418e-01,
           -1.1494e+00, -1.0059e+00],
          [-2.6929e-01, -2.6113e+00, -1.7354e+00,  ...,  1.1836e+00,
           -2.1523e+00, -2.4941e+00],
          [ 1.6504e+00,  8.8477e-01, -2.2441e+00,  ...,  9.5117e-01,
            4.0552e-01,  1.5234e+00]],

         [[ 4.9072e-01,  4.0527e-01, -3.4863e-01,  ...,  1.4966e-01,
            4.7192e-01, -2.6709e-01],
          [-9.1748e-01,  1.8984e+00, -4.1309e-01,  ..., -1.2415e-01,
           -1.0547e+00, -1.0176e+00],
          [ 1.3904e-01,  3.9404e-01,  1.1748e+00,  ...,  1.3306e-01,
           -1.5781e+00,  3.1812e-01],
          ...,
          [-1.8223e+00,  9.8975e-01,  2.9468e-01,  ...,  3.9600e-01,
           -1.8447e+00, -1.3965e-01],
          [-1.7139e+00,  1.2334e+00, -3.7061e-01,  ..., -7.2266e-01,
            4.9835e-02,  1.2383e+00],
          [-7.5830e-01,  9.2969e-01, -8.4521e-01,  ..., -4.5319e-02,
           -1.5986e+00, -1.3733e-01]],

         [[ 3.6157e-01,  2.8906e-01,  2.4231e-01,  ...,  1.1285e-01,
           -2.5635e-01, -5.5811e-01],
          [-9.0210e-02, -7.2803e-01,  6.1279e-01,  ...,  9.7949e-01,
            2.7168e+00,  3.8300e-02],
          [-6.2305e-01, -1.2520e+00,  5.3986e-02,  ..., -2.2498e-01,
            1.3770e+00, -5.9277e-01],
          ...,
          [ 7.7539e-01, -2.1008e-01, -6.5820e-01,  ...,  1.8372e-02,
            1.5537e+00, -5.9766e-01],
          [ 1.3174e+00, -1.4839e-02,  2.2888e-01,  ...,  7.2852e-01,
           -1.2622e-01, -8.3008e-01],
          [ 7.5439e-01, -1.0029e+00, -8.0469e-01,  ...,  9.7363e-01,
            3.3613e+00, -4.9023e-01]],

         ...,

         [[ 3.3667e-01, -1.2461e+00,  3.1079e-01,  ..., -5.0732e-01,
           -1.1395e-01,  1.9971e-01],
          [ 3.8239e-02,  1.5273e+00,  1.5234e+00,  ...,  4.7095e-01,
           -2.6587e-01,  5.1611e-01],
          [-4.6167e-01,  5.2734e-01, -1.3953e-01,  ...,  5.1611e-01,
           -1.0901e-01, -4.9121e-01],
          ...,
          [ 1.6571e-02,  1.0840e+00,  5.7617e-01,  ...,  2.9614e-01,
           -1.2390e-02,  3.0249e-01],
          [ 3.8892e-01,  4.1040e-01,  8.9050e-02,  ...,  1.0469e+00,
            1.9678e-01, -1.2256e-01],
          [-7.3535e-01,  1.2002e+00,  1.4453e+00,  ...,  7.1240e-01,
            3.7085e-01,  5.9082e-01]],

         [[-4.5825e-01,  7.2412e-01,  3.4644e-01,  ..., -3.1445e-01,
           -7.9102e-01,  3.1299e-01],
          [ 5.3359e+00, -4.7773e+00, -4.5117e+00,  ..., -3.7559e+00,
            2.3398e+00,  1.9590e+00],
          [ 1.9434e-01, -9.4482e-01, -7.7783e-01,  ..., -3.4668e-02,
            1.8301e+00,  4.0723e-01],
          ...,
          [ 1.1514e+00, -1.8213e+00, -1.1455e+00,  ..., -1.4414e+00,
            3.3418e+00,  6.5723e-01],
          [ 1.4619e+00, -8.7451e-01, -7.3047e-01,  ..., -1.1426e+00,
            2.7246e+00,  5.2588e-01],
          [ 3.2422e+00, -4.3789e+00, -1.0107e+00,  ..., -1.7266e+00,
            3.5625e+00,  2.5371e+00]],

         [[ 6.7078e-02, -1.1162e+00,  1.9800e-01,  ..., -1.7476e-04,
           -3.0273e-01, -1.5879e+00],
          [ 4.8926e-01, -8.0713e-01, -1.5713e+00,  ...,  1.3271e+00,
           -2.1445e+00,  1.9443e+00],
          [-1.0029e+00,  2.0605e-01, -1.4404e+00,  ..., -1.3074e-01,
            1.1104e+00,  2.3281e+00],
          ...,
          [-6.3574e-01, -1.1709e+00, -7.7930e-01,  ...,  1.9385e-01,
           -2.0469e+00,  2.0977e+00],
          [ 1.1689e+00, -1.1914e+00, -4.2480e-01,  ..., -3.2983e-01,
            9.0149e-02,  6.8848e-01],
          [-1.4490e-01, -7.7576e-02, -1.1260e+00,  ...,  1.1895e+00,
           -7.7100e-01,  1.5400e+00]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>), tensor([[[[ 1.5274e-02, -2.1133e-02,  5.0430e-03,  ..., -1.8463e-02,
            9.6970e-03,  1.7136e-02],
          [-3.5913e-01,  1.4246e-01,  7.8174e-01,  ..., -6.9629e-01,
           -1.3350e+00, -5.0049e-01],
          [-1.3818e-01, -1.4929e-01,  5.8887e-01,  ...,  8.4570e-01,
           -3.3472e-01,  5.1123e-01],
          ...,
          [-8.4717e-01,  2.0520e-01, -8.8867e-01,  ..., -8.0566e-01,
           -6.7139e-01, -1.0144e-01],
          [-4.5825e-01,  1.8860e-01,  2.3887e+00,  ..., -3.1030e-01,
           -1.8323e-01, -2.5757e-01],
          [-2.2620e-01, -1.1176e-01,  7.9248e-01,  ..., -3.7329e-01,
           -1.8965e+00, -5.3027e-01]],

         [[-8.1360e-02, -1.0266e-01,  2.2156e-02,  ...,  3.5278e-02,
           -8.0933e-02,  1.4771e-01],
          [ 4.6191e-01, -4.8779e-01, -4.5776e-01,  ...,  7.0557e-01,
            1.9177e-01,  3.3447e-01],
          [-4.1748e-01, -1.1504e+00, -9.5312e-01,  ..., -1.1298e-01,
            1.3269e-01,  1.9688e+00],
          ...,
          [-8.0750e-02, -6.8799e-01,  1.8848e-01,  ...,  1.1455e+00,
            7.2559e-01,  7.4414e-01],
          [ 3.1052e-02, -1.5479e+00,  4.7144e-01,  ...,  6.7578e-01,
           -5.5811e-01, -1.2930e+00],
          [-3.8477e-01, -3.8965e-01, -3.5083e-01,  ...,  8.5645e-01,
           -1.1572e-01, -2.6245e-01]],

         [[-1.1737e-01,  1.4587e-01,  1.2524e-01,  ...,  4.8340e-02,
            6.6650e-01,  5.0732e-01],
          [-2.1252e-01, -1.1982e+00, -3.7018e-02,  ..., -2.6465e-01,
            1.6074e+00,  9.4434e-01],
          [-1.9014e+00, -1.9443e+00,  1.5078e+00,  ..., -9.3506e-01,
            2.9258e+00, -4.1113e-01],
          ...,
          [-2.1277e-01, -1.4424e+00, -7.9883e-01,  ..., -7.4854e-01,
            2.0996e+00,  3.1616e-01],
          [ 7.6218e-03, -1.5459e+00, -2.1936e-01,  ..., -2.2021e-01,
            2.2129e+00,  4.6680e-01],
          [-2.3425e-01, -1.8281e+00, -8.3057e-01,  ..., -4.7217e-01,
            8.2568e-01,  7.4023e-01]],

         ...,

         [[-3.0174e-03,  1.0370e-01,  6.1371e-02,  ...,  1.1725e-01,
            2.2388e-01,  1.2128e-01],
          [-3.1621e+00, -5.2979e-01, -1.9541e+00,  ..., -2.9453e+00,
           -1.5527e+00, -1.0391e+00],
          [ 1.8140e-01, -5.7910e-01, -8.0469e-01,  ...,  1.2250e-01,
           -3.2275e-01, -2.8467e-01],
          ...,
          [ 5.5957e-01,  1.6632e-02, -2.1152e+00,  ..., -4.9170e-01,
            3.8501e-01,  5.9766e-01],
          [-4.6173e-02, -5.0391e-01, -1.6416e+00,  ..., -4.1016e-01,
            4.9097e-01,  4.6478e-02],
          [-2.9434e+00, -1.5996e+00, -2.0977e+00,  ..., -2.9590e+00,
           -1.1680e+00, -1.0547e+00]],

         [[ 2.1655e-01, -3.7500e-01,  5.2734e-01,  ..., -3.7256e-01,
           -3.1201e-01,  4.7089e-02],
          [ 1.1113e+00, -1.3013e-01, -4.8364e-01,  ..., -1.2373e+00,
           -2.5757e-01, -7.1729e-01],
          [-4.5508e-01,  9.4482e-01, -4.9683e-01,  ..., -7.7393e-01,
            6.9873e-01,  3.7933e-02],
          ...,
          [-1.3237e-02,  4.6387e-01, -5.8398e-01,  ..., -3.0298e-01,
            1.7041e-01,  3.9331e-01],
          [ 2.2864e-01,  8.9697e-01, -1.4922e+00,  ..., -2.6709e-01,
            1.1768e-01,  6.8457e-01],
          [ 7.2754e-01, -3.3569e-01, -6.4209e-01,  ..., -1.0400e+00,
           -5.3174e-01, -6.9385e-01]],

         [[-7.4036e-02,  5.9998e-02,  2.9278e-04,  ..., -6.5002e-02,
            3.4943e-02,  3.5126e-02],
          [ 5.7666e-01, -1.9211e-02,  2.2815e-01,  ..., -3.5474e-01,
           -1.0264e+00,  4.8157e-02],
          [-1.9324e-01,  1.5344e-01, -1.0195e+00,  ..., -7.9639e-01,
           -5.8105e-01, -3.9893e-01],
          ...,
          [ 1.4180e+00, -7.9785e-01,  1.0887e-02,  ...,  4.1772e-01,
           -1.5977e+00, -2.4353e-01],
          [-2.5610e-01,  3.6035e-01,  2.0273e+00,  ...,  5.2686e-01,
           -1.2109e+00, -1.5588e-01],
          [-2.9639e-01, -6.7627e-02,  8.6426e-01,  ..., -1.0779e-01,
           -3.1323e-01,  1.5588e-01]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>), tensor([[[[-0.6353,  0.1630,  0.5767,  ...,  0.7935, -0.9512, -1.2617],
          [-0.9355, -0.5244,  0.9971,  ..., -1.1162, -0.3252, -0.7280],
          [-0.4797, -0.3997, -0.0036,  ..., -1.3535, -0.3999, -0.9961],
          ...,
          [-0.0743,  0.8740,  0.0629,  ..., -0.0822, -0.1995,  0.0342],
          [ 0.4404,  1.5732,  0.1260,  ...,  0.4551,  0.7886, -0.3872],
          [-0.5347,  1.8936, -0.5420,  ...,  1.1904,  0.5410, -0.2908]],

         [[-0.0451, -0.8618,  0.3691,  ...,  0.5718,  0.0144, -0.1362],
          [-0.0575, -1.0391,  1.0049,  ..., -0.1150, -1.0244, -0.7852],
          [-0.8940, -1.0859,  1.0996,  ...,  0.1208,  0.8198, -0.1713],
          ...,
          [-0.8809, -0.8721,  0.0440,  ..., -0.8491,  0.6504, -0.3777],
          [ 0.6211, -0.3218,  0.3899,  ..., -0.0235,  0.2656, -0.2145],
          [-0.3137,  1.2100,  0.0281,  ..., -0.0473,  0.9058, -0.1138]],

         [[-0.7554,  0.1998, -0.4915,  ...,  0.2467,  2.5117,  1.2041],
          [-0.8115, -0.4348, -1.0566,  ...,  0.9434,  1.7275,  1.9287],
          [-0.3367, -0.8086, -0.6938,  ...,  2.1484,  1.6436,  0.9785],
          ...,
          [ 0.6133,  0.0998,  0.0742,  ...,  1.0391,  1.1621, -1.4180],
          [-0.1699, -1.5020, -0.2112,  ...,  1.5176,  2.0117,  1.2051],
          [-0.4368,  0.7925,  0.3601,  ..., -0.6772, -1.3262, -0.4875]],

         ...,

         [[-0.4385, -0.7549, -0.3748,  ...,  0.9058, -0.0854, -0.8154],
          [ 0.0984, -1.0371, -1.0879,  ...,  1.3154,  0.3245, -1.0410],
          [ 0.1793, -0.3479, -1.5820,  ...,  0.8867, -0.0543, -2.6797],
          ...,
          [-0.0811,  0.0394, -0.0099,  ...,  0.9595, -0.4189, -2.3730],
          [-0.0974,  1.0195, -1.0459,  ...,  1.3799,  0.2393, -0.6362],
          [ 0.6436, -0.2676, -0.2820,  ..., -0.1459,  0.0711,  1.5820]],

         [[-1.2510,  0.0279, -0.6680,  ..., -1.1045, -0.2710, -0.4656],
          [-1.6123,  0.0676, -0.4167,  ..., -3.1055, -1.1133, -2.0645],
          [-1.2891, -1.8809, -0.0856,  ..., -2.7832, -0.0515, -2.0742],
          ...,
          [-1.1592,  0.4275,  0.4854,  ..., -0.9194, -0.5664, -0.4868],
          [-1.5947,  0.4575, -0.4858,  ..., -1.1396, -0.0665, -0.3083],
          [ 3.3496, -0.1192, -0.9717,  ...,  0.4106, -1.1260,  0.1052]],

         [[ 0.8306,  0.0718,  0.6396,  ..., -2.1113, -1.0684, -0.9438],
          [ 0.9053, -1.6123, -0.6846,  ..., -1.4092, -0.5908, -1.1123],
          [-0.3154,  0.7817, -0.8696,  ..., -1.2148, -1.2217, -0.3977],
          ...,
          [ 0.2192,  0.1744, -0.7139,  ..., -0.9531, -0.7104,  0.4485],
          [ 0.1447, -1.5752,  0.4885,  ...,  0.8335, -0.0085,  1.4658],
          [-0.0791,  1.5938,  0.3430,  ...,  0.0488, -0.1109, -0.5693]]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<TransposeBackward0>), tensor([[[[-0.2009, -0.5283,  0.2245,  ...,  0.5459,  0.1125, -1.0254],
          [-0.0105, -1.7764, -0.9873,  ..., -1.2627,  0.7368,  0.4460],
          [-0.0393, -0.5156, -0.8179,  ..., -0.8418, -0.0271,  1.2197],
          ...,
          [ 0.6899,  1.1045,  0.5469,  ..., -0.9551,  0.4648,  0.2556],
          [ 1.4629, -0.7856,  1.5381,  ...,  0.2944,  0.5728,  0.3640],
          [ 0.4500,  0.0131,  0.4578,  ..., -0.4294, -0.2043,  0.2363]],

         [[ 1.0146, -0.1199, -0.6987,  ...,  0.0098, -0.6860, -0.6812],
          [-0.2986, -1.2266,  1.0781,  ...,  0.4861, -1.0127,  0.0928],
          [-0.5010, -0.3335,  0.9805,  ...,  1.1172, -0.2456, -0.2498],
          ...,
          [-0.3120, -0.5659,  1.1387,  ...,  1.2051, -1.1465, -0.5371],
          [ 0.1572,  0.1439,  0.4456,  ..., -0.1247, -0.5308, -0.0873],
          [-0.0818,  0.1919, -0.0573,  ...,  0.2336,  0.5933, -0.2113]],

         [[ 0.1020,  0.3281, -0.1223,  ...,  0.9160,  0.2179,  0.8608],
          [-0.9849, -0.0335,  0.2261,  ...,  1.1133,  0.6416,  0.1615],
          [-1.1279, -0.4988, -0.1729,  ...,  0.8564,  1.0410, -0.0843],
          ...,
          [-1.1777, -1.0332,  0.8525,  ...,  0.6123,  0.1567,  0.0839],
          [-1.0742, -1.1426,  2.1953,  ..., -0.5225, -1.4199, -0.0071],
          [ 0.1201, -0.0443,  0.0034,  ...,  0.1858, -0.1274, -0.1015]],

         ...,

         [[-0.9170,  0.2386,  1.2480,  ...,  0.1488, -0.3721, -1.6328],
          [-0.6602,  1.0605,  0.6597,  ...,  0.9390, -0.3188, -0.6729],
          [-1.1211,  0.7847,  0.5420,  ...,  1.0508,  0.0384, -1.0127],
          ...,
          [-1.4336,  1.9297, -1.3438,  ...,  0.8770,  0.4539, -0.6157],
          [-0.7363,  0.8247,  0.4817,  ...,  0.1796, -0.5664, -2.5801],
          [ 0.1087, -0.7134,  0.4290,  ...,  0.1980, -0.0111, -0.3269]],

         [[ 0.3232, -0.8862, -0.2773,  ..., -0.8066,  0.2328, -0.0967],
          [-0.2306,  0.6177, -0.9370,  ..., -1.3398, -0.7012, -0.1320],
          [ 0.0763,  0.0945, -0.8994,  ..., -0.9243, -0.0654, -0.2000],
          ...,
          [ 0.2201, -0.1401, -1.0205,  ..., -1.6836, -0.1783,  0.0881],
          [ 1.9121,  0.8315, -0.3765,  ...,  0.2277,  0.0806, -1.0439],
          [-0.1171,  0.0212,  0.3579,  ...,  0.3872,  0.4353,  0.1824]],

         [[ 1.9697,  0.1781,  0.0779,  ...,  1.0342, -0.0544, -1.1152],
          [ 0.0757,  0.2064, -0.1165,  ..., -0.6011, -0.6416, -1.8955],
          [-0.8306,  1.9844, -0.4233,  ...,  0.7017, -0.3774,  0.5801],
          ...,
          [-2.4707, -0.2130,  0.6113,  ..., -0.9136,  0.4763, -0.3428],
          [-1.4160,  0.0370, -0.1035,  ...,  1.4043, -0.3674, -0.0748],
          [ 0.3203, -0.1232,  0.3984,  ..., -0.3459,  0.2544, -0.1003]]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<TransposeBackward0>)), (tensor([[[[ 0.9316,  0.2169, -0.1622,  ...,  0.0877, -0.5361,  2.9004],
          [-1.0537,  0.1477,  0.2477,  ...,  0.8027,  1.1719, -2.0957],
          [-1.6514,  0.5771, -1.1289,  ...,  2.3750,  0.5049, -3.1621],
          ...,
          [-2.6562,  0.2024,  0.0487,  ...,  1.5908,  1.2607, -1.8672],
          [-1.8242,  0.8687,  0.2566,  ...,  1.5137,  0.9292, -3.2266],
          [-1.3936, -0.3071,  0.3892,  ..., -0.2773,  1.0283, -1.6943]],

         [[ 0.2245, -0.1868,  0.1593,  ...,  0.8628,  0.1025,  0.4644],
          [-0.9360,  0.7642,  0.6782,  ..., -0.5435,  0.6250, -0.1980],
          [-2.9258,  1.4834, -0.9600,  ...,  0.8633, -0.0475, -1.2803],
          ...,
          [-2.0781,  0.9688,  0.5835,  ..., -0.9072,  1.7061, -2.0527],
          [-1.2422,  1.5605, -0.4009,  ..., -1.6289,  1.9414, -1.0127],
          [-2.1953,  0.9150,  1.3789,  ..., -1.3916,  0.9673, -1.3975]],

         [[-0.2063,  0.7300, -0.1169,  ..., -0.6182, -0.1864,  0.4324],
          [-0.6909, -0.1655,  0.4622,  ...,  0.9170,  0.6006, -1.0420],
          [-1.4463,  0.4294, -0.1198,  ..., -0.1049,  0.2771, -0.2981],
          ...,
          [-0.4673, -0.4517,  0.5010,  ...,  1.3057,  0.9062, -1.4199],
          [-0.8560, -0.4841,  0.2622,  ...,  1.8154,  0.9736, -1.5791],
          [-0.5371, -1.1631,  0.7104,  ...,  1.5957,  0.6294, -0.2659]],

         ...,

         [[ 0.0623, -0.0755, -0.3696,  ...,  0.0380,  0.0714, -0.2000],
          [ 0.9595,  0.0950,  0.6519,  ...,  0.3699, -0.6387,  1.5771],
          [-0.3398, -0.5938,  0.2424,  ...,  0.5283, -0.8013,  0.7715],
          ...,
          [ 0.3896, -0.7192,  0.3120,  ...,  0.6577, -0.6499,  0.4316],
          [ 0.4387, -0.4656,  0.0245,  ...,  0.7593, -0.5747,  0.6738],
          [ 0.5728, -0.2184,  0.8896,  ...,  0.3806, -0.9492,  1.3096]],

         [[-0.0746,  0.6104,  0.0755,  ...,  0.1116,  0.0453, -0.1576],
          [-1.2510, -0.7681, -1.6436,  ..., -0.0210,  1.0537, -0.7056],
          [-0.1741, -0.2498, -0.5625,  ..., -0.0753,  1.0957, -0.7080],
          ...,
          [-2.1816, -1.0947, -1.0469,  ..., -0.1205,  1.6992,  0.6680],
          [-1.3057, -0.9941, -0.4136,  ...,  0.2625,  1.1455,  0.1775],
          [-0.4395, -3.5762, -2.3223,  ...,  0.4219,  1.5986,  0.1222]],

         [[ 0.5229, -0.0876, -0.2439,  ..., -0.1121, -0.1058, -2.4668],
          [-0.5571, -0.5762,  0.7905,  ...,  0.0085, -0.7124,  1.5811],
          [-0.5576, -0.1068,  0.1031,  ...,  1.8232, -0.5591,  1.5908],
          ...,
          [-1.3564, -0.1013,  0.2983,  ..., -0.6074,  0.4531,  1.1865],
          [-0.3997, -1.2314,  0.4092,  ..., -0.4795,  0.0786,  1.4209],
          [-1.3096, -0.4233,  0.5107,  ...,  0.1903,  0.9873,  2.2441]]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<TransposeBackward0>), tensor([[[[ 1.2903e-01,  1.7212e-01,  1.2457e-01,  ...,  1.6907e-01,
           -1.0901e-01,  7.6843e-02],
          [-1.3350e+00, -5.7471e-01, -8.7158e-01,  ...,  3.4277e-01,
           -6.5723e-01,  2.2537e-02],
          [-1.6777e+00, -4.3921e-01, -1.4668e+00,  ...,  2.0293e+00,
           -1.1309e+00,  7.1924e-01],
          ...,
          [ 1.4099e-01, -1.3574e-01, -7.5928e-01,  ..., -9.9072e-01,
            1.2863e-02, -5.1172e-01],
          [-1.1445e+00,  5.3906e-01, -3.5132e-01,  ..., -9.7534e-02,
           -8.6731e-02, -1.5146e+00],
          [-5.7861e-01, -5.4932e-01,  2.7759e-01,  ..., -9.0820e-01,
            2.0977e+00, -5.7910e-01]],

         [[-8.5022e-02,  6.1157e-02,  2.3804e-01,  ...,  2.0233e-02,
            3.5461e-02,  9.5581e-02],
          [ 1.1426e+00,  9.8730e-01,  9.9561e-01,  ..., -9.6338e-01,
           -4.0479e-01, -2.7710e-01],
          [ 7.1191e-01,  1.5928e+00, -8.8135e-01,  ..., -1.6396e+00,
           -1.6211e+00, -9.9463e-01],
          ...,
          [ 1.4600e+00, -9.2346e-02, -7.0459e-01,  ..., -3.0640e-01,
           -2.0203e-01,  4.8340e-01],
          [ 9.6533e-01,  4.3774e-01, -7.3608e-02,  ..., -1.1797e+00,
           -8.4839e-02, -5.7568e-01],
          [ 1.2188e+00, -5.8838e-02,  6.0059e-01,  ..., -5.9863e-01,
           -4.6851e-01,  4.8853e-01]],

         [[-7.4768e-02,  1.6382e-01, -1.2769e-01,  ..., -1.5771e-01,
            1.5503e-01, -3.3228e-01],
          [-1.2500e+00, -4.6600e-02, -9.9609e-02,  ...,  9.8633e-01,
           -2.5049e-01, -2.5317e-01],
          [ 2.1436e-01,  2.1118e-01, -4.8279e-02,  ...,  6.9385e-01,
           -4.4678e-01, -1.1846e+00],
          ...,
          [-6.7725e-01, -1.2286e-01,  2.1606e-01,  ...,  9.1113e-01,
           -2.8906e-01, -3.4570e-01],
          [-1.0117e+00,  1.8579e-01,  3.7109e-01,  ...,  6.5967e-01,
           -1.5735e-01, -2.3181e-01],
          [-1.0947e+00, -9.9512e-01,  4.9512e-01,  ...,  1.7178e+00,
           -9.4580e-01, -1.0107e+00]],

         ...,

         [[ 1.1206e-01, -1.1334e-01,  3.9673e-02,  ..., -1.1780e-01,
            6.4087e-02, -3.1421e-01],
          [ 4.8615e-02,  4.1333e-01, -1.9202e-01,  ...,  3.8013e-01,
            3.3301e-01,  1.2539e+00],
          [ 1.0986e+00,  6.4795e-01, -1.1719e+00,  ...,  1.4404e+00,
            8.3862e-02,  1.3516e+00],
          ...,
          [ 1.2305e+00,  6.8555e-01,  2.3584e-01,  ...,  2.9248e-01,
            5.4248e-01,  4.8767e-02],
          [ 1.3789e+00,  5.2881e-01,  4.1943e-01,  ...,  3.4131e-01,
            9.7119e-01, -2.3962e-01],
          [ 1.4141e+00,  3.5010e-01,  1.1865e+00,  ..., -1.3086e+00,
            2.4551e+00, -1.0664e+00]],

         [[-1.0699e-01, -1.7700e-01, -6.3672e-01,  ..., -2.3889e-01,
            2.8052e-01, -2.1826e-01],
          [ 7.1777e-01, -7.7686e-01, -5.7275e-01,  ...,  1.3193e+00,
            1.4807e-01,  3.6108e-01],
          [-9.6985e-02, -3.8135e-01,  4.2847e-01,  ...,  1.6101e-01,
           -4.2871e-01, -5.9473e-01],
          ...,
          [ 9.3555e-01, -6.2793e-01, -1.3206e-02,  ..., -5.1514e-01,
           -4.6387e-01,  6.2675e-03],
          [ 3.9819e-01, -1.6279e+00,  3.4790e-02,  ..., -5.7324e-01,
            9.0967e-01,  5.3027e-01],
          [ 6.6016e-01, -5.8154e-01,  1.0576e+00,  ...,  9.4922e-01,
            3.7280e-01, -4.5776e-01]],

         [[-7.9203e-04,  5.3589e-02,  1.2549e-01,  ...,  1.4624e-01,
           -7.0679e-02, -7.9285e-02],
          [-9.0271e-02, -1.8469e-01,  2.9224e-01,  ..., -3.3234e-02,
            5.9814e-01,  4.2603e-01],
          [ 3.8623e-01, -2.9639e-01,  6.2549e-01,  ..., -8.9941e-01,
           -7.8613e-01, -3.1641e-01],
          ...,
          [ 4.8242e-01,  1.4561e+00, -5.5713e-01,  ...,  4.9683e-02,
            7.4072e-01,  5.3662e-01],
          [ 4.0601e-01,  8.4766e-01,  1.1940e-03,  ..., -2.2668e-01,
            9.7168e-01,  3.0884e-01],
          [-9.6973e-01, -2.4829e-01,  3.8501e-01,  ...,  1.7725e+00,
            4.5142e-01,  5.7080e-01]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>), tensor([[[[-0.7041, -0.7788,  0.3198,  ..., -0.1868, -0.6157, -0.2947],
          [-0.2396,  0.8911,  0.5454,  ..., -0.0970, -0.3787,  0.1917],
          [ 0.1591, -0.3503,  0.2108,  ..., -0.4165, -0.9175,  1.1123],
          ...,
          [ 1.8877,  0.6182, -0.5283,  ..., -0.5356,  0.7100, -0.2463],
          [ 0.3091, -0.5786,  0.1793,  ..., -1.0830, -0.3608, -0.3960],
          [-0.1997, -1.2441,  1.0439,  ..., -0.4038, -1.0254, -0.0240]],

         [[-0.7446,  0.0056,  0.0518,  ...,  1.0342,  0.6343,  0.4202],
          [ 0.0532,  1.0615,  1.0713,  ...,  2.1211, -0.4617,  0.0083],
          [-0.2510, -0.5278,  0.2944,  ...,  1.4619, -0.2422,  0.1279],
          ...,
          [ 0.2690, -0.1998, -0.3003,  ...,  0.6592, -0.6055,  0.1527],
          [-1.4053,  0.9258, -0.9829,  ...,  1.1348,  0.5723, -0.3635],
          [ 2.1113, -1.5342,  0.3176,  ..., -0.4016, -0.3962,  0.5522]],

         [[ 0.2465,  1.8154,  0.0751,  ...,  1.1953, -0.6646, -0.7407],
          [ 0.0313,  1.3916, -0.7217,  ...,  0.8325, -0.2715,  1.0283],
          [ 0.8105,  0.6611, -0.4846,  ...,  1.0088, -0.1891,  0.5825],
          ...,
          [ 0.6445,  0.1792,  0.1080,  ...,  1.3740,  0.3357,  1.2715],
          [ 1.1104,  0.7192, -0.4824,  ...,  0.5376,  0.6558, -0.4272],
          [-0.0914, -0.0056, -0.0193,  ...,  0.1671, -0.1262, -1.7100]],

         ...,

         [[ 0.6743, -0.6138,  0.9297,  ...,  0.2291,  0.6235, -0.3352],
          [ 0.4189,  0.9365,  0.6289,  ...,  0.3555,  0.8643, -0.6572],
          [ 0.4756, -0.3406, -0.4409,  ...,  0.1646,  1.1367, -0.4475],
          ...,
          [ 0.6709, -0.4348,  0.1326,  ...,  0.8545,  0.0173,  0.2771],
          [ 1.7744, -0.8892, -0.4080,  ...,  0.9536,  0.1262,  0.9810],
          [-0.3269, -0.8022,  0.5708,  ...,  0.1038,  0.5811,  0.1885]],

         [[ 1.4932, -1.0889,  0.6401,  ...,  0.0795,  0.3130,  0.5664],
          [ 2.1895, -1.1943,  1.3955,  ..., -0.2871, -1.0869,  1.4189],
          [ 1.3018, -0.8623,  0.3970,  ..., -0.2382,  0.0399,  1.4961],
          ...,
          [ 1.3604, -0.0460, -0.0448,  ...,  0.1681,  1.3633, -0.4883],
          [ 0.3589, -0.4602, -0.3003,  ...,  1.3936,  2.0234, -0.1422],
          [-0.7583, -0.6411,  0.1990,  ...,  0.4033, -1.0762, -0.1823]],

         [[-0.1831, -0.9419,  0.3164,  ..., -0.7695,  0.9453, -0.4277],
          [-0.8599, -0.5400,  0.9214,  ..., -1.9688, -0.1488,  0.1098],
          [-1.2578,  0.6719,  0.9365,  ..., -0.6470, -0.3167, -0.3130],
          ...,
          [ 0.5474,  1.4150, -0.0433,  ...,  0.5913,  1.0713,  0.5210],
          [-0.3792,  0.2417, -0.3333,  ..., -0.2825,  0.4629, -0.3416],
          [-0.2236, -1.2119, -0.1431,  ..., -0.3845, -0.3274, -0.0787]]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<TransposeBackward0>), tensor([[[[ 1.2529e+00,  1.6342e-02,  1.1096e-01,  ..., -5.2032e-03,
            1.1523e+00, -4.2407e-01],
          [ 2.0371e+00,  9.8291e-01, -7.4854e-01,  ...,  2.1230e+00,
            1.2634e-01, -3.0664e-01],
          [ 9.7070e-01,  1.6270e+00, -1.1826e+00,  ...,  1.1963e+00,
            5.3857e-01, -1.0244e+00],
          ...,
          [-2.0630e-01,  3.3325e-01,  5.0415e-02,  ...,  8.1641e-01,
            9.4727e-02,  1.1221e+00],
          [-7.0068e-01,  8.7219e-02,  1.2051e+00,  ..., -6.8457e-01,
           -6.2561e-02,  1.8340e+00],
          [ 1.1163e-01, -2.8091e-02,  1.2421e-01,  ..., -8.0322e-02,
            1.8091e-01,  4.0869e-01]],

         [[-5.6006e-01,  7.5195e-01,  2.6855e-01,  ...,  2.7905e-01,
            6.4880e-02,  2.7466e-01],
          [-1.6406e-01,  2.7124e-01,  2.1426e+00,  ..., -5.9863e-01,
           -7.8491e-02, -4.3579e-02],
          [-9.8340e-01, -2.0374e-01,  1.5527e+00,  ..., -7.0215e-01,
           -5.1367e-01, -3.6499e-01],
          ...,
          [ 5.8057e-01,  2.1570e-01,  1.3262e+00,  ..., -1.8567e-01,
            1.3794e-01,  5.4590e-01],
          [ 1.0107e+00,  5.2148e-01,  7.9199e-01,  ..., -4.7144e-01,
           -1.1768e-01, -1.3311e+00],
          [-7.1167e-02,  5.1904e-01, -1.1709e+00,  ...,  2.7954e-01,
           -3.7842e-01,  1.3828e+00]],

         [[-1.2656e+00,  9.4238e-01,  1.2988e+00,  ...,  4.4824e-01,
            3.5474e-01, -4.5508e-01],
          [-1.0557e+00, -6.0107e-01,  2.3789e+00,  ..., -1.1592e+00,
            6.9434e-01,  8.2471e-01],
          [-1.0732e+00, -4.9512e-01,  1.4375e+00,  ..., -8.4521e-01,
            6.7285e-01,  2.1753e-01],
          ...,
          [-7.5635e-01,  8.1201e-01, -1.4912e+00,  ...,  8.8379e-02,
           -2.1521e-01, -7.5439e-01],
          [-3.7524e-01,  1.7568e+00,  6.0645e-01,  ..., -9.3896e-01,
            1.0596e+00, -2.3230e-01],
          [ 1.7078e-01, -3.5962e-01,  2.3590e-02,  ..., -1.4758e-01,
            1.6638e-01, -1.2537e-01]],

         ...,

         [[ 4.2358e-01, -6.3965e-01,  1.5479e-01,  ..., -1.4404e-01,
            3.5254e-01, -1.7004e-01],
          [-1.3281e-01, -1.5811e+00,  2.0142e-01,  ..., -6.5479e-01,
            9.2578e-01, -9.4727e-01],
          [ 2.8418e-01, -1.4570e+00,  1.0928e+00,  ...,  3.8745e-01,
            6.7578e-01, -1.2024e-01],
          ...,
          [ 5.6299e-01,  8.0371e-01,  1.3672e+00,  ...,  5.3076e-01,
            9.3848e-01,  2.3022e-01],
          [-2.0312e+00,  5.8740e-01,  4.9225e-02,  ..., -4.4434e-01,
            9.1992e-01,  5.2979e-01],
          [ 4.2664e-02, -1.6406e-01,  3.5547e-01,  ...,  4.0430e-01,
           -3.7427e-01, -2.3043e-04]],

         [[-1.9043e-02, -8.7549e-01,  7.4707e-01,  ..., -1.0742e+00,
            1.1212e-01, -8.5205e-01],
          [-1.7900e+00,  6.6650e-01,  4.7290e-01,  ..., -1.0410e+00,
           -3.6377e-01, -9.2773e-01],
          [-4.2065e-01,  1.6638e-01,  8.8428e-01,  ..., -4.4214e-01,
           -5.7959e-01,  2.6520e-02],
          ...,
          [-9.2346e-02, -7.5195e-01, -3.8965e-01,  ...,  1.5674e+00,
           -7.0117e-01,  2.5244e-01],
          [-1.6504e-01, -1.4814e+00,  1.1914e-01,  ...,  3.8501e-01,
           -5.8496e-01,  3.4570e-01],
          [-3.1555e-02,  2.9248e-01,  8.4473e-02,  ...,  4.5410e-02,
           -1.1194e-01, -1.1401e-01]],

         [[-8.1104e-01,  2.3230e-01, -8.7109e-01,  ..., -1.5420e+00,
            1.6670e+00, -3.9478e-01],
          [ 1.5552e-01, -3.9642e-02, -1.7715e+00,  ..., -8.4766e-01,
            9.2871e-01,  2.0239e-01],
          [-7.5317e-02, -3.2104e-01, -1.0596e+00,  ..., -3.8916e-01,
            6.6748e-01, -5.9619e-01],
          ...,
          [-9.7559e-01,  1.3953e-01, -7.2461e-01,  ...,  1.1853e-01,
            2.0630e-01, -4.5337e-01],
          [ 2.1814e-01,  4.1162e-01,  4.9536e-01,  ...,  1.8030e-01,
           -1.8467e+00, -2.0781e+00],
          [-1.5710e-01, -1.2012e-01, -2.2003e-02,  ..., -2.1545e-01,
           -1.0773e-01, -3.7994e-02]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>)), (tensor([[[[-0.1141,  0.4600, -0.1891,  ...,  0.1033,  0.1620,  0.5786],
          [ 0.1637, -0.7031,  0.4927,  ..., -2.1465, -1.3574,  1.1592],
          [ 0.8579,  0.1692, -0.7798,  ...,  0.2522, -2.0898, -1.2598],
          ...,
          [ 0.2517, -0.8618,  2.8945,  ..., -0.4663,  0.7085,  0.1454],
          [-0.5366, -0.3955,  1.1816,  ..., -1.0117,  0.2272, -0.3647],
          [ 1.1045,  1.3223,  1.6709,  ..., -1.9385,  0.2142,  1.0527]],

         [[-0.1473, -0.6382,  0.0476,  ...,  0.3223, -0.2935, -0.4517],
          [-0.1320,  0.8530, -1.1250,  ...,  0.6035,  0.7720, -1.6064],
          [-0.0558,  2.3633,  0.7676,  ...,  0.2463, -0.2257, -0.4885],
          ...,
          [ 0.2812,  2.8203, -0.5010,  ...,  1.1406,  1.1855, -0.8770],
          [ 0.5493,  2.8516, -0.2273,  ...,  0.5308,  0.9897, -0.4692],
          [ 0.0126,  2.3105,  0.2988,  ...,  1.2285,  1.8398, -0.2244]],

         [[-0.5254,  0.0551, -0.4587,  ...,  0.4312, -1.2490, -0.1146],
          [ 1.3926, -1.6045,  1.2881,  ..., -0.0806,  4.2188, -0.0629],
          [ 0.8066,  0.5435, -0.6841,  ..., -0.8569,  4.2188,  1.6826],
          ...,
          [-0.4355,  0.0072,  1.7607,  ..., -1.4443,  5.2891,  1.9658],
          [ 0.1476,  0.7173,  1.1221,  ..., -0.7070,  4.7500,  1.5078],
          [-0.0742,  0.1616,  0.1602,  ...,  0.7480,  5.2617,  0.5078]],

         ...,

         [[ 0.2249, -0.1104,  0.0442,  ..., -0.0837, -0.0247,  0.0268],
          [ 0.8252,  0.5996,  0.1683,  ...,  0.6401,  0.3372,  0.7368],
          [ 0.3801, -0.8833, -0.8730,  ...,  0.9946, -1.6475, -1.5674],
          ...,
          [ 1.1123,  0.2969,  0.9907,  ..., -0.2722, -0.9966,  0.7959],
          [ 0.5479,  0.4053,  0.5649,  ..., -0.8589, -1.2305,  1.7598],
          [ 0.6978,  0.0963, -1.0986,  ..., -0.0374, -0.9360,  0.1318]],

         [[ 0.9897, -0.1296,  0.5503,  ...,  0.5840, -0.0401,  0.3970],
          [-0.7188,  1.1543, -0.7241,  ..., -2.0176, -0.1687, -0.8501],
          [-1.5127,  0.6909, -0.1362,  ...,  1.0635, -1.7686, -0.6787],
          ...,
          [-0.4832,  1.1230,  0.3379,  ..., -0.8218,  0.4246, -0.5840],
          [-1.4707,  1.5059, -0.0433,  ..., -1.1367, -0.1691, -1.0674],
          [-0.3665,  0.4773, -0.2893,  ..., -1.6221, -0.6045, -1.6777]],

         [[-1.7480, -0.0386, -0.0583,  ..., -0.2200,  0.7520,  0.5239],
          [ 2.0664,  0.6528, -0.0230,  ...,  0.2091, -1.3330, -2.7617],
          [ 2.9648,  0.8452, -0.1058,  ..., -0.2363, -0.0881, -0.7002],
          ...,
          [ 2.7773,  1.1689, -1.9502,  ..., -0.5840, -2.7734,  0.7578],
          [ 3.3516,  0.1909, -1.0713,  ...,  0.7153, -2.0840,  0.3076],
          [ 3.1992,  1.1748,  1.0879,  ...,  0.0811, -2.8711, -1.8252]]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<TransposeBackward0>), tensor([[[[ 2.5024e-01, -8.6975e-02, -1.0071e-01,  ..., -2.1057e-03,
           -1.6235e-02, -4.4952e-02],
          [ 6.2451e-01, -1.0234e+00,  1.4673e-01,  ...,  5.9326e-01,
           -1.6406e-01,  2.0898e+00],
          [ 2.2871e+00, -4.3481e-01, -1.2148e+00,  ..., -4.5776e-01,
           -3.4454e-02, -1.3887e+00],
          ...,
          [-5.9277e-01, -4.8486e-01,  2.8027e-01,  ...,  1.3887e+00,
            2.8296e-01,  3.3447e-01],
          [-1.1504e+00, -5.7959e-01, -1.1554e-01,  ...,  2.2742e-01,
           -8.0762e-01,  1.2148e+00],
          [-2.2693e-01, -4.4849e-01,  3.3838e-01,  ...,  3.4521e-01,
           -5.9033e-01,  2.9907e-01]],

         [[ 1.3684e-01, -5.2299e-03,  1.5601e-01,  ..., -2.1240e-02,
            6.7139e-02,  1.6992e-01],
          [ 1.1445e+00, -1.9727e+00,  1.4038e-01,  ..., -3.7231e-01,
            3.4692e-01, -1.3193e+00],
          [ 3.5645e-01, -1.0771e+00, -2.9727e+00,  ..., -1.8096e+00,
           -6.4270e-02,  2.4878e-01],
          ...,
          [-6.9678e-01, -1.0918e+00, -1.4092e+00,  ..., -1.4033e+00,
           -3.7964e-01, -1.5369e-01],
          [-6.2793e-01, -8.2178e-01, -1.6514e+00,  ..., -1.9316e+00,
           -1.1240e+00, -7.8418e-01],
          [ 3.4863e-01, -8.0176e-01, -3.4258e+00,  ..., -1.1680e+00,
           -1.1865e+00, -5.8624e-02]],

         [[-1.3367e-01, -3.5095e-02, -9.0637e-02,  ...,  7.5073e-02,
           -1.9012e-02,  1.6833e-01],
          [-2.6172e+00, -4.2090e-01,  1.3398e+00,  ...,  5.8691e-01,
            1.4170e+00, -1.1191e+00],
          [-1.3545e+00, -1.0321e-01, -1.9951e+00,  ...,  2.7573e-02,
            1.1859e-01, -7.2815e-02],
          ...,
          [ 1.5234e+00,  2.6733e-01,  1.1973e+00,  ...,  6.4600e-01,
           -1.1191e+00, -1.0547e+00],
          [-1.4297e+00,  1.1396e+00,  1.1221e+00,  ..., -7.2168e-01,
           -1.4980e+00,  2.3950e-01],
          [-3.7744e-01,  2.9907e-02,  6.5381e-01,  ..., -9.9072e-01,
           -8.5645e-01, -2.8750e+00]],

         ...,

         [[-3.4454e-02, -9.0942e-02, -3.4106e-01,  ...,  2.7563e-01,
            5.0684e-01,  1.8848e-01],
          [-5.9619e-01,  7.2168e-01, -2.4160e+00,  ..., -3.8301e+00,
           -1.4609e+00, -8.2275e-01],
          [ 1.2073e-01, -2.9102e-01, -2.1465e+00,  ..., -3.0039e+00,
           -2.0586e+00, -1.4404e+00],
          ...,
          [-3.3142e-02, -3.9819e-01, -1.3838e+00,  ..., -6.1133e-01,
           -4.0576e-01,  1.0781e+00],
          [-1.0818e-02, -6.2549e-01, -1.5869e+00,  ..., -2.3105e+00,
            3.8330e-02, -3.0563e-02],
          [-6.0645e-01, -1.5723e+00, -3.7354e-01,  ..., -2.1074e+00,
           -5.9717e-01,  2.0469e+00]],

         [[-6.3232e-02, -1.4392e-01, -3.9258e-01,  ...,  3.7323e-02,
           -5.9961e-01, -8.9722e-02],
          [-2.4463e-01, -3.9111e-01,  1.1875e+00,  ...,  2.2773e+00,
           -2.2715e+00, -1.5117e+00],
          [-1.2549e+00, -1.8906e+00,  2.6562e+00,  ..., -1.8057e+00,
           -2.5508e+00, -5.0732e-01],
          ...,
          [ 5.7227e-01, -4.4281e-02,  7.9980e-01,  ..., -5.7129e-01,
           -1.9287e+00, -3.7832e+00],
          [ 6.0498e-01, -1.5908e+00,  3.4277e-01,  ...,  3.1250e+00,
           -1.6631e+00, -5.4901e-02],
          [-6.4893e-01, -2.6318e-01,  8.9551e-01,  ..., -9.8828e-01,
           -1.1240e+00,  2.5371e+00]],

         [[ 1.0979e-02, -9.5886e-02,  5.3955e-02,  ..., -1.3892e-01,
            1.2335e-01, -4.6417e-02],
          [ 6.4575e-02, -2.1191e-01,  6.2598e-01,  ...,  7.5537e-01,
            4.2407e-01, -2.7070e+00],
          [ 1.3799e+00,  5.2881e-01,  8.9258e-01,  ...,  1.4238e+00,
            1.8623e+00, -1.6494e+00],
          ...,
          [-2.5024e-01,  9.6289e-01,  8.9453e-01,  ...,  6.3232e-02,
            4.8218e-01,  4.8535e-01],
          [ 1.9751e-01, -2.2168e-01,  1.9453e+00,  ...,  1.0928e+00,
           -1.2469e-01, -5.8398e-01],
          [ 5.4492e-01,  2.2430e-02,  1.5264e+00,  ...,  1.7510e+00,
            9.8535e-01, -1.1533e+00]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>), tensor([[[[ 2.5254e+00, -1.5801e+00, -2.1317e-02,  ..., -1.4355e+00,
           -5.6549e-02, -1.1699e+00],
          [ 3.0396e-01, -1.2402e+00, -1.7412e+00,  ..., -2.0984e-01,
            2.1328e+00, -5.7715e-01],
          [ 1.1006e+00, -1.9812e-01, -5.7343e-02,  ...,  3.3765e-01,
            1.2617e+00, -1.2314e+00],
          ...,
          [ 1.3525e+00,  7.3486e-01,  5.0537e-01,  ...,  1.6797e-01,
            2.9932e-01, -8.6182e-01],
          [-5.9131e-01,  2.0430e+00,  1.7930e+00,  ...,  1.0449e+00,
            1.2920e+00, -5.6982e-01],
          [-1.2783e+00,  4.4702e-01, -5.8447e-01,  ..., -4.6143e-01,
            3.4668e-01, -9.6985e-02]],

         [[ 1.2051e+00,  2.7979e-01,  1.8994e+00,  ...,  7.6953e-01,
            1.7444e-01, -1.0654e+00],
          [ 2.8594e+00,  6.6101e-02,  3.1860e-02,  ...,  3.5498e-01,
           -1.6504e-01,  6.5771e-01],
          [ 2.3711e+00,  7.9199e-01,  8.5498e-01,  ..., -7.4805e-01,
           -6.7253e-03,  1.0498e+00],
          ...,
          [ 2.0352e+00,  3.4454e-02,  7.5195e-01,  ..., -1.9316e+00,
           -6.6162e-01,  5.4199e-01],
          [ 9.1113e-01,  2.8848e+00,  3.5889e-01,  ..., -2.6543e+00,
           -8.8916e-01,  4.7656e-01],
          [-3.1982e-01,  7.9053e-01, -7.2070e-01,  ..., -4.7340e-03,
           -5.2490e-01, -7.6025e-01]],

         [[ 1.8047e+00,  9.8584e-01,  7.7002e-01,  ...,  4.9536e-01,
            5.5322e-01, -1.0918e+00],
          [ 3.0273e+00,  1.8848e+00,  2.4570e+00,  ..., -1.7029e-01,
            2.0566e+00,  5.1709e-01],
          [ 9.2334e-01,  1.5596e+00,  1.2676e+00,  ...,  6.2158e-01,
            1.6436e+00, -6.0107e-01],
          ...,
          [ 2.2031e+00,  1.7373e+00,  2.0586e+00,  ..., -1.4172e-01,
            1.7148e+00, -2.1619e-01],
          [ 2.7891e+00,  5.1562e-01,  1.0977e+00,  ...,  3.1860e-01,
            4.0820e-01,  1.9893e+00],
          [-9.9062e+00, -2.1387e+00, -1.3633e+00,  ..., -4.6069e-01,
           -3.6621e-01, -1.3916e+00]],

         ...,

         [[-2.0938e+00, -1.8447e+00,  1.7266e+00,  ..., -1.4343e-01,
            3.0508e+00,  2.3828e+00],
          [-2.4453e+00, -8.4131e-01,  1.9990e+00,  ..., -8.6133e-01,
            2.4316e+00,  5.8057e-01],
          [-1.4355e+00, -3.1885e-01,  2.8262e+00,  ..., -6.3232e-01,
            1.9766e+00,  7.2070e-01],
          ...,
          [-1.7432e-01, -4.7705e-01, -3.6401e-01,  ...,  7.4280e-02,
            1.8877e+00,  2.1509e-01],
          [-1.4072e+00, -4.8584e-01,  5.8984e-01,  ..., -9.0430e-01,
            2.3477e+00,  3.8770e-01],
          [ 3.0396e-01,  1.9604e-01, -3.6133e-01,  ...,  1.7539e+00,
           -9.1357e-01, -1.1113e+00]],

         [[-1.9055e-01, -1.1768e+00, -1.0420e+00,  ...,  1.2920e+00,
            1.3701e+00,  3.5278e-01],
          [ 1.7275e+00, -1.1514e+00, -1.2080e+00,  ...,  8.7158e-01,
           -4.7241e-01, -8.3838e-01],
          [ 1.0742e+00, -8.7842e-01, -1.3730e+00,  ...,  2.0471e-01,
            8.0957e-01,  1.1975e-01],
          ...,
          [ 2.1606e-01, -1.3262e+00, -1.8789e+00,  ..., -1.1611e+00,
            2.2021e-01,  3.6426e-01],
          [ 3.4937e-01, -1.3936e+00, -3.7549e-01,  ...,  1.3242e+00,
           -1.8262e+00,  2.2266e+00],
          [-5.6689e-01,  1.2830e-01,  9.9316e-01,  ..., -6.0638e-02,
           -7.5111e-03,  2.0471e-01]],

         [[-5.6104e-01,  1.4531e+00, -1.5195e+00,  ..., -1.1270e+00,
           -9.8584e-01, -8.4961e-01],
          [-1.3291e+00,  9.6484e-01, -1.7432e-01,  ..., -3.2090e+00,
           -1.0791e+00,  1.0996e+00],
          [-9.4092e-01,  2.3474e-01, -8.1982e-01,  ..., -1.7168e+00,
           -2.7422e+00,  2.4805e-01],
          ...,
          [-9.3555e-01, -2.4170e-01, -3.4302e-01,  ..., -1.8428e+00,
            1.2881e+00, -1.0137e+00],
          [-6.4819e-02,  3.4912e-01, -3.0151e-01,  ..., -2.2539e+00,
            1.3662e+00,  1.2805e-01],
          [ 9.6680e-02, -8.9160e-01,  2.9062e+00,  ...,  1.2117e+01,
           -1.4685e-01,  2.3926e+00]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>), tensor([[[[-1.4170,  0.2720, -2.5078,  ...,  1.7041,  1.9521,  0.6196],
          [ 0.2896,  1.8662, -2.3477,  ...,  0.3274,  1.5088,  0.9619],
          [ 0.8003, -0.0266, -1.4619,  ...,  1.0420,  1.2236, -0.1379],
          ...,
          [-1.6611, -0.3711, -1.4121,  ...,  0.2499,  0.2169,  1.7119],
          [ 0.4209, -0.5010,  2.4746,  ..., -0.2379, -0.0045,  0.4468],
          [ 0.0508, -0.0438, -0.1630,  ..., -0.1379, -0.2883, -0.1844]],

         [[ 1.6113, -0.7998,  0.0885,  ..., -0.4834,  0.7754, -0.7598],
          [ 2.6836,  0.0109,  0.0287,  ..., -0.8647, -0.3516, -0.7275],
          [ 0.7769,  0.5601,  0.7344,  ..., -0.4734,  0.3054, -0.4199],
          ...,
          [ 1.9961,  0.1702,  0.9229,  ..., -2.1699,  0.8335, -1.5537],
          [-0.1046, -0.2551,  1.1094,  ..., -0.4480,  2.1621, -1.4980],
          [ 0.0142, -0.1992, -0.2041,  ...,  0.0148,  0.2644,  0.0578]],

         [[-0.7339,  0.2241,  0.0270,  ...,  2.8262, -0.0699,  0.0535],
          [-1.9082, -0.7241,  1.2617,  ...,  0.9541, -0.8999,  0.3445],
          [-1.4619,  0.8550,  0.6392,  ...,  1.8447, -1.7852, -1.1270],
          ...,
          [ 1.4375,  0.5518, -0.3057,  ...,  2.1973, -0.3660,  0.7446],
          [ 1.2656, -0.0965, -0.4011,  ..., -0.2465,  0.5679,  0.5894],
          [ 0.0198, -0.2998, -0.9663,  ...,  0.1914, -0.1985, -0.0843]],

         ...,

         [[-0.3062, -0.6099, -1.5791,  ..., -0.7290, -0.0350, -1.1475],
          [ 1.9668, -0.2627, -0.6929,  ...,  0.0883, -0.1109,  0.5752],
          [ 0.7329,  1.4570, -0.7925,  ...,  0.4895, -0.0944,  0.7544],
          ...,
          [ 0.8486,  0.8896,  1.6641,  ..., -2.1035, -2.0176, -1.3037],
          [-0.0969, -1.4893,  0.2369,  ..., -0.4360,  0.0964,  0.7100],
          [ 0.3440, -0.0537,  0.1486,  ...,  0.2476,  0.3230,  0.1871]],

         [[-0.1913,  1.2617, -1.2891,  ...,  1.5957, -0.4758,  2.5684],
          [-0.1737,  0.3230, -1.3789,  ...,  0.2515,  0.6396,  0.5352],
          [ 0.0391,  0.8804, -0.7827,  ...,  0.1080, -0.7769,  1.3389],
          ...,
          [-2.3125,  0.0247, -1.5771,  ...,  0.7930, -0.4736,  0.6084],
          [-2.2344,  1.6514,  0.5952,  ...,  0.8867, -1.5195,  0.9702],
          [ 0.2103, -0.2084, -0.3594,  ...,  0.0469, -0.1456, -0.4233]],

         [[ 1.0928, -0.4390,  1.0479,  ..., -0.3804,  0.2310, -0.7930],
          [ 0.4231, -0.4360,  0.6045,  ...,  0.0035, -1.4824,  0.2454],
          [ 0.3379, -0.5957,  0.7026,  ..., -0.3293, -1.0371, -0.6206],
          ...,
          [ 1.0068, -1.5186,  0.3108,  ..., -0.1427, -1.2861, -0.5127],
          [-0.3950, -0.6348,  0.0260,  ...,  0.5996, -0.1439,  0.0838],
          [-0.0948,  0.1782, -0.1569,  ..., -0.0088,  0.0084,  0.0925]]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<TransposeBackward0>)), (tensor([[[[-9.7656e-03,  5.9174e-02,  6.0486e-02,  ...,  2.6001e-01,
           -2.6688e-02,  3.5645e-01],
          [ 1.4365e+00, -2.4353e-01,  1.9592e-02,  ..., -2.5020e+00,
           -1.5859e+00, -1.8030e-01],
          [ 6.4990e-01, -3.5156e-01, -8.9844e-01,  ..., -1.5645e+00,
           -1.2812e+00,  1.7305e+00],
          ...,
          [-8.9844e-01, -1.9736e+00,  5.4004e-01,  ..., -8.9355e-01,
           -1.0459e+00,  1.5879e+00],
          [-1.8970e-01, -1.5264e+00, -3.8086e-01,  ..., -1.0859e+00,
           -1.9365e+00,  1.1465e+00],
          [ 1.6367e+00, -7.0923e-02, -2.5645e+00,  ..., -9.2432e-01,
           -1.0996e+00,  1.3994e+00]],

         [[-2.1326e-01,  6.8970e-02, -1.4307e-01,  ...,  4.0991e-01,
           -1.0583e-01,  2.7954e-01],
          [-1.7471e+00, -1.8008e+00, -1.4268e+00,  ...,  8.4961e-01,
            7.2070e-01, -8.2812e-01],
          [ 3.9795e-02, -2.5757e-01,  1.1426e-01,  ...,  8.6609e-02,
            1.3545e+00, -1.7168e+00],
          ...,
          [ 2.3669e-01, -1.1748e+00, -4.4409e-01,  ..., -2.2791e-01,
            1.3369e+00,  1.2285e+00],
          [ 1.1416e+00, -1.2383e+00, -4.1162e-01,  ..., -1.5312e+00,
            5.0244e-01, -6.6467e-02],
          [-3.4180e-01,  9.5947e-01, -1.0632e-01,  ...,  5.0049e-01,
            5.0146e-01, -9.5215e-01]],

         [[-1.2891e-01, -1.7761e-01, -2.5781e-01,  ...,  1.1353e-02,
            1.2969e+00,  1.4905e-01],
          [ 1.1284e-02, -1.1025e+00,  1.6709e+00,  ...,  8.0859e-01,
           -1.4609e+00,  5.0830e-01],
          [-1.7490e+00,  7.3926e-01,  1.3418e+00,  ...,  6.9141e-01,
           -2.0547e+00,  6.1768e-01],
          ...,
          [-3.9697e-01, -1.4709e-01,  8.7500e-01,  ...,  2.4757e-03,
           -2.5645e+00,  6.7480e-01],
          [-3.1152e-01, -4.9469e-02,  8.4180e-01,  ...,  5.1367e-01,
           -2.8223e+00,  3.0981e-01],
          [-5.0049e-01, -9.0918e-01,  4.6851e-01,  ...,  3.0762e-01,
           -4.2656e+00,  1.0376e-02]],

         ...,

         [[ 7.9590e-02,  1.2871e-02,  5.1086e-02,  ...,  3.6224e-02,
            9.6985e-02,  3.9404e-01],
          [-2.2910e+00, -8.8721e-01, -6.7236e-01,  ...,  3.8794e-01,
            7.4707e-01, -2.0195e+00],
          [-1.9922e+00, -5.5225e-01, -1.4492e+00,  ...,  1.0137e+00,
            3.8281e-01, -6.3574e-01],
          ...,
          [ 6.6895e-01, -1.8984e+00, -1.1875e+00,  ...,  9.8389e-01,
            1.4854e+00, -2.0227e-01],
          [-7.8174e-01, -1.2598e+00, -1.0098e+00,  ...,  7.5049e-01,
            1.0166e+00, -7.4658e-01],
          [-7.4756e-01, -1.1045e+00, -1.5430e+00,  ..., -2.5171e-01,
           -4.1479e-01,  7.2217e-01]],

         [[ 1.7932e-01,  1.7129e+00,  1.1145e-01,  ..., -3.4454e-02,
           -7.7820e-02,  9.3201e-02],
          [ 1.4258e-01, -3.3672e+00, -6.4893e-01,  ..., -1.3176e-02,
            3.1226e-01, -1.0537e+00],
          [-1.4531e+00, -1.9580e+00,  8.6670e-01,  ...,  1.1260e+00,
           -1.8574e+00, -2.9004e+00],
          ...,
          [ 1.3174e+00, -3.0547e+00,  9.9023e-01,  ...,  1.0537e+00,
            1.6611e+00, -9.5654e-01],
          [ 1.1221e+00, -2.3906e+00,  1.1328e+00,  ...,  8.1104e-01,
            1.9893e+00, -1.3945e+00],
          [ 7.6465e-01, -3.4141e+00,  5.4639e-01,  ...,  1.4268e+00,
           -9.1406e-01, -1.1592e+00]],

         [[-3.4424e-01, -1.8600e-02,  4.0698e-01,  ..., -1.8567e-01,
           -2.0044e-01, -2.1561e-02],
          [ 1.7158e+00,  1.1592e+00, -2.5020e+00,  ...,  7.0801e-02,
            1.4131e+00,  2.0566e+00],
          [ 1.5449e+00,  9.4482e-01, -6.0547e-01,  ..., -8.7793e-01,
            1.1758e+00, -1.1371e-01],
          ...,
          [ 3.0103e-01,  6.1035e-01, -2.6895e+00,  ...,  1.4355e+00,
            1.6626e-01,  1.5557e+00],
          [ 1.2832e+00,  9.0283e-01, -7.2070e-01,  ...,  1.0358e-01,
            1.7959e+00,  1.8945e+00],
          [ 7.2083e-02,  1.0234e+00, -1.6611e+00,  ...,  1.7266e+00,
            1.5488e+00, -2.5537e-01]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>), tensor([[[[ 2.5223e-02,  2.4548e-01, -9.2712e-02,  ..., -2.0325e-01,
           -1.7139e-01,  1.0327e-01],
          [-1.4834e+00, -1.2090e+00,  5.6982e-01,  ...,  9.3457e-01,
            6.9434e-01,  1.5029e+00],
          [-6.4014e-01, -1.1094e+00, -1.2122e-01,  ...,  1.1172e+00,
            8.7158e-01,  8.4375e-01],
          ...,
          [-9.6387e-01, -2.0430e+00, -3.1641e+00,  ...,  3.6157e-01,
            2.0801e+00,  2.4533e-04],
          [ 4.1577e-01, -5.1807e-01, -9.9170e-01,  ...,  9.1260e-01,
            1.8096e+00,  2.1411e-01],
          [ 9.3994e-01,  1.9014e+00, -1.6318e+00,  ...,  2.0566e+00,
           -6.3379e-01,  6.3037e-01]],

         [[ 1.8079e-01, -3.4607e-02,  1.1589e-02,  ...,  2.3279e-01,
            3.5725e-03,  1.8053e-03],
          [-8.4277e-01, -1.4814e+00, -2.2278e-01,  ...,  1.2178e+00,
           -8.8721e-01, -7.1191e-01],
          [-1.5234e+00,  2.8770e+00,  1.2976e-01,  ...,  6.6650e-01,
            7.6123e-01,  1.5889e+00],
          ...,
          [-6.8359e-02, -1.8525e+00, -1.2666e+00,  ...,  1.2598e+00,
            1.8633e+00,  7.7051e-01],
          [ 1.5234e+00,  4.7913e-02, -1.8848e+00,  ...,  1.6162e-01,
            2.2148e+00,  5.4053e-01],
          [-1.7100e+00, -2.8398e+00, -1.4600e+00,  ...,  1.5247e-01,
            6.4697e-01,  1.1261e-01]],

         [[-4.7729e-02,  1.2634e-01,  1.6541e-01,  ...,  5.8319e-02,
           -1.5503e-02,  4.6631e-02],
          [-1.7705e+00,  1.3975e+00,  5.9131e-01,  ...,  4.7266e-01,
            8.2080e-01,  1.9746e+00],
          [ 8.5303e-01, -2.3320e+00,  2.0352e+00,  ..., -2.2188e+00,
            1.7324e+00, -6.6455e-01],
          ...,
          [ 1.9785e+00,  2.4146e-01,  1.6846e+00,  ..., -5.5566e-01,
            1.4971e+00,  5.8746e-02],
          [ 1.5381e+00,  9.3262e-02,  1.3662e+00,  ..., -1.6445e+00,
            7.5244e-01,  4.9707e-01],
          [ 2.9614e-01,  2.9932e-01, -3.0098e+00,  ...,  2.1758e+00,
            1.7617e+00, -5.2948e-02]],

         ...,

         [[-7.2510e-02, -1.1255e-01,  2.4414e-02,  ...,  1.9800e-01,
            2.6343e-01,  1.4026e-01],
          [ 1.4795e+00, -1.9502e+00,  4.1602e+00,  ...,  8.4424e-01,
           -1.4570e+00,  1.1699e+00],
          [ 2.5610e-01,  7.3047e-01, -2.6562e+00,  ..., -1.1348e+00,
            1.1191e+00, -2.1309e+00],
          ...,
          [ 9.0820e-01, -7.8320e-01, -2.2422e+00,  ...,  1.1123e+00,
           -1.8711e+00,  4.7578e+00],
          [ 1.1902e-01, -6.7863e-03, -2.0618e-01,  ..., -1.2500e-01,
           -1.3564e+00,  3.3105e+00],
          [ 8.7598e-01, -1.7773e+00,  9.4727e-01,  ..., -1.1299e+00,
           -2.7129e+00,  1.2568e+00]],

         [[ 3.7476e-02,  8.9844e-02, -6.0791e-02,  ...,  1.0303e-01,
           -1.6235e-02, -1.4267e-02],
          [-5.3369e-01, -2.6660e+00,  4.9487e-01,  ..., -1.1169e-01,
            2.9551e+00, -1.2421e-01],
          [ 6.7432e-01, -2.3984e+00, -2.1211e+00,  ...,  2.7420e-02,
           -6.0791e-02, -3.2776e-02],
          ...,
          [ 3.3667e-01, -3.0137e+00,  2.4219e+00,  ..., -6.9043e-01,
            2.2402e+00, -1.3623e+00],
          [ 4.1772e-01, -3.8281e+00, -1.8174e+00,  ..., -1.2793e+00,
            3.2129e+00, -1.4473e+00],
          [-1.0576e+00,  9.9902e-01,  1.7832e+00,  ..., -1.1348e+00,
            2.2090e+00, -1.0872e-02]],

         [[-3.7158e-01,  7.8247e-02, -1.7990e-02,  ...,  2.5366e-01,
           -1.7468e-01,  6.9214e-02],
          [ 6.9031e-02,  2.7930e+00,  2.1155e-01,  ..., -2.9102e+00,
           -8.1445e-01, -3.0859e-01],
          [ 1.3672e+00,  6.6162e-01,  8.8135e-02,  ..., -1.4941e+00,
            5.6836e-01, -3.9624e-01],
          ...,
          [ 3.0254e+00,  5.3174e-01, -3.8525e-01,  ..., -1.8125e+00,
           -3.9746e-01,  7.2754e-01],
          [ 1.3623e+00,  2.3184e+00,  2.0059e+00,  ..., -1.1367e+00,
           -1.3457e+00, -8.9600e-01],
          [ 2.7930e+00,  2.4043e+00,  4.1284e-01,  ..., -7.9443e-01,
           -3.6450e-01, -3.0684e+00]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>), tensor([[[[ 2.6875e+00,  2.9850e-03, -3.0000e+00,  ..., -1.9547e-02,
           -1.2947e-02,  1.4453e+00],
          [ 1.2275e+00,  1.6904e+00, -1.6660e+00,  ...,  6.8555e-01,
           -1.1846e+00, -6.7529e-01],
          [ 1.2090e+00,  1.3672e+00, -1.7969e+00,  ..., -1.4832e-01,
           -8.0273e-01, -3.5083e-01],
          ...,
          [ 9.6631e-01, -1.8574e+00, -6.8115e-02,  ...,  9.3701e-01,
           -1.2588e+00, -4.5654e-01],
          [ 7.0020e-01, -8.1885e-01, -1.0236e-01,  ...,  1.0615e+00,
            7.9492e-01,  4.1064e-01],
          [-1.1377e+00,  8.5254e-01, -3.8354e-01,  ...,  1.5857e-01,
            4.9951e-01, -2.9224e-01]],

         [[ 1.8398e+00, -3.1719e+00, -1.5352e+00,  ..., -1.7700e-01,
           -1.2148e+00,  1.5908e+00],
          [ 1.3486e+00, -2.8164e+00, -2.7930e+00,  ...,  5.2441e-01,
            9.6240e-01,  3.5723e+00],
          [ 1.0098e+00, -2.1797e+00, -1.7217e+00,  ..., -7.7734e-01,
            1.1553e+00,  1.5889e+00],
          ...,
          [ 5.5225e-01, -5.0018e-02,  3.1079e-01,  ..., -9.2529e-01,
            4.7632e-01,  1.3291e+00],
          [ 1.1494e+00, -8.4131e-01, -1.5942e-01,  ..., -5.0098e-01,
           -6.6064e-01, -3.7183e-01],
          [-4.8413e-01, -1.3574e+00, -4.8486e-01,  ...,  1.0703e+00,
           -3.0254e+00, -1.6211e-01]],

         [[ 1.2783e+00, -3.3750e+00,  6.6846e-01,  ...,  2.0078e+00,
           -9.5557e-01,  1.0107e-01],
          [ 7.7686e-01, -3.1172e+00,  2.1545e-01,  ...,  2.4961e+00,
            4.4067e-01,  4.0332e-01],
          [ 1.3213e+00, -5.4805e+00, -5.2197e-01,  ...,  2.3906e+00,
           -6.0938e-01,  1.2812e+00],
          ...,
          [ 1.9165e-02, -2.6660e+00,  2.1406e+00,  ...,  1.3906e+00,
           -1.1035e+00, -8.8928e-02],
          [-1.8726e-01, -1.4375e+00,  5.6519e-02,  ..., -7.3389e-01,
           -8.3398e-01,  2.0918e+00],
          [ 2.7168e+00,  3.8340e+00, -1.2402e+00,  ..., -2.0762e+00,
           -9.9219e-01,  1.4238e+00]],

         ...,

         [[ 5.5225e-01, -1.3867e+00, -1.8965e+00,  ..., -4.0723e-01,
           -1.1064e+00, -1.5146e+00],
          [ 2.2441e+00,  6.1963e-01, -5.0488e-01,  ...,  1.0312e+00,
            9.8193e-01,  1.0029e+00],
          [ 1.8105e+00, -1.0231e-02, -2.1851e-01,  ..., -5.0000e-01,
            7.9541e-01, -5.1318e-01],
          ...,
          [-1.1650e+00,  1.9409e-02,  2.9277e+00,  ..., -4.9854e-01,
           -4.8981e-02, -1.5615e+00],
          [-4.8096e-01,  7.3291e-01,  4.5781e+00,  ..., -1.4951e+00,
           -1.2139e+00, -7.6221e-01],
          [ 1.4694e-02, -1.0312e+00, -3.7378e-01,  ...,  3.6475e-01,
           -2.1973e+00, -2.1143e-01]],

         [[-2.7129e+00,  2.9980e+00,  2.3779e-01,  ...,  6.3782e-03,
           -7.0898e-01, -1.7792e-02],
          [-1.6748e+00,  4.7773e+00,  4.9536e-01,  ...,  1.5469e+00,
            8.0273e-01, -2.2734e+00],
          [-1.5967e+00,  3.8730e+00,  2.6855e-01,  ...,  1.7695e+00,
           -1.4107e-02, -6.2695e-01],
          ...,
          [-5.4492e-01,  1.7383e+00,  8.9264e-03,  ..., -4.1748e-01,
           -2.5820e+00,  7.9053e-01],
          [-1.7686e+00,  1.4923e-02,  2.7515e-01,  ..., -8.1885e-01,
           -2.6523e+00, -1.4004e+00],
          [-7.8278e-03, -4.0869e-01, -1.2422e+00,  ..., -1.2266e+00,
           -2.8857e-01,  1.5259e-01]],

         [[-1.2482e-01, -4.0405e-01, -1.1133e+00,  ...,  4.6118e-01,
           -7.8809e-01,  2.0679e-01],
          [-1.1982e+00, -8.1055e-01, -2.3906e+00,  ..., -2.2864e-01,
           -1.1920e-01,  1.2354e-01],
          [-4.6606e-01,  5.5273e-01, -2.4805e+00,  ...,  3.9355e-01,
           -1.2189e-01,  1.1982e+00],
          ...,
          [ 1.0216e-02,  1.4277e+00, -9.7119e-01,  ...,  6.5771e-01,
           -3.1836e-01, -6.0303e-01],
          [ 1.4854e+00,  1.0303e+00, -2.1836e+00,  ...,  1.7920e+00,
           -7.3389e-01,  8.4033e-01],
          [ 9.0039e-01,  9.7705e-01, -1.6858e-01,  ..., -1.4463e+00,
            1.3574e+00,  7.0410e-01]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>), tensor([[[[ 5.7568e-01, -2.3809e+00, -1.9814e+00,  ..., -1.0498e-01,
           -4.4189e-01,  4.1162e-01],
          [ 1.8984e+00, -5.5469e-01,  4.7192e-01,  ...,  5.7080e-01,
            4.3872e-01,  1.2314e+00],
          [ 5.1514e-01, -1.6797e+00, -5.9863e-01,  ...,  5.1807e-01,
           -1.1895e+00,  7.4268e-01],
          ...,
          [ 1.7773e+00,  1.6833e-01,  1.8152e-01,  ...,  1.6689e+00,
           -1.2783e+00, -1.6382e-01],
          [ 1.5713e+00,  1.6670e+00,  8.8232e-01,  ...,  2.7461e+00,
           -5.9131e-01, -6.1914e-01],
          [-1.3367e-01, -1.0724e-01,  4.8486e-01,  ...,  2.6318e-01,
            6.2073e-02, -3.0029e-02]],

         [[-1.3877e+00,  4.7534e-01,  1.5049e+00,  ...,  1.4722e-01,
            4.3506e-01,  1.2480e+00],
          [-9.8340e-01, -9.1406e-01, -6.3770e-01,  ..., -4.7437e-01,
            4.9686e-04,  6.0645e-01],
          [ 1.0869e+00, -5.5078e-01,  1.5215e+00,  ...,  5.0018e-02,
           -7.9346e-01, -9.3359e-01],
          ...,
          [-1.1406e+00, -5.7556e-02,  1.9414e+00,  ..., -5.5127e-01,
           -1.2129e+00, -2.8763e-02],
          [ 8.3203e-01,  5.1636e-02, -2.6797e+00,  ..., -1.4697e+00,
           -9.2139e-01, -6.3574e-01],
          [-6.1328e-01, -1.2646e-01, -1.2109e-01,  ..., -4.7424e-02,
           -8.9600e-02, -1.8188e-01]],

         [[-8.1006e-01, -4.6411e-01,  2.1436e-01,  ...,  5.3857e-01,
            1.2100e+00,  2.8638e-01],
          [-3.5571e-01, -9.2627e-01,  9.3408e-01,  ...,  4.5752e-01,
            1.0381e+00, -7.8247e-02],
          [-7.0117e-01,  1.0635e-02,  3.7085e-01,  ...,  9.2676e-01,
            1.0117e+00, -4.2896e-01],
          ...,
          [-1.4521e+00, -1.0566e+00, -1.1676e-01,  ..., -4.1357e-01,
           -1.5320e-02, -2.6099e-01],
          [-7.7246e-01, -1.4658e+00,  1.0400e+00,  ...,  2.3364e-01,
            6.8652e-01, -9.1016e-01],
          [ 3.3838e-01, -2.5513e-01, -1.5566e+00,  ...,  2.1172e+00,
           -1.7603e-01,  1.2164e-01]],

         ...,

         [[ 1.0713e+00, -8.5303e-01, -9.4482e-01,  ...,  1.7715e+00,
            7.1240e-01,  2.0664e+00],
          [ 4.2896e-01, -1.2012e+00,  9.0430e-01,  ...,  1.2344e+00,
           -9.5215e-01,  2.1699e+00],
          [ 1.1504e+00, -1.8828e+00,  1.9946e-01,  ...,  3.5571e-01,
            1.0840e+00,  8.3984e-01],
          ...,
          [-3.2910e-01, -1.5225e+00, -3.1934e-01,  ..., -8.0469e-01,
            1.8633e+00,  1.4668e+00],
          [-1.1553e+00, -1.2773e+00, -1.5381e-01,  ..., -2.9551e+00,
            1.3945e+00, -7.5098e-01],
          [-6.3629e-03, -2.0605e-01,  1.6464e-02,  ..., -1.6190e-02,
           -3.4424e-02, -3.0908e-01]],

         [[-1.5312e+00, -1.6199e-01, -5.0928e-01,  ...,  2.9565e-01,
            1.4453e+00, -8.9453e-01],
          [ 9.8633e-02,  2.3926e+00, -1.4434e+00,  ...,  9.7900e-01,
            3.1094e+00, -1.2539e+00],
          [ 1.8164e-01,  1.9404e+00, -2.6779e-02,  ..., -1.2910e+00,
            1.2344e+00, -1.9990e+00],
          ...,
          [-7.4121e-01, -1.1807e+00,  1.2119e+00,  ...,  3.8525e-01,
           -8.7061e-01, -1.2646e+00],
          [-3.9453e-01, -8.3313e-02,  8.7451e-01,  ..., -1.3049e-01,
            1.7031e+00,  1.7505e-01],
          [-2.6147e-01,  1.1328e-01,  2.3450e-01,  ..., -2.0215e-01,
           -1.6675e-01,  2.5586e-01]],

         [[-1.7871e+00, -9.9951e-01,  5.0244e-01,  ..., -3.7346e-03,
           -1.8787e-01, -6.0107e-01],
          [ 3.8281e-01, -2.1816e+00,  4.5288e-01,  ..., -1.5059e+00,
           -9.1699e-01, -3.6011e-02],
          [-3.0713e-01, -1.4102e+00, -1.9275e-01,  ..., -2.4280e-01,
           -1.2915e-01, -5.6836e-01],
          ...,
          [-1.0332e+00, -2.6929e-01,  1.9961e+00,  ...,  2.4072e-01,
            2.1411e-01, -2.2031e+00],
          [ 1.6943e-01, -1.3555e+00, -2.6602e+00,  ...,  3.9062e-01,
           -5.3520e-03, -7.8955e-01],
          [ 6.3574e-01,  1.4709e-01, -7.1484e-01,  ...,  3.7476e-01,
            4.4824e-01,  1.1438e-01]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>)), (tensor([[[[ 3.4375e-01,  2.3254e-01,  9.4385e-01,  ..., -7.2705e-01,
            7.0679e-02,  8.5254e-01],
          [ 2.4377e-01,  1.4465e-01, -1.1318e+00,  ...,  2.4297e+00,
           -1.7510e+00, -1.6543e+00],
          [-1.6729e+00, -2.5469e+00, -4.2188e-01,  ...,  1.0762e+00,
           -8.8477e-01, -2.7207e+00],
          ...,
          [ 9.6143e-01,  1.7168e+00, -2.6484e+00,  ...,  2.4453e+00,
           -2.9888e-03, -3.3936e-01],
          [ 7.8076e-01,  9.2871e-01, -1.9316e+00,  ...,  1.0605e+00,
           -6.1133e-01, -1.9141e+00],
          [-2.0820e+00, -1.2764e+00, -5.7373e-01,  ...,  2.1152e+00,
           -2.7461e+00,  2.9956e-01]],

         [[ 1.4185e-01,  9.6777e-01,  8.4961e-02,  ..., -1.2067e-01,
            2.6196e-01,  1.2280e-01],
          [-4.4287e-01, -2.1753e-01,  1.1123e+00,  ..., -2.0957e+00,
           -2.2263e-02,  7.3438e-01],
          [ 6.2793e-01, -1.0371e+00, -1.6492e-01,  ..., -1.1553e+00,
            1.3154e+00,  2.0098e+00],
          ...,
          [ 9.5337e-02, -2.0801e+00,  6.2256e-01,  ..., -1.4834e+00,
           -2.0911e-01, -1.3994e+00],
          [-1.0638e-01, -1.9014e+00,  1.0791e+00,  ..., -2.5098e+00,
           -2.2998e-01, -1.5845e-01],
          [ 2.3010e-01, -2.1934e+00, -1.4514e-01,  ..., -3.2178e-01,
            9.5605e-01,  1.5771e+00]],

         [[-1.2854e-01,  2.4500e-01,  1.2964e-01,  ...,  5.5786e-02,
            6.7139e-02, -1.5430e-01],
          [-7.8467e-01,  1.0596e+00, -2.3750e+00,  ...,  2.7588e-01,
           -8.2324e-01, -4.4824e-01],
          [ 5.2979e-01,  1.1836e+00, -2.6719e+00,  ...,  6.5552e-02,
           -1.0068e+00,  5.6152e-02],
          ...,
          [-2.4207e-01,  5.3027e-01, -5.5195e+00,  ..., -2.9028e-01,
            2.1267e-03,  1.3086e+00],
          [ 1.3049e-01,  4.6216e-01, -4.2461e+00,  ...,  6.2695e-01,
           -3.8159e-01,  1.4883e+00],
          [-2.4963e-01, -6.0254e-01, -3.6270e+00,  ..., -1.2090e+00,
            1.0625e+00,  7.0020e-01]],

         ...,

         [[ 3.0396e-01,  7.7576e-02,  8.9160e-01,  ...,  1.3379e-01,
           -2.5537e-01,  3.2715e-02],
          [ 4.9536e-01, -2.1055e+00, -1.2490e+00,  ..., -4.7211e-02,
            1.0195e+00,  3.0371e-01],
          [ 1.5830e+00, -1.4600e+00, -6.2988e-01,  ..., -2.9541e-01,
            5.8643e-01,  1.0098e+00],
          ...,
          [ 1.3691e+00, -4.3433e-01, -3.0527e+00,  ...,  8.8477e-01,
            2.3379e+00,  8.1238e-02],
          [ 1.0820e+00, -1.1338e+00, -3.8281e+00,  ...,  4.9927e-01,
            1.2754e+00,  2.0605e-01],
          [ 1.9912e+00, -1.4268e+00, -3.6621e+00,  ...,  2.6250e+00,
            1.7510e+00,  2.9492e-01]],

         [[ 8.7341e-02,  2.0645e+00,  1.4026e-01,  ..., -1.2866e-01,
            8.0933e-02,  8.8135e-02],
          [ 2.0391e+00, -4.3867e+00, -2.6504e+00,  ...,  6.6602e-01,
           -6.4453e-01, -1.4277e+00],
          [ 3.9014e-01, -3.0273e+00, -1.2744e+00,  ...,  2.0081e-02,
           -2.7686e-01, -1.8193e+00],
          ...,
          [ 1.0217e-01, -5.7734e+00,  7.1411e-02,  ...,  7.0508e-01,
           -1.3848e+00, -1.0693e+00],
          [ 1.1113e+00, -5.4844e+00, -3.5938e-01,  ...,  9.5312e-01,
            3.5449e-01, -1.9873e+00],
          [-8.7051e-03, -4.8320e+00, -5.5713e-01,  ...,  1.5068e+00,
           -7.1582e-01,  5.6689e-01]],

         [[ 1.2524e-01,  3.2177e-03,  2.7573e-02,  ...,  4.1077e-02,
           -2.3087e-02,  1.3542e-02],
          [-8.7280e-02,  3.1812e-01, -3.3325e-01,  ..., -1.3672e+00,
           -2.3071e-01, -3.7427e-01],
          [-1.1055e+00, -2.3425e-01,  9.3445e-02,  ..., -1.3389e+00,
            2.3477e+00, -1.8481e-01],
          ...,
          [-6.8457e-01, -1.6865e+00,  1.9668e+00,  ..., -1.0625e+00,
           -2.6152e+00, -2.5073e-01],
          [-1.1699e+00, -2.6367e+00,  3.0054e-01,  ..., -4.4727e-01,
           -2.4922e+00,  1.3989e-01],
          [-2.2715e+00, -1.2324e+00, -1.1035e-01,  ..., -2.5415e-01,
            8.1055e-01, -1.3955e+00]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>), tensor([[[[-1.6760e-01, -1.4392e-01,  1.3580e-02,  ..., -8.1406e-03,
            5.7739e-02,  6.3171e-03],
          [ 1.1445e+00,  1.7070e+00,  1.8408e+00,  ...,  8.5254e-01,
           -1.3936e+00, -9.2236e-01],
          [ 1.2920e+00, -1.1152e+00, -1.5342e+00,  ...,  1.5801e+00,
           -1.2383e+00,  7.8223e-01],
          ...,
          [ 2.6699e+00, -6.6943e-01,  3.8164e+00,  ...,  6.1475e-01,
           -1.4229e+00, -2.2773e+00],
          [ 2.0645e+00,  5.4053e-01,  9.5947e-01,  ...,  3.0176e-01,
           -1.4121e+00, -1.0957e+00],
          [ 4.9103e-02, -7.9688e-01,  1.6729e+00,  ..., -2.3574e+00,
           -8.3447e-01,  2.0764e-01]],

         [[ 2.1765e-01,  8.9355e-02,  4.7821e-02,  ...,  1.2549e-01,
           -3.6224e-02, -1.2091e-01],
          [ 1.1810e-01, -2.0918e+00,  4.6411e-01,  ..., -2.0117e+00,
            1.9893e+00,  1.0293e+00],
          [ 1.3330e+00,  3.5889e-01,  8.8916e-01,  ..., -6.0645e-01,
           -2.8242e+00, -1.5350e-02],
          ...,
          [-5.8008e-01, -5.3857e-01,  1.7004e-01,  ..., -1.7461e+00,
           -5.9521e-01,  4.4116e-01],
          [-4.2773e-01, -1.3489e-02,  8.4045e-02,  ..., -4.8242e-01,
            5.4395e-01, -5.5371e-01],
          [-4.3896e-01, -4.9133e-02, -2.0996e+00,  ..., -2.1660e+00,
           -8.2812e-01,  1.9727e+00]],

         [[-6.3232e-02, -7.3486e-02,  1.4795e-01,  ...,  2.2510e-01,
            4.9652e-02,  5.0079e-02],
          [ 1.1670e+00,  7.1436e-01, -1.4883e+00,  ...,  1.9453e+00,
            2.7905e-01, -1.5312e+00],
          [-8.2275e-01,  1.9800e-01, -2.0996e+00,  ...,  4.3633e+00,
            1.0967e+00,  1.3936e+00],
          ...,
          [ 6.6162e-01,  8.1787e-01,  1.2979e+00,  ...,  9.3701e-01,
           -7.6904e-01, -5.9912e-01],
          [ 1.1318e+00,  1.3506e+00,  8.0078e-01,  ...,  7.5342e-01,
           -9.3945e-01, -1.0322e+00],
          [-5.2637e-01, -6.2256e-01,  5.8301e-01,  ..., -7.7539e-01,
            1.0068e+00, -3.2363e+00]],

         ...,

         [[ 8.5938e-02, -1.9580e-01, -4.6753e-02,  ..., -9.9731e-02,
           -3.8483e-02,  2.1835e-02],
          [ 6.8604e-01, -8.3691e-01, -1.4424e+00,  ..., -1.5928e+00,
           -2.0762e+00,  2.1719e+00],
          [-1.3760e+00,  2.3496e+00, -3.3691e-02,  ...,  1.2393e+00,
            1.0078e+00, -1.7297e-01],
          ...,
          [-7.2021e-01,  2.2285e+00, -2.3633e+00,  ..., -2.0391e+00,
           -4.8999e-01,  3.3496e-01],
          [-6.9678e-01,  1.9873e+00, -2.6738e+00,  ..., -1.5693e+00,
           -4.6265e-01,  1.2148e+00],
          [-1.3513e-01,  4.8047e+00, -2.5020e+00,  ...,  4.8633e-01,
            2.4824e+00,  1.9512e+00]],

         [[-2.6810e-02, -2.0520e-01,  1.0651e-01,  ...,  4.6814e-02,
            1.0718e-01, -2.0325e-02],
          [ 3.2422e-01,  9.9805e-01, -4.8511e-01,  ..., -2.6582e+00,
           -5.9033e-01, -9.2285e-02],
          [-9.9023e-01, -2.0664e+00,  5.1465e-01,  ...,  1.3613e+00,
           -1.8477e+00, -9.7461e-01],
          ...,
          [ 3.5132e-01,  6.8506e-01,  4.3335e-01,  ..., -2.3218e-01,
           -1.4570e+00, -7.2266e-01],
          [-4.4434e-01, -1.8494e-01, -1.4805e+00,  ...,  5.8887e-01,
            2.6489e-01, -2.0293e+00],
          [-1.3467e+00, -2.9517e-01,  1.5322e+00,  ..., -1.9082e+00,
            1.1357e+00, -1.9248e+00]],

         [[ 4.0649e-01,  5.3589e-02,  7.0557e-02,  ...,  9.4223e-03,
           -1.2866e-01, -9.2087e-03],
          [-3.0151e-01,  8.3350e-01,  8.8379e-01,  ...,  1.2927e-01,
            1.5762e+00,  3.1958e-01],
          [-1.6797e+00, -6.3770e-01,  1.1260e+00,  ...,  6.8701e-01,
           -8.3154e-01,  1.6641e+00],
          ...,
          [-7.6660e-02, -8.2617e-01,  8.9258e-01,  ...,  1.8799e+00,
            9.7363e-01,  6.2500e-01],
          [-1.5898e+00,  4.9438e-03, -2.7734e-01,  ...,  1.0869e+00,
            2.0227e-01,  6.3916e-01],
          [-3.0332e+00,  1.9600e+00,  4.8022e-01,  ..., -9.9915e-02,
            7.0264e-01,  3.9244e-04]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>), tensor([[[[-1.5508e+00, -1.6162e+00, -1.9316e+00,  ..., -2.3477e+00,
           -2.5220e-01, -8.4229e-01],
          [-2.1406e+00, -2.8066e+00, -1.3682e+00,  ..., -8.0322e-01,
           -2.1094e+00, -3.0029e-01],
          [-1.0400e+00, -2.9688e+00, -9.4873e-01,  ..., -2.8896e-03,
           -5.2246e-01, -1.2274e-01],
          ...,
          [-1.3242e+00, -8.0615e-01, -2.0422e-01,  ..., -4.3481e-01,
            1.4131e+00, -1.3486e+00],
          [-9.6143e-01, -1.6416e+00, -9.3115e-01,  ...,  9.0820e-01,
            4.6021e-01, -7.0496e-02],
          [ 8.0234e+00,  4.2188e-01,  9.6497e-02,  ...,  4.3213e-01,
           -4.7095e-01,  6.6162e-01]],

         [[-3.9722e-01,  1.0127e+00,  4.2920e-01,  ...,  6.4795e-01,
           -6.9043e-01, -1.1455e+00],
          [ 1.0938e+00, -2.0469e+00,  1.2451e+00,  ..., -2.2571e-01,
           -3.4985e-01, -7.5732e-01],
          [ 6.6943e-01,  3.5913e-01,  4.1748e-01,  ..., -4.6196e-03,
           -1.6543e+00, -9.7266e-01],
          ...,
          [-7.3535e-01, -4.4287e-01,  2.4023e+00,  ..., -5.6006e-01,
           -2.0508e+00, -2.9126e-01],
          [-5.0195e-01,  1.3525e-01, -3.6182e-01,  ..., -1.1885e+00,
           -9.7607e-01, -1.5371e+00],
          [-2.9346e-01,  4.1953e+00, -4.1821e-01,  ...,  1.1829e-01,
            5.9717e-01,  6.5771e-01]],

         [[ 1.6045e+00,  1.7834e-01,  1.3306e-01,  ..., -4.7217e-01,
           -4.7998e-01, -1.0732e+00],
          [ 5.1123e-01, -5.0781e-01,  1.0977e+00,  ..., -2.8467e-01,
           -2.0625e+00, -7.0459e-01],
          [ 1.1973e+00,  2.5537e-01,  8.6182e-01,  ..., -1.1542e-01,
           -2.8711e+00, -5.7324e-01],
          ...,
          [ 1.4600e+00,  4.4360e-01,  8.9209e-01,  ...,  7.2900e-01,
            4.5142e-01, -7.5391e-01],
          [ 1.1191e+00, -1.8320e+00, -1.6475e+00,  ...,  8.0322e-01,
            4.6899e-01,  9.7595e-02],
          [ 1.5117e+00,  2.6596e-02,  6.2305e-01,  ...,  3.8770e-01,
           -2.8320e-01,  1.7017e-01]],

         ...,

         [[-1.3525e+00, -3.8354e-01, -1.6172e+00,  ...,  2.5803e-02,
            3.3828e+00, -1.0791e+00],
          [-6.1377e-01, -3.9961e+00,  2.0447e-02,  ...,  8.8916e-01,
            2.5508e+00,  1.3159e-01],
          [-2.7515e-01, -2.7285e+00, -6.1584e-02,  ...,  1.7021e+00,
            3.3359e+00,  2.1016e+00],
          ...,
          [-2.3457e+00, -2.9694e-02,  6.6162e-02,  ...,  2.8052e-01,
            1.1914e-01, -2.1814e-01],
          [-9.2725e-01, -2.2285e+00,  6.8018e-01,  ..., -5.0879e-01,
           -5.4053e-01, -6.8799e-01],
          [-6.0400e-01,  6.0078e+00,  5.3418e-01,  ...,  3.5913e-01,
           -2.2109e+00, -5.8545e-01]],

         [[ 2.1741e-01,  1.6377e+00, -9.4666e-02,  ...,  8.2715e-01,
            8.8477e-01, -5.2588e-01],
          [ 1.8379e+00,  1.0576e+00,  2.3574e+00,  ..., -1.6094e+00,
            9.8877e-01,  1.3164e+00],
          [ 1.2812e+00,  4.9365e-01,  1.2842e+00,  ...,  9.3262e-02,
           -1.1035e+00, -2.4756e-01],
          ...,
          [ 1.6953e+00,  1.0083e-01,  7.6123e-01,  ..., -1.8506e-01,
           -7.6855e-01,  9.8584e-01],
          [-3.9502e-01, -8.7585e-02,  2.2852e+00,  ..., -1.5264e+00,
           -8.7952e-02,  9.9072e-01],
          [ 1.1572e+00,  4.3564e-03, -1.1611e+00,  ...,  1.7041e+00,
            1.3145e+00, -1.1035e+00]],

         [[ 9.4482e-01, -4.1992e-01, -2.1406e+00,  ..., -2.1562e+00,
           -2.1348e+00,  6.4209e-01],
          [ 8.4473e-01, -4.4141e-01, -2.0508e+00,  ...,  4.5239e-01,
           -5.1514e-01, -1.5615e+00],
          [-1.6284e-01,  6.8066e-01, -3.8330e-01,  ...,  7.6221e-01,
           -6.9580e-01, -1.9072e+00],
          ...,
          [ 1.9082e+00, -1.0361e+00,  2.4727e+00,  ...,  1.1006e+00,
            1.5781e+00, -3.0078e+00],
          [ 1.2314e+00, -4.0820e-01,  8.2812e-01,  ...,  7.6318e-01,
            1.6250e+00, -2.1699e+00],
          [ 7.2217e-01,  4.3335e-01,  1.7441e+00,  ...,  1.3660e-01,
            1.2100e+00,  4.2188e-01]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>), tensor([[[[ 1.7891e+00,  1.5613e-01, -2.6406e+00,  ...,  1.6260e+00,
           -9.0430e-01,  1.9912e+00],
          [ 1.6641e+00,  7.3389e-01, -3.3008e+00,  ...,  3.2178e-01,
            6.0840e-01,  5.9021e-02],
          [-2.8906e-01,  1.6855e+00, -1.3076e+00,  ...,  5.1807e-01,
            1.9417e-03,  4.7266e-01],
          ...,
          [ 4.5190e-01,  2.8242e+00, -6.0205e-01,  ..., -1.5557e+00,
           -8.5059e-01, -8.6914e-01],
          [ 8.8135e-01,  3.2275e-01,  2.2383e+00,  ..., -2.6523e+00,
           -2.5469e+00,  3.4229e-01],
          [ 1.2250e-01, -4.3384e-01, -2.7466e-01,  ...,  3.0322e-01,
            3.1079e-01, -2.9102e-01]],

         [[ 8.7988e-01, -2.4102e+00, -1.2207e+00,  ...,  2.3523e-01,
           -2.8867e+00,  1.2900e+00],
          [-1.0791e+00, -2.9102e+00,  9.6045e-01,  ...,  3.9697e-01,
           -3.3359e+00,  1.1133e+00],
          [ 1.5918e-01, -2.3828e+00, -4.5654e-01,  ...,  1.1738e+00,
           -1.0576e+00,  5.8789e-01],
          ...,
          [ 3.4595e-01, -1.9277e+00, -1.3989e-01,  ..., -5.4248e-01,
           -5.4541e-01,  5.2441e-01],
          [ 6.5967e-01,  1.7842e+00, -1.5796e-01,  ...,  1.1113e+00,
            5.7227e-01, -3.9795e-01],
          [-2.9297e-01,  7.0374e-02, -7.3975e-02,  ..., -2.8000e-02,
            4.1943e-01,  2.3975e-01]],

         [[ 1.6318e+00, -2.2734e+00, -7.9102e-01,  ..., -1.2314e+00,
           -3.0371e-01, -3.6719e-01],
          [-1.6484e+00, -2.1250e+00, -2.3301e+00,  ..., -5.7520e-01,
            2.3262e+00, -2.0859e+00],
          [-5.7227e-01, -1.0039e+00, -3.1406e+00,  ...,  4.6997e-01,
            6.4111e-01, -4.3281e+00],
          ...,
          [ 3.4790e-01,  1.0068e+00,  9.0430e-01,  ..., -1.7402e+00,
           -1.1309e+00, -1.5273e+00],
          [ 2.7759e-01, -2.6680e+00,  3.5718e-01,  ...,  2.6587e-01,
           -1.9055e-01, -1.0234e+00],
          [-4.2328e-02,  5.0476e-02,  2.8488e-02,  ...,  3.7207e-01,
           -2.9126e-01, -2.2864e-01]],

         ...,

         [[ 2.7910e+00,  2.2344e+00, -1.8770e+00,  ...,  2.9434e+00,
           -1.0420e+00, -5.8301e-01],
          [ 2.4863e+00,  9.4141e-01, -1.9014e+00,  ...,  2.4199e+00,
            3.9038e-01, -5.2216e-02],
          [ 1.3174e+00,  8.9648e-01, -4.2261e-01,  ...,  1.6328e+00,
           -1.1816e+00, -3.3539e-02],
          ...,
          [ 2.0781e+00,  4.2676e-01, -1.3203e+00,  ...,  1.0752e+00,
           -4.9316e-01, -8.1445e-01],
          [ 9.0918e-01, -1.2891e+00, -1.9067e-01,  ...,  4.0186e-01,
           -1.4736e+00, -3.1641e-01],
          [ 1.8286e-01,  1.7456e-01, -4.6265e-01,  ...,  5.6335e-02,
           -1.8616e-01,  2.5342e-01]],

         [[ 1.6016e+00, -2.4727e+00,  1.2471e+00,  ...,  7.8809e-01,
            1.2109e+00,  1.0859e+00],
          [ 1.4902e+00, -1.5439e+00,  8.1592e-01,  ...,  7.3730e-01,
           -8.5938e-01,  1.4180e+00],
          [ 2.1055e+00, -1.7542e-01,  7.6074e-01,  ...,  4.1211e-01,
           -1.9443e+00,  9.5825e-02],
          ...,
          [ 7.9443e-01,  1.7520e+00,  8.5840e-01,  ...,  1.0410e+00,
           -2.3340e-01, -6.6016e-01],
          [ 2.2891e+00,  7.2852e-01, -9.8816e-02,  ..., -4.1992e-01,
           -9.4580e-01,  1.8223e+00],
          [-1.7993e-01, -4.1064e-01, -5.2881e-01,  ..., -1.4734e-01,
           -3.9673e-01, -2.2998e-01]],

         [[-4.5703e-01, -2.6318e-01, -2.0273e+00,  ..., -1.2412e+00,
           -1.5469e+00, -1.6719e+00],
          [-3.1934e-01,  2.4492e+00, -6.3330e-01,  ...,  3.2788e-01,
            4.0991e-01, -5.6641e-01],
          [-5.2734e-01,  6.8550e-03, -3.2695e+00,  ..., -5.9814e-01,
           -1.1387e+00,  1.8188e-01],
          ...,
          [-6.6406e-01,  3.4399e-01, -2.4746e+00,  ..., -7.1777e-01,
            1.1250e+00, -2.0234e+00],
          [-2.0820e+00,  1.1455e+00, -9.4287e-01,  ...,  1.0127e+00,
           -4.7455e-02,  6.4111e-01],
          [-5.8789e-01, -7.4170e-01,  1.6321e-01,  ..., -2.1716e-01,
            3.8623e-01,  5.1855e-01]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>)), (tensor([[[[ 2.7368e-01, -2.4573e-01, -1.2708e-01,  ..., -3.2135e-02,
           -1.8958e-01,  8.7891e-02],
          [-1.1914e+00,  8.0859e-01,  1.7207e+00,  ..., -5.0146e-01,
            7.9053e-01,  8.9941e-01],
          [-1.9990e+00,  2.7734e+00,  5.4590e-01,  ..., -2.1423e-01,
           -1.0193e-01, -1.6284e-01],
          ...,
          [ 5.0049e-01, -8.8281e-01,  2.3328e-01,  ...,  3.4375e-01,
           -5.8496e-01,  9.6826e-01],
          [ 4.7314e-01,  1.3635e-01,  3.6450e-01,  ..., -3.1082e-02,
            1.1738e+00,  1.2030e-01],
          [-1.7031e+00, -5.2295e-01,  9.3994e-01,  ..., -3.7598e-01,
           -7.2266e-01, -5.3955e-01]],

         [[ 6.0974e-02,  3.0420e-01, -1.2512e-01,  ..., -2.3047e-01,
           -9.8206e-02, -3.8788e-02],
          [-1.3203e+00, -2.2986e-01,  1.7266e+00,  ..., -9.0332e-01,
            6.3232e-01, -1.2402e-01],
          [-7.2705e-01, -7.0605e-01,  8.2568e-01,  ...,  1.9756e+00,
           -5.5664e-01, -6.9702e-02],
          ...,
          [ 3.3936e-02, -4.1528e-01,  5.8447e-01,  ..., -9.6094e-01,
            1.4248e+00, -1.0771e+00],
          [ 6.5346e-03, -3.1421e-01,  1.3174e+00,  ..., -3.8477e-01,
            7.9980e-01, -4.5093e-01],
          [-2.3767e-01, -2.4890e-01,  1.9697e+00,  ...,  1.4758e-01,
            1.3604e+00,  6.3330e-01]],

         [[ 3.2178e-01, -7.1350e-02,  2.0401e-02,  ..., -3.1891e-02,
            4.1919e-01,  3.0981e-01],
          [ 6.2744e-01, -4.8462e-01,  7.2510e-01,  ...,  2.3281e+00,
           -1.3369e+00, -1.9995e-01],
          [-3.4424e-01,  1.8193e+00,  1.0508e+00,  ...,  1.7441e+00,
           -6.3379e-01, -2.0410e-01],
          ...,
          [ 2.5757e-01,  1.7803e+00,  5.2002e-01,  ...,  1.3535e+00,
            1.2238e-01, -4.7241e-02],
          [ 1.7842e+00,  7.1680e-01,  1.9961e+00,  ...,  1.9619e+00,
           -1.1270e+00, -6.7627e-01],
          [-1.2732e-01,  7.4121e-01, -1.2207e+00,  ...,  2.5410e+00,
            2.0105e-01, -3.0781e+00]],

         ...,

         [[ 1.2195e-01, -2.0325e-01,  9.4116e-02,  ...,  1.5710e-01,
            5.2002e-01,  2.3087e-02],
          [-4.5288e-01, -2.2715e+00, -9.9609e-01,  ..., -7.0312e-01,
           -7.9688e-01, -9.5654e-01],
          [-3.0410e+00, -3.9233e-01,  9.4482e-01,  ..., -2.7266e+00,
           -3.0054e-01, -1.9031e-01],
          ...,
          [-2.9766e+00, -1.3779e+00,  1.0459e+00,  ..., -1.7715e+00,
           -1.7598e+00,  1.2188e+00],
          [-3.0996e+00, -1.8906e+00,  1.0195e+00,  ..., -1.9346e+00,
           -2.5195e+00,  5.1562e-01],
          [-1.6318e+00, -2.7002e-01,  6.6113e-01,  ..., -2.9248e-01,
           -9.6680e-01,  1.3623e+00]],

         [[-6.7322e-02,  3.1689e-01,  5.7220e-04,  ..., -9.1858e-02,
           -1.4197e-01,  7.1777e-02],
          [-6.1816e-01, -2.2422e+00, -2.1509e-01,  ..., -9.6664e-03,
            2.1895e+00, -9.2236e-01],
          [-2.0957e+00, -2.6523e+00,  3.2373e-01,  ..., -1.4648e+00,
            6.4404e-01, -2.6758e-01],
          ...,
          [-1.8105e+00, -3.4644e-01,  2.8613e-01,  ..., -1.0352e+00,
            1.2754e+00, -3.2495e-01],
          [-1.0020e+00, -1.3896e+00, -4.1504e-01,  ..., -5.1318e-01,
            1.2178e+00,  2.7856e-01],
          [-1.6089e-01, -1.7998e+00, -1.8770e+00,  ...,  6.6064e-01,
            1.4992e-02,  1.0068e+00]],

         [[ 8.4412e-02, -3.9185e-02,  3.5309e-02,  ..., -1.0400e-01,
            2.3071e-01,  7.3303e-02],
          [-1.2412e+00, -8.7793e-01,  2.1225e-02,  ..., -1.1938e-01,
           -9.2090e-01,  1.2676e+00],
          [-4.9927e-01, -3.9526e-01,  8.3691e-01,  ...,  1.3135e+00,
           -3.3179e-01,  3.7915e-01],
          ...,
          [-2.5625e+00, -1.4482e+00, -9.9902e-01,  ..., -5.3027e-01,
           -1.9562e-02,  1.0869e+00],
          [-1.0762e+00, -2.3457e+00, -2.4473e+00,  ..., -2.9565e-01,
           -7.0068e-01,  1.1713e-01],
          [-9.6289e-01, -2.0625e+00,  8.2642e-02,  ..., -6.3965e-01,
            6.5186e-01,  2.5732e-01]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>), tensor([[[[-1.7059e-02, -1.8433e-01, -1.0687e-01,  ..., -9.3384e-02,
           -8.9172e-02,  1.6632e-02],
          [ 1.4473e+00,  1.0518e+00, -2.2681e-01,  ...,  6.1719e-01,
            1.5303e+00, -4.6021e-01],
          [ 4.4844e+00, -2.5176e+00, -1.6123e+00,  ..., -7.0801e-01,
            3.8159e-01,  3.5820e+00],
          ...,
          [-4.0088e-01,  5.8252e-01, -4.6448e-02,  ..., -1.1475e+00,
            9.9756e-01,  9.4189e-01],
          [-6.5869e-01,  5.5273e-01, -1.1943e+00,  ..., -1.7725e+00,
            1.8672e+00,  7.7197e-01],
          [ 1.9941e+00,  1.5244e+00,  6.2451e-01,  ..., -2.2891e+00,
           -3.9215e-02,  4.7314e-01]],

         [[-2.3828e-01, -2.8137e-02,  2.3575e-02,  ...,  4.3762e-02,
            3.7498e-03,  4.2084e-02],
          [ 2.6875e+00,  8.5498e-01, -1.1543e+00,  ...,  1.6230e+00,
           -9.2383e-01,  5.9229e-01],
          [ 3.7852e+00, -1.1816e-01,  1.8467e+00,  ...,  3.9941e+00,
           -1.3506e+00, -2.1074e+00],
          ...,
          [ 3.9316e+00,  3.3643e-01,  4.8926e-01,  ...,  2.4170e-01,
           -1.2227e+00, -7.8223e-01],
          [ 4.5039e+00,  1.2471e+00,  6.4941e-01,  ...,  3.0469e+00,
           -9.4147e-03, -1.3340e+00],
          [ 3.6230e-01,  8.2324e-01, -9.1602e-01,  ..., -5.0928e-01,
           -7.6611e-01,  8.8281e-01]],

         [[-6.0539e-03,  1.2671e-01,  5.6854e-02,  ..., -1.4465e-01,
            1.3464e-01, -9.2285e-02],
          [ 9.9060e-02, -2.9736e-03, -5.5713e-01,  ...,  4.4873e-01,
            4.1064e-01,  1.8164e+00],
          [-7.3340e-01,  9.2334e-01, -3.6074e+00,  ...,  2.8516e+00,
           -3.0996e+00,  9.5410e-01],
          ...,
          [ 7.3828e-01, -1.8594e+00, -4.7734e+00,  ..., -2.4961e+00,
            5.5811e-01,  2.8535e+00],
          [-5.5762e-01, -2.0820e+00, -3.7754e+00,  ..., -5.4150e-01,
           -3.3789e-01,  2.9902e+00],
          [-2.0264e-01, -4.9170e-01, -1.9570e+00,  ..., -7.3486e-01,
            2.2290e-01,  2.3652e+00]],

         ...,

         [[-4.1779e-02,  2.8992e-02,  1.0809e-01,  ...,  9.0942e-03,
            5.5542e-02,  1.1826e-02],
          [ 2.1683e-02, -4.0942e-01,  1.0801e+00,  ..., -1.2415e-01,
            1.0781e+00,  2.2305e+00],
          [ 6.1865e-01,  5.4590e-01, -1.4004e+00,  ..., -3.7910e+00,
            4.0625e+00, -3.6504e+00],
          ...,
          [ 1.2715e+00,  5.7373e-02,  7.4268e-01,  ..., -1.7471e+00,
            1.9629e+00, -1.7734e+00],
          [-3.0371e-01,  4.6069e-01, -4.9390e-01,  ..., -5.8398e-01,
            1.2910e+00, -1.9053e+00],
          [ 1.4990e-01,  1.1074e+00,  9.3555e-01,  ...,  3.8770e-01,
            1.6931e-01,  8.0994e-02]],

         [[ 1.3477e-01, -6.3362e-03,  2.9144e-02,  ..., -1.4026e-01,
           -8.5754e-03, -3.1860e-02],
          [ 9.3799e-01, -8.2129e-01,  9.3311e-01,  ...,  9.8486e-01,
            1.5752e+00, -1.0576e+00],
          [ 7.5830e-01,  7.8430e-03,  1.3193e+00,  ..., -1.1562e+00,
            1.9570e+00,  1.4932e+00],
          ...,
          [ 2.4551e+00, -2.6602e+00, -5.8008e-01,  ..., -1.2080e+00,
           -8.0762e-01, -8.3838e-01],
          [ 5.9277e-01, -1.1123e+00, -6.4600e-01,  ...,  1.1094e+00,
           -9.9121e-02, -8.7402e-01],
          [-7.1484e-01, -1.7932e-01, -2.5684e-01,  ...,  1.3418e+00,
            6.2500e-01, -1.0547e+00]],

         [[ 1.0468e-01,  8.8215e-04, -1.4294e-01,  ...,  1.9531e-01,
           -1.9470e-01, -6.1523e-02],
          [ 7.9590e-01,  6.4014e-01,  3.6108e-01,  ..., -7.7393e-01,
            6.5674e-01,  8.9209e-01],
          [-1.8818e+00, -4.8462e-01, -4.1382e-01,  ..., -1.7285e+00,
           -3.4741e-01, -1.7646e+00],
          ...,
          [ 8.4717e-02, -1.5601e-01, -2.6387e+00,  ..., -1.2842e+00,
           -9.2676e-01,  3.6699e+00],
          [ 8.8428e-01, -2.1936e-01, -2.5098e+00,  ..., -1.2998e+00,
            9.4043e-01,  8.1982e-01],
          [-8.6133e-01,  6.6602e-01, -2.6172e-01,  ..., -1.8584e+00,
           -8.3789e-01,  1.4172e-01]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>), tensor([[[[ 1.2158e+00,  1.7175e-01,  1.0293e+00,  ...,  2.8574e+00,
            3.3594e+00, -1.3691e+00],
          [-8.1689e-01,  2.2148e+00, -8.5840e-01,  ...,  2.1836e+00,
            3.8223e+00, -1.6934e+00],
          [-2.2180e-01,  5.4102e-01, -3.9673e-01,  ...,  2.0879e+00,
            4.1719e+00, -1.7119e+00],
          ...,
          [-8.5840e-01, -1.1680e+00,  2.3008e+00,  ...,  1.5879e+00,
            2.1973e+00,  8.1885e-01],
          [-3.9727e+00,  1.2323e-01,  3.3945e+00,  ...,  2.1055e+00,
            1.6670e+00,  4.9658e-01],
          [ 6.6602e-01,  6.2305e-01,  9.5264e-01,  ...,  6.8408e-01,
           -1.0967e+00,  4.6362e-01]],

         [[ 1.9248e+00,  1.1807e+00,  6.2695e-01,  ...,  1.7354e+00,
           -6.4621e-03, -2.0293e+00],
          [ 3.6304e-01,  7.0410e-01,  2.4160e+00,  ...,  1.3184e+00,
           -1.7217e+00, -1.8242e+00],
          [ 1.6992e+00,  1.4697e+00,  2.8027e+00,  ...,  1.2041e+00,
            1.1143e+00,  7.1472e-02],
          ...,
          [ 3.7090e+00,  1.5537e+00, -9.7607e-01,  ...,  1.9189e+00,
           -7.2559e-01, -6.7041e-01],
          [ 4.5654e-01,  9.7266e-01, -2.2109e+00,  ...,  2.2754e-01,
            1.0956e-01,  1.6797e+00],
          [-4.8071e-01, -3.6060e-01,  2.6382e-02,  ..., -1.3740e+00,
           -4.1943e-01, -5.0049e-01]],

         [[ 1.2920e+00, -1.3760e+00, -2.6953e+00,  ..., -1.5498e+00,
            2.9551e+00,  5.1025e-01],
          [ 1.6348e+00, -1.1211e+00, -2.0078e+00,  ..., -1.4277e+00,
            3.1523e+00,  2.2903e-02],
          [ 8.0273e-01, -1.3584e+00, -5.4297e-01,  ..., -1.2549e+00,
            1.3066e+00,  4.9146e-01],
          ...,
          [-7.1289e-01, -1.8535e+00, -1.9568e-01,  ..., -1.3057e+00,
            1.9736e+00, -1.5625e-01],
          [ 4.9658e-01, -1.7334e+00, -1.5908e+00,  ..., -1.9275e-01,
            1.6592e+00, -8.7952e-02],
          [-3.7573e-01, -1.2793e+00, -1.0078e+00,  ...,  8.2080e-01,
           -3.5801e+00, -1.1611e+00]],

         ...,

         [[ 9.1504e-01, -3.6035e-01, -5.0586e-01,  ..., -1.9824e+00,
            3.6914e+00,  1.9131e+00],
          [-1.8184e+00,  6.4941e-01, -2.3105e+00,  ..., -1.8955e+00,
            3.8203e+00,  3.4863e+00],
          [-3.7524e-01, -1.0791e+00, -1.2520e+00,  ..., -1.5557e+00,
            3.7578e+00,  1.9727e+00],
          ...,
          [-1.3799e+00, -1.0332e+00, -1.9746e+00,  ..., -6.7188e-01,
            1.5137e+00,  1.7725e+00],
          [ 6.0889e-01, -7.6172e-01, -1.3203e+00,  ..., -1.0732e+00,
           -1.2969e+00,  3.4512e+00],
          [ 1.0846e-01, -2.5171e-01,  5.1367e-01,  ...,  2.4434e+00,
            2.6465e-01, -4.2603e-01]],

         [[ 1.8799e-01,  5.4346e-01,  2.4060e-01,  ...,  8.0420e-01,
            1.6816e+00, -9.5654e-01],
          [-2.3291e-01, -1.4639e+00,  1.1094e+00,  ..., -1.2617e+00,
            8.5449e-01, -5.4932e-01],
          [-6.9580e-01, -2.1191e+00, -8.2275e-01,  ...,  7.0312e-02,
            1.2285e+00, -4.6289e-01],
          ...,
          [-1.2217e+00, -2.0645e+00, -3.4961e-01,  ..., -3.5034e-01,
            6.5918e-01, -1.4189e+00],
          [ 8.6719e-01,  1.0176e+00, -2.0142e-01,  ..., -3.0859e-01,
            1.5352e+00, -1.1475e+00],
          [ 8.5449e-01,  6.4941e-01, -8.7585e-02,  ..., -1.2363e+00,
           -1.0293e+00,  9.7607e-01]],

         [[-1.0566e+00, -5.8350e-01,  8.4424e-01,  ...,  1.2246e+00,
           -4.5715e-02,  1.8018e+00],
          [-1.7373e+00, -1.2451e+00,  1.7451e+00,  ...,  9.3896e-01,
           -2.0547e+00,  1.1963e+00],
          [-1.7793e+00, -8.2275e-01,  8.6816e-01,  ..., -9.4543e-02,
            1.0537e+00,  1.6553e+00],
          ...,
          [-1.6367e+00,  1.7373e+00,  1.4287e+00,  ..., -2.7881e-01,
           -2.1094e+00,  1.0928e+00],
          [-1.1846e+00,  1.4170e+00,  2.3223e+00,  ..., -2.5195e+00,
           -1.9600e+00,  1.5498e+00],
          [ 2.0544e-01, -5.5908e-01, -6.3330e-01,  ..., -1.2236e+00,
            1.1250e+01, -2.2773e+00]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>), tensor([[[[ 1.7285e-01,  2.1326e-01, -1.2207e+00,  ..., -3.3730e+00,
           -2.3999e-01, -1.4404e+00],
          [-6.9519e-02, -9.8047e-01, -2.2578e+00,  ..., -1.5215e+00,
           -2.1035e+00, -2.0227e-01],
          [-1.8164e-01, -2.2500e+00, -2.5059e+00,  ..., -2.5137e+00,
           -3.1172e+00,  5.2344e-01],
          ...,
          [-4.3799e-01, -2.9272e-01, -5.3027e-01,  ..., -4.0352e+00,
            3.7402e-01,  1.7314e+00],
          [ 1.7578e-01, -9.8047e-01, -2.3926e+00,  ..., -4.5532e-01,
           -1.0742e+00,  2.9590e+00],
          [-7.8174e-01, -2.8735e-01,  8.0957e-01,  ...,  7.4316e-01,
            5.0928e-01, -1.7139e-01]],

         [[ 2.5482e-02, -7.5244e-01,  1.7207e+00,  ..., -2.7344e+00,
            1.2383e+00,  3.5195e+00],
          [ 1.1641e+00, -1.9062e+00,  8.5742e-01,  ..., -8.1738e-01,
            3.3301e-01,  2.3279e-01],
          [ 2.5625e+00, -3.6670e-01,  2.5547e+00,  ...,  6.3477e-01,
           -1.0752e+00,  1.0283e+00],
          ...,
          [ 5.7715e-01,  3.0098e+00, -2.8242e+00,  ...,  5.7129e-01,
           -2.2449e-01, -1.0098e+00],
          [ 3.9575e-01,  1.4189e+00, -1.9678e-01,  ...,  9.1797e-02,
           -1.2588e+00, -1.1191e+00],
          [ 6.7253e-03,  5.4047e-02,  1.7365e-02,  ..., -1.9421e-01,
           -2.0581e-01, -7.4365e-01]],

         [[-3.4551e+00, -8.4961e-01,  2.2734e+00,  ..., -3.1113e+00,
           -6.6895e-01,  1.2070e+00],
          [-1.3096e+00,  2.7954e-01,  3.8281e+00,  ..., -4.2969e+00,
           -1.5410e+00,  3.5742e+00],
          [-1.4502e+00,  1.5303e+00,  3.5508e+00,  ..., -2.9102e+00,
           -1.0938e+00,  2.7793e+00],
          ...,
          [-5.4688e+00, -3.6157e-01,  1.6309e+00,  ..., -5.2422e+00,
           -1.2275e+00,  3.2598e+00],
          [-2.8906e+00, -1.0166e+00,  2.8271e-01,  ..., -2.8000e-02,
            1.4346e+00,  3.2520e-01],
          [ 1.2384e-01,  4.7803e-01, -8.2764e-01,  ..., -1.7023e-03,
           -2.5317e-01,  1.9385e-01]],

         ...,

         [[-2.6211e+00,  4.4141e-01,  8.9014e-01,  ..., -1.4233e-01,
           -6.5039e-01, -4.4141e+00],
          [-1.7920e+00,  9.5068e-01, -1.3984e+00,  ...,  1.0088e+00,
           -4.9438e-01, -2.7637e+00],
          [-2.5859e+00, -4.5557e-01, -1.0605e+00,  ...,  4.0845e-01,
            1.2510e+00, -3.0195e+00],
          ...,
          [ 8.2520e-01,  1.2080e+00, -5.7037e-02,  ..., -1.6143e+00,
           -1.5173e-01, -1.0312e+00],
          [-4.7461e-01,  1.9717e+00, -2.5625e+00,  ..., -2.8340e+00,
           -1.1660e+00, -1.8145e+00],
          [ 5.5518e-01,  3.8794e-01,  3.6353e-01,  ..., -3.1738e-01,
           -5.6445e-01,  6.1621e-01]],

         [[-4.6582e-01, -9.1064e-01, -6.8799e-01,  ...,  1.1162e+00,
            1.0283e+00,  2.6055e+00],
          [ 1.3115e+00, -2.0154e-01,  2.5253e-02,  ...,  2.9473e+00,
            1.8350e+00,  2.3828e+00],
          [ 3.6279e-01, -3.1885e-01, -8.1299e-01,  ...,  2.0410e+00,
            3.3242e+00,  7.1338e-01],
          ...,
          [-3.8452e-01,  1.7607e+00,  6.5479e-01,  ...,  7.7051e-01,
            2.3242e+00, -9.5654e-01],
          [-6.8420e-02, -2.1465e+00,  1.2090e+00,  ...,  7.7734e-01,
            4.4165e-01, -1.2178e+00],
          [ 3.7964e-01,  1.8164e-01, -6.2354e-01,  ..., -5.0195e-01,
            8.7341e-02,  2.3474e-01]],

         [[ 1.6729e+00,  1.0029e+00, -6.7773e-01,  ..., -1.1807e+00,
           -4.5742e+00, -1.6614e-01],
          [ 1.9443e+00, -3.4473e+00, -3.0391e+00,  ...,  2.1895e+00,
           -2.6230e+00,  1.5635e+00],
          [ 2.9238e+00, -4.3750e+00, -4.3506e-01,  ...,  2.0156e+00,
           -1.3994e+00, -1.1260e+00],
          ...,
          [-2.0156e+00,  1.1310e-01, -2.1621e+00,  ...,  2.0098e+00,
           -2.8164e+00,  1.6758e+00],
          [ 2.2578e+00, -4.6118e-01, -2.1621e+00,  ..., -2.5762e+00,
           -3.0020e+00,  1.4912e+00],
          [ 7.8003e-02, -2.4023e-01,  2.2729e-01,  ..., -5.3516e-01,
            3.9966e-01, -6.6528e-02]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>)), (tensor([[[[ 2.3474e-01,  7.9102e-01,  1.2695e-01,  ...,  1.2683e-01,
           -6.8750e-01, -5.7281e-02],
          [-9.7168e-01, -1.0449e-01, -1.8789e+00,  ..., -2.0898e+00,
            1.0186e+00,  3.2898e-02],
          [-2.1558e-01,  5.1904e-01,  1.4062e-01,  ..., -5.9766e-01,
            1.9668e+00, -1.8994e+00],
          ...,
          [-2.3657e-01, -9.4287e-01, -1.5811e+00,  ..., -5.1514e-01,
           -9.1162e-01, -8.5986e-01],
          [-1.9617e-01, -8.5352e-01, -1.0068e+00,  ..., -1.2354e+00,
           -3.4668e-01, -1.1023e-01],
          [-6.6016e-01, -2.7612e-01, -1.1230e+00,  ..., -1.5605e+00,
            6.3574e-01, -7.7051e-01]],

         [[ 1.4514e-01,  1.2866e-01, -3.4717e-01,  ...,  2.4084e-01,
           -4.3060e-02,  4.0112e-01],
          [-2.1758e+00,  2.3711e+00,  8.5547e-01,  ...,  6.4600e-01,
           -7.3145e-01,  2.4231e-02],
          [ 9.4727e-02,  1.2832e+00,  9.3945e-01,  ..., -1.4746e-01,
           -5.0000e-01, -1.4385e+00],
          ...,
          [ 4.6973e-01,  4.3945e-01, -8.6035e-01,  ...,  1.2803e+00,
            7.7051e-01, -3.3813e-01],
          [-3.7158e-01,  7.7197e-01,  3.7231e-02,  ...,  6.9971e-01,
            1.1211e+00, -5.2148e-01],
          [ 1.3457e+00, -2.7344e-01,  5.5566e-01,  ...,  2.0898e+00,
            1.6396e+00,  1.4355e+00]],

         [[-6.1377e-01, -5.3857e-01,  4.0820e-01,  ...,  9.6558e-02,
           -1.5686e-01,  3.8509e-03],
          [-2.1289e-01,  1.4766e+00, -1.1973e+00,  ...,  1.5996e+00,
            1.4758e-01,  1.1504e+00],
          [-8.0615e-01,  4.0381e-01, -6.2451e-01,  ...,  1.0986e+00,
           -6.9287e-01, -7.3975e-02],
          ...,
          [ 6.3574e-01,  2.8442e-01, -5.4004e-01,  ..., -1.0155e-02,
           -1.7090e+00, -5.7471e-01],
          [-1.6626e-01,  1.8030e-01, -7.3975e-01,  ...,  4.1919e-01,
           -1.2461e+00,  6.3428e-01],
          [ 1.0333e-01, -3.0908e-01, -1.6211e+00,  ..., -3.3423e-01,
            4.5630e-01, -7.0850e-01]],

         ...,

         [[-1.1536e-01, -1.8204e-02,  1.5295e-01,  ..., -2.8638e-01,
           -4.8248e-02,  1.8518e-01],
          [-5.5127e-01, -5.9473e-01, -6.2598e-01,  ..., -8.3557e-02,
            1.2955e-02,  8.2373e-01],
          [-1.3203e+00, -3.7964e-01,  3.6694e-01,  ..., -6.2622e-02,
            5.6152e-01,  8.3887e-01],
          ...,
          [-1.0625e+00,  3.1689e-01, -5.4639e-01,  ..., -1.0234e+00,
            9.2139e-01,  6.1328e-01],
          [-6.8994e-01,  8.6731e-02, -5.9424e-01,  ..., -1.7542e-01,
            1.6592e+00,  4.1357e-01],
          [ 1.9722e-03, -9.9609e-02,  3.4131e-01,  ...,  3.4863e-01,
            8.6975e-02,  2.2241e-01]],

         [[ 3.0908e-01, -1.6455e-01, -1.2170e-01,  ...,  9.9548e-02,
            2.2034e-01, -2.0068e-01],
          [ 5.0684e-01, -1.0605e+00, -7.5134e-02,  ...,  2.9468e-01,
           -4.9609e-01,  1.1631e+00],
          [ 3.2983e-01,  7.3535e-01,  8.1543e-01,  ..., -9.2334e-01,
            6.8262e-01,  5.4932e-01],
          ...,
          [-8.9307e-01,  6.2744e-01,  2.8467e-01,  ...,  6.1719e-01,
           -1.2451e+00,  2.9980e-01],
          [ 9.0576e-02,  1.1029e-01, -4.3213e-01,  ...,  7.2998e-01,
           -1.9385e+00,  4.1577e-01],
          [-1.0156e+00,  1.1309e+00,  8.0078e-02,  ...,  6.5381e-01,
           -6.9727e-01,  1.8933e-01]],

         [[-9.8389e-02,  2.4255e-01,  6.9237e-03,  ..., -1.6052e-01,
            1.4429e-01, -9.1019e-03],
          [ 2.7808e-01, -2.1448e-01,  1.2100e+00,  ..., -1.7256e+00,
           -1.0869e+00,  1.5430e+00],
          [-4.4971e-01,  2.7759e-01, -5.7617e-01,  ..., -1.1084e+00,
           -7.2461e-01, -1.2146e-01],
          ...,
          [-1.7998e+00,  4.7095e-01, -1.0557e+00,  ..., -3.6011e-01,
            6.5967e-01,  8.5596e-01],
          [-2.2461e+00,  1.0752e+00,  6.7969e-01,  ..., -1.0762e+00,
            9.3506e-01,  5.8594e-02],
          [-1.2622e-01, -8.2764e-01, -1.3311e+00,  ...,  8.7207e-01,
            2.1387e+00,  6.1249e-02]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>), tensor([[[[ 4.0955e-02, -8.1665e-02, -9.4788e-02,  ..., -2.4902e-01,
           -1.0635e-02,  1.2103e-01],
          [-2.0532e-01,  2.4219e+00,  2.4500e-01,  ..., -3.8025e-02,
           -1.0420e+00, -6.8726e-02],
          [-2.1191e+00,  4.7388e-01,  3.8853e-03,  ..., -1.8174e+00,
           -2.5122e-01,  2.0273e+00],
          ...,
          [-4.0454e-01, -1.4490e-01, -1.0342e+00,  ...,  2.2192e-01,
           -2.8906e+00,  8.6963e-01],
          [ 8.9600e-01, -1.5942e-01, -1.7168e+00,  ..., -1.1846e+00,
           -1.1865e+00,  1.3115e+00],
          [ 7.1045e-01, -2.6094e+00,  1.7080e+00,  ..., -7.7441e-01,
           -8.5022e-02, -1.0039e+00]],

         [[-3.2562e-02, -2.0920e-02, -1.0986e-01,  ...,  6.9336e-02,
            1.4313e-02,  7.7087e-02],
          [-8.5547e-01, -1.1172e+00, -2.5391e+00,  ..., -2.0605e+00,
           -8.7256e-01, -2.8867e+00],
          [ 4.8828e-01,  2.8271e-01,  7.8174e-01,  ..., -3.9316e+00,
            1.4531e+00, -2.4316e+00],
          ...,
          [-2.9180e+00, -1.5869e-01,  2.5469e+00,  ..., -1.7959e+00,
            1.2188e+00,  3.7915e-01],
          [-3.2373e-01,  3.2007e-01,  1.3262e+00,  ..., -2.4043e+00,
           -9.8096e-01,  7.4609e-01],
          [-8.3984e-01,  4.9121e-01, -1.1172e+00,  ...,  1.9756e+00,
           -1.4521e+00, -1.0846e-01]],

         [[ 1.8244e-03, -3.7231e-02, -8.9722e-02,  ...,  1.0628e-02,
            1.5906e-01,  2.8351e-02],
          [ 7.1631e-01, -3.1094e+00,  1.3208e-01,  ...,  9.3896e-01,
            3.1036e-02, -2.6260e-02],
          [-8.8623e-01, -7.9688e-01, -8.7744e-01,  ...,  2.1523e+00,
            2.2285e+00, -2.0176e+00],
          ...,
          [-8.5938e-01, -1.6592e+00, -9.1309e-01,  ...,  3.4082e-01,
            1.5215e+00, -1.9082e+00],
          [ 1.6479e-01, -1.1279e+00, -1.7617e+00,  ...,  1.5942e-01,
            8.7061e-01, -1.5439e+00],
          [ 4.2847e-01, -7.5586e-01, -1.2471e+00,  ..., -1.1064e+00,
            7.3828e-01,  1.5273e+00]],

         ...,

         [[ 1.1755e-01, -1.3562e-01, -8.2764e-02,  ..., -1.3354e-01,
           -1.5491e-01,  1.7712e-01],
          [-1.1738e+00, -1.1816e+00, -2.5781e+00,  ..., -7.1777e-01,
            1.1871e-01,  4.2908e-02],
          [ 4.5581e-01, -3.8789e+00,  1.1748e+00,  ..., -3.1152e-01,
           -1.2021e+00,  1.6475e+00],
          ...,
          [ 6.8726e-02,  1.2441e+00, -5.7080e-01,  ...,  9.7314e-01,
           -8.7207e-01,  1.8633e+00],
          [-7.9639e-01, -6.6162e-01, -8.0518e-01,  ...,  6.4258e-01,
           -9.0527e-01,  1.0488e+00],
          [ 7.1143e-01,  1.7273e-01, -2.3203e+00,  ...,  1.0459e+00,
            6.5674e-02,  3.7988e+00]],

         [[ 5.8632e-03,  8.8135e-02, -1.4076e-03,  ..., -3.7476e-02,
           -4.1321e-02, -3.6426e-01],
          [-2.0781e+00,  7.9541e-01, -1.0205e+00,  ..., -6.6748e-01,
           -6.9727e-01,  4.4453e+00],
          [-3.4497e-01, -2.2988e+00,  7.3438e-01,  ..., -1.2588e+00,
           -2.1289e+00,  4.6602e+00],
          ...,
          [-3.4980e+00, -2.2832e+00,  4.7388e-01,  ..., -2.0625e+00,
           -1.6895e+00,  6.6641e+00],
          [-3.2871e+00, -3.5918e+00,  1.4082e+00,  ..., -2.5996e+00,
           -2.2129e+00,  4.7891e+00],
          [ 1.3760e+00, -9.6338e-01,  2.2402e+00,  ..., -3.2031e-01,
           -1.6621e+00,  1.7383e+00]],

         [[-1.9470e-01,  3.9734e-02, -3.5828e-02,  ...,  8.6670e-02,
            6.1584e-02, -2.1460e-01],
          [ 9.2383e-01,  6.3171e-02,  8.3057e-01,  ..., -3.5693e-01,
           -1.9980e+00,  1.1934e+00],
          [-1.7168e+00,  1.5615e+00,  2.1602e+00,  ..., -9.0820e-01,
            1.3154e+00, -1.7444e-01],
          ...,
          [ 2.4863e+00,  1.5352e+00,  5.5225e-01,  ..., -1.6777e+00,
           -3.2754e+00,  4.0391e+00],
          [-6.9092e-02, -2.7295e-01, -1.4258e-01,  ..., -8.2666e-01,
           -2.1094e+00,  2.9434e+00],
          [-2.2070e+00, -3.6890e-01,  2.3291e-01,  ...,  1.5635e+00,
           -1.3535e+00,  1.8896e+00]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>), tensor([[[[-0.2480, -1.7188,  2.4590,  ...,  4.6875,  1.4404,  4.0391],
          [-3.0625, -2.5176, -1.2178,  ...,  1.9961, -1.0488,  3.1445],
          [ 0.6650, -3.9121, -0.0331,  ...,  3.2441,  1.0107,  4.3867],
          ...,
          [ 0.1724,  2.6719, -0.7944,  ..., -0.6479,  0.4529,  1.5557],
          [-0.6802,  0.8574,  0.7124,  ...,  1.6787,  1.9209,  0.4133],
          [10.6016, -1.6875,  0.3394,  ..., -2.0801, -1.5391,  0.5063]],

         [[ 1.2188, -2.5723, -0.8706,  ...,  0.1431, -3.6484,  1.4414],
          [-0.0157, -0.1844,  2.5508,  ...,  0.4878, -3.6016,  0.0476],
          [-1.3408, -1.3770,  0.9121,  ..., -0.7168, -3.6855,  0.9668],
          ...,
          [ 0.4412,  3.5059, -2.0410,  ..., -0.2010, -1.3545, -0.2253],
          [-0.7451, -0.2517, -0.7466,  ...,  0.4949,  2.4648, -1.6514],
          [ 2.1074,  3.5293,  1.3721,  ...,  0.7422,  0.6934, -0.7964]],

         [[-0.2407, -1.0361,  1.4609,  ..., -0.7842, -1.2939,  0.2457],
          [ 0.9858,  0.7793, -0.6060,  ..., -0.3230, -0.5444, -0.8442],
          [ 1.3398,  0.8560,  0.1271,  ...,  0.0716, -1.4121, -1.1357],
          ...,
          [ 1.4658,  0.2061,  0.2421,  ...,  1.5986, -1.1865, -3.1230],
          [ 2.5586, -0.9697,  0.1431,  ..., -0.6104, -2.4043,  0.9888],
          [-3.1953, -0.3167,  0.3445,  ..., -0.5273,  2.2207,  0.3850]],

         ...,

         [[-1.3848,  2.5645, -1.1338,  ..., -1.7422, -0.4519, -0.0441],
          [-1.4219, -0.5264,  2.0918,  ..., -1.1406, -1.0332, -2.1895],
          [-0.2651,  1.4619,  0.1086,  ...,  0.0671, -1.5439, -0.7275],
          ...,
          [ 0.3223,  3.3691, -0.6172,  ..., -1.4707,  0.4993, -0.6143],
          [-0.0109,  0.6235,  0.4470,  ..., -1.2314, -1.0723, -2.2539],
          [-0.8276,  0.5117,  1.3281,  ...,  1.3320, -1.0713,  0.7559]],

         [[-0.0903,  1.7754,  0.9438,  ..., -1.7129, -2.6934,  3.0332],
          [ 0.2747,  1.6279,  3.0684,  ...,  1.5186, -1.6865,  1.0938],
          [ 1.8037,  0.5771,  2.6621,  ...,  2.4766, -3.4473,  0.9463],
          ...,
          [ 1.5293,  3.3867, -0.0656,  ..., -1.4609, -2.6191,  1.5830],
          [ 0.5645,  1.3145,  0.2001,  ..., -1.9570, -1.3389, -0.2832],
          [-0.0874, -6.1953,  0.8921,  ..., -0.6343,  1.3975, -2.3926]],

         [[-0.5737, -0.3262,  2.3340,  ..., -0.1035, -3.2578,  2.3594],
          [ 0.2389, -0.9082,  0.5649,  ...,  0.1493, -3.3086,  0.2446],
          [ 0.3848,  0.5757, -1.2236,  ...,  0.6245, -2.2129, -0.2800],
          ...,
          [-2.4297, -0.7651,  0.2390,  ...,  2.1875, -0.9863, -1.0352],
          [-1.9639,  1.0703,  1.9609,  ...,  1.3027,  0.3713, -2.1035],
          [ 0.3079, -0.5762,  0.7881,  ..., -1.0518,  0.5698,  0.1070]]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<TransposeBackward0>), tensor([[[[-2.2207e+00,  1.0508e+00,  3.8721e-01,  ...,  2.6426e+00,
            8.0176e-01, -2.2188e+00],
          [-6.7236e-01, -5.8887e-01, -1.4685e-01,  ...,  8.6279e-01,
            7.4658e-01, -2.5938e+00],
          [-3.4668e-01,  3.5693e-01,  6.7432e-01,  ..., -3.2764e-01,
            2.7852e+00, -3.3086e+00],
          ...,
          [ 3.9722e-01, -1.3203e+00, -7.4658e-01,  ..., -2.1375e-01,
            1.6191e+00,  1.2139e+00],
          [-6.6699e-01,  3.5815e-01, -5.6641e-01,  ..., -1.7793e+00,
           -5.1904e-01, -1.7803e+00],
          [ 4.2175e-02, -1.5283e-01, -5.9033e-01,  ..., -2.3657e-01,
            1.4148e-01, -3.5059e-01]],

         [[-8.5254e-01, -1.4233e-01,  2.2637e+00,  ..., -2.9907e-01,
            1.6719e+00, -1.4160e+00],
          [ 7.3730e-01, -2.0723e+00,  3.7559e+00,  ..., -1.5020e+00,
            1.2461e+00, -1.1914e+00],
          [-3.3691e-01, -2.6904e-01,  1.8457e+00,  ..., -1.4502e+00,
            3.6835e-02,  8.1982e-01],
          ...,
          [-3.1616e-02,  8.4326e-01, -5.2002e-01,  ..., -1.3262e+00,
            7.1729e-01,  2.6992e+00],
          [ 8.7952e-02,  2.0625e+00, -1.3989e-01,  ..., -5.2734e-01,
            2.2480e+00,  2.8672e+00],
          [ 4.2798e-01,  3.2666e-01,  4.0698e-01,  ..., -5.2197e-01,
            1.2054e-01,  2.1448e-01]],

         [[ 3.8008e+00, -2.0195e+00,  1.8877e+00,  ..., -8.2910e-01,
           -2.4102e+00,  9.9365e-01],
          [-4.8901e-01, -3.0469e+00, -7.9346e-01,  ..., -7.5781e-01,
           -1.2158e+00, -3.0420e-01],
          [ 1.6240e+00, -2.4590e+00,  2.5708e-01,  ..., -1.1777e+00,
           -1.4619e+00,  8.1836e-01],
          ...,
          [ 1.6660e+00, -4.9062e+00, -1.8076e+00,  ...,  5.2100e-01,
            6.5820e-01, -1.5879e+00],
          [-7.8955e-01, -1.4932e+00, -1.4795e+00,  ..., -1.7373e+00,
            4.7227e+00, -2.3828e+00],
          [-2.8223e-01, -2.1204e-01, -2.4146e-01,  ...,  8.0518e-01,
           -3.1470e-01,  3.5620e-01]],

         ...,

         [[ 3.8828e+00,  3.6934e+00, -1.9258e+00,  ..., -1.8076e+00,
            7.4646e-02,  6.9238e-01],
          [ 2.6113e+00,  2.6680e+00, -1.5000e+00,  ..., -3.6938e-01,
           -8.1543e-01,  9.2383e-01],
          [ 2.0430e+00,  2.3750e+00, -7.9785e-01,  ..., -1.3262e+00,
            3.6450e-01,  1.5015e-01],
          ...,
          [ 2.2773e+00,  1.4150e+00, -2.2852e+00,  ..., -1.8438e+00,
            2.4492e+00, -4.3091e-01],
          [ 1.4131e+00,  1.0986e+00, -5.1172e+00,  ..., -1.7695e+00,
            6.3232e-01, -7.2217e-01],
          [ 6.8115e-02,  2.9932e-01, -6.5674e-02,  ..., -6.9763e-02,
           -7.0068e-01,  8.2153e-02]],

         [[-1.2100e+00,  9.0088e-02,  2.3770e+00,  ..., -1.3464e-01,
            1.0615e+00,  7.5879e-01],
          [-9.5764e-02, -2.4082e+00, -4.6265e-02,  ...,  2.1406e+00,
           -2.5684e+00,  6.3184e-01],
          [-8.8281e-01, -1.8984e+00, -2.2227e+00,  ...,  1.6172e+00,
           -2.5703e+00,  4.4873e-01],
          ...,
          [-3.9707e+00, -2.1887e-01, -8.9233e-02,  ..., -1.6289e+00,
            1.6543e+00, -4.0454e-01],
          [ 4.8584e-01,  7.8711e-01, -1.7461e+00,  ..., -2.2168e+00,
            1.5586e+00, -3.4912e-01],
          [-2.4658e-01, -4.7668e-02, -1.1700e-01,  ...,  6.4258e-01,
           -2.4826e-02, -1.3074e-01]],

         [[ 4.9316e-01,  8.7695e-01,  3.6504e+00,  ...,  1.8740e+00,
           -1.3257e-01, -8.4277e-01],
          [ 1.4863e+00, -1.4795e+00,  6.7539e+00,  ...,  2.6230e+00,
           -2.3359e+00, -9.5361e-01],
          [ 1.3867e+00, -4.1523e+00,  3.9766e+00,  ...,  1.1414e-01,
           -1.4294e-01, -3.5742e-01],
          ...,
          [ 1.1025e+00,  1.8682e+00, -1.3184e+00,  ...,  2.5269e-01,
            2.3105e+00,  1.8418e+00],
          [ 2.3633e+00, -1.4062e-01, -1.7338e-03,  ...,  1.5176e+00,
            1.3018e+00, -6.6309e-01],
          [-5.1318e-01,  1.5771e-01, -6.7529e-01,  ..., -7.8711e-01,
           -1.6431e-01,  5.2344e-01]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>)), (tensor([[[[ 3.0609e-02, -2.4277e+00, -1.9287e-01,  ...,  2.3901e-01,
            8.7280e-02,  1.4197e-01],
          [-8.7451e-01,  3.3145e+00,  6.7383e-01,  ...,  3.2446e-01,
           -2.0977e+00, -1.0264e+00],
          [-8.7354e-01,  4.1680e+00,  2.7783e-01,  ...,  1.1963e-01,
            1.4551e-01,  1.2871e+00],
          ...,
          [-1.1689e+00,  5.5469e+00,  1.2744e-01,  ...,  3.4155e-01,
            5.0977e-01, -1.7334e-01],
          [-1.9619e+00,  6.1016e+00, -4.8950e-01,  ...,  1.9397e-01,
           -4.2676e-01, -3.0542e-01],
          [-2.0391e+00,  6.1523e+00, -5.4834e-01,  ..., -1.1641e+00,
            1.2432e+00,  6.2805e-02]],

         [[-6.4270e-02,  1.6516e-01, -6.1621e-01,  ..., -4.4116e-01,
           -1.3269e-01, -3.8281e-01],
          [-6.9482e-01,  6.3354e-02,  1.5654e+00,  ...,  1.3398e+00,
            9.3506e-01,  6.2927e-02],
          [ 4.2017e-01, -1.0850e+00,  8.9209e-01,  ...,  2.8711e+00,
            4.8877e-01, -1.0625e+00],
          ...,
          [-4.9097e-01,  2.2363e-01,  5.6836e-01,  ...,  1.7322e-01,
            1.4526e-01, -3.3472e-01],
          [-3.9038e-01,  4.1992e-01,  1.1682e-01,  ...,  8.2617e-01,
           -5.6689e-01, -5.1953e-01],
          [ 5.4688e-01,  8.9795e-01,  3.3423e-01,  ...,  1.5967e+00,
           -1.1885e+00, -5.2783e-01]],

         [[ 4.5052e-03, -2.9858e-01,  3.6401e-01,  ...,  5.5420e-02,
            2.1011e-02,  1.2500e-01],
          [-7.2168e-01,  1.7520e+00, -3.1934e-01,  ..., -7.1533e-01,
            2.0625e+00,  2.6758e-01],
          [-9.3701e-01,  2.4062e+00, -2.2925e-01,  ...,  2.3059e-01,
            3.1641e-01,  8.8574e-01],
          ...,
          [-1.8359e-01,  1.0977e+00,  1.5840e+00,  ..., -6.6309e-01,
            2.0984e-01, -1.1650e+00],
          [ 3.9771e-01,  1.9580e+00,  1.2432e+00,  ..., -1.3555e+00,
            8.5645e-01, -9.9304e-02],
          [-6.4990e-01,  1.3916e+00, -3.6450e-01,  ..., -2.0586e+00,
            7.9785e-01, -3.0420e-01]],

         ...,

         [[-9.4748e-04, -1.4441e-01, -4.2389e-02,  ..., -3.0420e-01,
            3.0859e-01, -4.8511e-01],
          [-8.3789e-01, -8.1055e-01,  3.0469e-01,  ..., -1.2341e-01,
           -1.2695e+00, -2.9590e-01],
          [-1.4883e+00,  2.6138e-02,  1.8799e+00,  ..., -8.6523e-01,
           -1.4971e+00,  3.7048e-02],
          ...,
          [-1.3496e+00,  6.3782e-02,  1.0742e+00,  ..., -2.6514e-01,
            8.5156e-01,  8.4863e-01],
          [-9.9658e-01, -1.1270e+00,  1.0996e+00,  ...,  4.1821e-01,
           -2.9517e-01, -3.3154e-01],
          [-2.1619e-01, -1.3525e+00,  1.2861e+00,  ...,  5.4053e-01,
           -1.1650e+00, -9.1455e-01]],

         [[ 8.3923e-02, -3.6987e-01, -8.8013e-02,  ..., -1.6675e-01,
           -1.8726e-01, -4.0619e-02],
          [ 7.3486e-01,  7.4756e-01,  3.8354e-01,  ..., -1.5723e+00,
           -5.7617e-01, -1.2842e+00],
          [ 9.2090e-01,  1.1230e+00, -7.0801e-01,  ..., -6.7236e-01,
           -1.0459e+00, -1.5547e+00],
          ...,
          [-4.9829e-01,  1.2910e+00,  1.0283e+00,  ...,  5.6684e-05,
            2.6611e-01,  3.0713e-01],
          [ 2.3120e-01,  5.7959e-01,  7.0654e-01,  ..., -6.5479e-01,
            3.4253e-01,  8.2373e-01],
          [-8.8770e-01, -2.9126e-01,  5.0781e-01,  ..., -1.1006e+00,
            1.0635e+00,  4.9561e-01]],

         [[ 2.9565e-01, -4.4556e-01, -2.5879e-01,  ..., -1.9751e-01,
            3.4814e-01,  1.2134e-01],
          [-4.4141e-01,  1.4023e+00,  2.4492e+00,  ..., -2.6611e-01,
            1.8921e-01, -1.1543e+00],
          [-9.1943e-01,  1.7812e+00,  1.9502e+00,  ...,  9.0625e-01,
           -1.5117e+00, -2.7563e-01],
          ...,
          [-2.5244e-01, -7.4219e-01, -6.9824e-01,  ...,  1.0186e+00,
           -3.9136e-01, -1.5264e+00],
          [-1.4824e+00, -4.2664e-02, -2.4475e-02,  ...,  1.0052e-01,
           -1.0931e-01, -1.3213e+00],
          [-1.6675e-01,  4.4897e-01,  3.8330e-02,  ...,  2.7979e-01,
           -1.4287e+00, -2.1191e+00]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>), tensor([[[[ 1.4282e-01, -6.1707e-02, -1.1688e-01,  ..., -6.3171e-02,
           -2.1240e-01, -3.2501e-02],
          [-1.9751e-01, -3.2202e-01, -1.9297e+00,  ...,  8.4424e-01,
           -6.0986e-01, -3.9941e-01],
          [ 3.1660e+00,  3.8984e+00, -1.1387e+00,  ..., -2.2734e+00,
           -3.3325e-02,  7.6318e-01],
          ...,
          [ 1.2422e+00,  1.2383e+00, -4.7188e+00,  ...,  4.3633e+00,
            1.7402e+00,  1.3506e+00],
          [ 1.6973e+00,  1.9834e+00, -2.6172e+00,  ...,  2.2129e+00,
            1.7539e+00,  1.3306e-01],
          [ 1.7151e-01,  2.9043e+00,  9.8096e-01,  ...,  3.8965e-01,
            1.2344e+00,  2.8555e+00]],

         [[-6.0211e-02, -4.2938e-02, -9.3628e-02,  ..., -2.1472e-01,
           -1.4441e-01, -1.0339e-01],
          [ 9.7412e-01,  1.4648e+00, -1.2744e+00,  ...,  1.2480e+00,
            1.0400e+00, -1.3350e+00],
          [ 1.2559e+00, -2.4487e-01,  1.7031e+00,  ..., -1.1139e-01,
           -1.2080e+00,  8.6035e-01],
          ...,
          [-2.0056e-01,  2.8223e+00, -2.2734e+00,  ...,  1.5264e+00,
            1.0635e+00, -8.8196e-02],
          [ 5.4004e-01,  3.7871e+00, -6.9336e-01,  ...,  2.2051e+00,
           -6.2939e-01, -1.7070e+00],
          [-1.8848e+00,  7.6133e+00,  7.6318e-01,  ...,  3.4668e+00,
           -2.1895e+00, -1.0732e+00]],

         [[ 5.6396e-02,  5.9601e-02,  8.6304e-02,  ...,  2.0959e-01,
            1.7065e-01,  8.8440e-02],
          [-8.1055e-02, -1.4980e+00,  4.4751e-01,  ...,  9.2834e-02,
           -8.2129e-01, -8.4180e-01],
          [-3.3997e-02, -2.7090e+00,  8.6328e-01,  ..., -4.3047e+00,
           -1.8037e+00, -1.1025e+00],
          ...,
          [ 1.0430e+00, -4.3945e+00, -3.4785e+00,  ..., -5.3711e-01,
            1.5029e+00,  2.4216e-02],
          [-6.2451e-01, -5.1611e-01, -2.6836e+00,  ...,  1.3320e+00,
           -7.1338e-01,  8.2861e-01],
          [-1.2725e+00, -1.7051e+00, -1.9668e+00,  ..., -2.0898e+00,
            1.5811e+00,  3.0859e+00]],

         ...,

         [[ 5.7312e-02, -3.7384e-02, -1.7700e-02,  ..., -1.0767e-01,
           -4.2305e-03,  7.4890e-02],
          [-4.2188e-01,  1.8496e+00,  1.1973e+00,  ..., -1.3447e+00,
            8.4570e-01, -4.9512e-01],
          [ 2.0293e+00,  1.0312e+00,  2.0254e+00,  ...,  2.8223e-01,
            7.1338e-01, -2.1074e+00],
          ...,
          [ 7.4170e-01,  2.2852e+00,  2.2246e+00,  ...,  1.8096e+00,
            1.4316e+00, -9.9243e-02],
          [ 2.6440e-01,  1.1797e+00,  1.3896e+00,  ...,  9.9023e-01,
            2.3621e-02,  1.8799e-01],
          [ 2.0723e+00, -4.1016e+00,  1.2979e+00,  ..., -1.7510e+00,
           -5.0244e-01,  1.7224e-01]],

         [[-1.6992e-01, -1.1041e-01, -5.9845e-02,  ..., -3.9978e-02,
           -9.6191e-02, -1.5869e-02],
          [ 1.1797e+00, -3.4766e+00,  1.1504e+00,  ...,  1.8936e+00,
            5.2490e-01, -2.3613e+00],
          [ 1.5088e-01, -1.0361e+00, -1.1934e+00,  ...,  3.0176e+00,
           -1.9795e+00, -3.0859e+00],
          ...,
          [ 9.9170e-01, -2.8809e+00, -3.2495e-01,  ..., -3.9258e-01,
           -2.7051e+00, -1.8291e+00],
          [ 2.6196e-01, -3.3535e+00, -6.5527e-01,  ..., -1.3896e+00,
           -1.5254e+00, -2.3242e+00],
          [-2.4980e+00, -4.1133e+00, -1.8633e+00,  ...,  1.0098e+00,
            4.3579e-01, -2.3164e+00]],

         [[-5.1727e-02,  2.3682e-01,  2.7808e-01,  ..., -3.9642e-02,
           -1.5393e-01,  1.7883e-01],
          [ 1.5674e+00, -1.2803e+00,  4.2076e-03,  ..., -8.4619e-01,
            2.2930e+00,  3.4414e+00],
          [-6.0938e-01, -3.6108e-01, -9.2712e-02,  ..., -7.0068e-01,
           -3.9111e-01,  2.0898e+00],
          ...,
          [ 4.2578e+00, -1.4238e+00, -5.2637e-01,  ..., -3.8594e+00,
            8.9355e-01,  4.1523e+00],
          [ 1.8564e+00, -1.8213e+00,  5.5469e-01,  ..., -2.6562e+00,
            1.1221e+00,  2.4355e+00],
          [ 1.3213e+00, -1.9834e+00, -1.4805e+00,  ..., -1.2998e+00,
            4.1479e-01,  1.9531e-01]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>), tensor([[[[-0.3318, -0.7964,  1.1426,  ...,  0.7578,  1.7949,  3.4414],
          [-1.3682, -0.9116, -0.4077,  ...,  2.8086,  3.6992,  3.3359],
          [-2.2012, -0.5581,  0.5024,  ...,  0.2659,  2.6152,  1.6670],
          ...,
          [-0.8730, -1.9824,  0.4160,  ..., -0.6621,  1.4062,  0.1567],
          [-2.0645,  1.2393,  3.0996,  ...,  0.0128,  0.4680, -0.6660],
          [-0.7642,  1.9883,  0.4985,  ..., -5.5195,  0.4971,  1.6514]],

         [[ 0.9956, -0.0673, -1.1602,  ..., -0.6138,  0.4656,  0.4775],
          [ 1.7930,  2.9551, -2.6621,  ..., -0.8818, -0.0486,  0.8203],
          [ 2.5879,  0.6470, -1.0527,  ..., -0.4819,  0.1908, -0.3203],
          ...,
          [ 0.7217, -0.6021, -1.6455,  ..., -2.0273,  0.8521, -0.8721],
          [ 1.1572,  1.1748, -0.5044,  ..., -0.6426, -1.0723, -0.3962],
          [-0.1394,  2.2480,  0.3386,  ..., -1.3545, -1.3379, -1.5732]],

         [[ 1.6865,  4.0586, -3.6523,  ..., -0.0608, -0.6748, -0.3584],
          [-1.4355,  1.9023, -3.7051,  ...,  0.7080, -1.0195, -0.0117],
          [-0.0894,  1.7900, -1.7451,  ...,  1.3457,  0.7900, -0.1726],
          ...,
          [ 1.9531,  2.3691,  0.2234,  ..., -2.1348,  1.5986, -1.6240],
          [ 2.2480, -0.0683, -0.0728,  ..., -1.1895,  3.7031,  1.7617],
          [-0.8237, -2.0840,  0.3706,  ...,  0.7578,  0.9053,  0.5430]],

         ...,

         [[-1.6670,  2.0605, -2.2441,  ..., -2.8594, -2.3633, -0.7573],
          [ 0.2478,  0.5708, -2.3301,  ..., -1.8467, -0.7646, -0.1503],
          [ 0.0854, -1.3945, -0.9155,  ..., -1.8701, -0.7949, -0.8281],
          ...,
          [ 1.4756,  0.5020, -3.7012,  ..., -0.5117, -2.8027,  1.5703],
          [-1.5654,  0.0175,  0.6035,  ...,  0.2463,  0.4165,  1.0752],
          [-1.2891, -1.8105,  0.6226,  ...,  2.0781, -0.3789, -0.9019]],

         [[-1.1855,  0.8848, -4.0312,  ...,  1.0410, -0.2461,  0.0305],
          [-1.4131, -0.1764, -1.0576,  ..., -0.0827, -1.7461,  1.2793],
          [-1.0039, -1.3857, -2.1191,  ..., -1.0088, -0.7358,  0.3333],
          ...,
          [ 1.6992, -2.0801,  0.5747,  ..., -2.3750,  2.0508, -3.8691],
          [ 1.4580,  1.0479,  0.5884,  ..., -1.7881,  2.4102, -2.6406],
          [ 0.0162, -0.4954, -0.4390,  ..., -1.1094, -0.9380,  0.3418]],

         [[ 1.8760, -2.5625, -1.3203,  ..., -0.1486,  1.2451, -1.0078],
          [ 2.0977, -3.2949, -0.9629,  ..., -0.9502,  1.5381,  0.2825],
          [ 0.2827, -2.0762, -0.2098,  ...,  1.3730,  2.1914, -0.9678],
          ...,
          [ 1.7070, -2.2090,  1.2832,  ...,  0.5942,  4.8516,  1.1992],
          [ 1.0586, -5.4609,  2.3340,  ..., -1.1025,  2.0527,  1.0625],
          [-0.6504,  6.0547,  1.5830,  ...,  2.5430, -7.0430, -1.0859]]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<TransposeBackward0>), tensor([[[[-4.0391e+00, -2.9941e+00,  1.8145e+00,  ..., -1.3887e+00,
            5.1318e-01,  4.2017e-01],
          [-3.7617e+00, -5.3359e+00, -9.2676e-01,  ..., -2.4570e+00,
            8.8184e-01,  1.2217e+00],
          [-1.5928e+00, -3.8359e+00, -7.1094e-01,  ..., -1.9834e+00,
           -1.3855e-01,  8.1445e-01],
          ...,
          [-1.1426e+00, -2.9434e+00, -2.1348e+00,  ...,  5.4590e-01,
            1.8242e+00, -3.2935e-01],
          [-2.9316e+00, -1.1895e+00,  8.7061e-01,  ...,  1.7900e+00,
            1.4570e+00, -1.1826e+00],
          [-9.0869e-01,  1.0516e-01,  2.1094e-01,  ...,  1.2299e-01,
            1.1127e-01, -1.9421e-01]],

         [[ 3.7207e+00,  4.0796e-01, -3.5352e+00,  ...,  2.5039e+00,
            1.3594e+00,  1.1113e+00],
          [-8.3643e-01,  8.5352e-01,  1.6240e+00,  ...,  2.5176e+00,
           -4.1260e-01, -8.5010e-01],
          [ 1.0850e+00,  2.1875e+00,  1.0342e+00,  ...,  5.2051e-01,
            1.3877e+00, -1.7871e+00],
          ...,
          [ 4.6265e-01,  1.5801e+00, -1.9170e+00,  ...,  2.8076e-01,
            1.3489e-01,  1.1517e-01],
          [-1.4287e+00, -9.4873e-01,  9.1357e-01,  ...,  1.5273e+00,
           -1.0703e+00,  1.0879e+00],
          [-8.8232e-01,  2.8809e-01,  6.7334e-01,  ..., -7.3193e-01,
            1.3443e-02,  5.2002e-02]],

         [[-1.3594e+00,  2.2070e+00, -2.3398e+00,  ...,  5.3672e+00,
           -2.1152e+00,  3.8940e-01],
          [-4.3213e-01,  2.4036e-01, -2.4473e+00,  ..., -1.0654e+00,
            3.3154e-01,  4.2852e+00],
          [-5.4785e-01,  4.0356e-01,  1.1230e+00,  ...,  1.0430e+00,
           -7.6270e-01,  4.5039e+00],
          ...,
          [ 2.4492e+00,  4.6758e+00, -2.3418e+00,  ...,  6.9336e-01,
            3.7549e-01,  5.5938e+00],
          [-7.8857e-01,  2.2812e+00,  1.8525e+00,  ...,  6.8896e-01,
           -3.1787e-01,  4.2891e+00],
          [-1.4170e+00, -1.1270e+00,  2.5317e-01,  ...,  1.9006e-01,
            1.2152e-01, -3.4131e-01]],

         ...,

         [[-1.1260e+00, -8.2568e-01,  2.0859e+00,  ..., -4.6328e+00,
           -1.1279e+00, -1.6199e-01],
          [-1.6504e+00, -2.7148e+00,  6.2305e-01,  ..., -5.0859e+00,
           -7.8857e-01, -2.7871e+00],
          [-5.7715e-01, -1.2637e+00,  1.7383e+00,  ..., -2.2422e+00,
           -1.1748e+00, -1.4766e+00],
          ...,
          [ 8.6768e-01, -4.9854e-01,  8.0518e-01,  ..., -2.8833e-01,
            1.8164e+00, -1.9385e+00],
          [ 1.5332e+00, -5.3008e+00,  5.8496e-01,  ..., -8.2715e-01,
           -1.0785e-01,  3.2676e+00],
          [ 4.7852e-01, -7.7271e-02,  4.7035e-03,  ..., -1.4539e-01,
            9.5154e-02,  2.6733e-01]],

         [[ 7.0361e-01,  1.9092e+00, -4.8516e+00,  ...,  4.3335e-01,
           -2.5171e-01, -5.4297e-01],
          [-7.6709e-01, -4.2773e-01, -4.3672e+00,  ...,  2.4180e+00,
           -2.6562e-01,  1.2354e+00],
          [-2.2441e+00,  1.1494e+00, -8.8965e-01,  ...,  4.9023e+00,
            6.4355e-01,  2.2773e+00],
          ...,
          [-4.7510e-01,  5.4150e-01, -4.3594e+00,  ...,  4.4570e+00,
           -1.5732e+00,  3.4883e+00],
          [ 6.2383e+00, -1.5264e+00,  1.4170e+00,  ..., -1.6394e-01,
            1.5283e+00,  1.2041e+00],
          [ 4.1885e-03, -4.4263e-01,  2.0215e+00,  ..., -1.1895e+00,
            1.8372e-01,  4.8511e-01]],

         [[ 1.1855e+00, -9.8389e-01,  1.0547e+00,  ...,  9.2188e-01,
           -1.5879e+00, -2.0605e+00],
          [ 8.3008e-01,  2.3352e-01,  3.2690e-01,  ..., -3.0137e+00,
           -3.5176e+00,  1.3125e+00],
          [ 1.0176e+00,  1.4922e+00,  1.8945e+00,  ..., -2.7285e+00,
           -1.6191e+00,  1.2012e-01],
          ...,
          [ 2.4475e-01, -1.4075e-01, -1.2295e+00,  ...,  1.4966e-01,
           -9.9512e-01, -1.9258e+00],
          [ 5.0098e-01, -2.1230e+00, -5.5817e-02,  ..., -4.2734e+00,
            8.9453e-01,  1.4395e+00],
          [-3.9307e-02,  3.1665e-01,  3.0249e-01,  ...,  3.9087e-01,
            1.2396e-01, -3.7567e-02]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>)), (tensor([[[[ 2.5244e-01,  4.3262e-01, -1.2617e+00,  ...,  3.2422e-01,
            5.4596e-02,  1.3477e-01],
          [-1.6035e+00, -4.2065e-01,  2.5156e+00,  ..., -3.2178e-01,
           -6.9727e-01,  4.3872e-01],
          [ 8.1299e-01, -8.3252e-01,  2.5918e+00,  ..., -1.9053e+00,
           -1.2520e+00,  6.8018e-01],
          ...,
          [ 1.5215e+00, -1.5112e-01,  3.2422e+00,  ..., -1.3193e+00,
           -3.3057e-01,  1.3984e+00],
          [-8.9062e-01,  8.8379e-01,  3.1719e+00,  ..., -8.5303e-01,
           -9.6484e-01,  1.6084e+00],
          [ 1.1475e+00, -2.4524e-01,  2.7578e+00,  ..., -1.0234e+00,
           -9.9365e-01,  1.2822e+00]],

         [[-8.3496e-01,  5.1331e-02,  1.3257e-01,  ..., -8.2397e-02,
           -9.5947e-02, -1.8237e-01],
          [ 1.8250e-01,  4.0356e-01,  8.7549e-01,  ...,  1.2139e+00,
            1.3806e-01, -1.9275e-01],
          [ 6.8945e-01, -1.6138e-01, -5.6348e-01,  ..., -1.9714e-01,
           -9.8633e-01, -7.3682e-01],
          ...,
          [ 5.2588e-01, -6.5430e-01,  5.7324e-01,  ...,  6.9336e-01,
           -5.2051e-01, -8.6719e-01],
          [ 7.4268e-01, -3.6987e-01,  1.3096e+00,  ...,  9.6924e-01,
           -3.0542e-01, -8.5059e-01],
          [ 1.1201e+00, -7.4280e-02,  4.6606e-01,  ..., -1.5576e-01,
           -1.1367e+00, -1.4490e-01]],

         [[-2.3804e-01, -2.6709e-01,  5.9863e-01,  ..., -1.0956e-01,
           -1.8066e-01, -8.8745e-02],
          [-6.6452e-03, -9.2725e-01, -6.6772e-02,  ..., -5.9570e-01,
           -1.5996e+00, -9.0283e-01],
          [ 4.2041e-01,  1.8726e-01, -1.4685e-01,  ..., -1.8262e+00,
           -8.8672e-01,  3.2104e-01],
          ...,
          [-7.8760e-01,  9.5398e-02, -3.3789e-01,  ..., -5.4248e-01,
           -4.5459e-01, -7.4512e-01],
          [-7.9199e-01, -8.8721e-01,  6.4062e-01,  ...,  1.8506e-01,
           -2.8955e-01, -1.3291e+00],
          [-5.4980e-01,  9.7119e-01,  3.3350e-01,  ..., -6.4746e-01,
            8.0225e-01, -3.9087e-01]],

         ...,

         [[ 3.3008e-01,  2.3962e-01, -5.1117e-02,  ..., -6.0938e-01,
            1.2306e-02,  4.8291e-01],
          [ 7.5928e-01,  2.2734e+00, -5.6299e-01,  ...,  4.1797e-01,
           -1.5967e-01,  1.4137e-02],
          [-2.0642e-01,  2.8015e-02, -5.4004e-01,  ..., -2.0156e+00,
           -1.5515e-01, -1.3440e-01],
          ...,
          [-5.0195e-01, -1.0957e+00,  1.0410e+00,  ..., -9.4971e-01,
           -8.8135e-02,  1.1016e+00],
          [ 3.2715e-01, -7.8857e-01, -2.7930e-01,  ...,  5.4736e-01,
           -1.6870e-01,  1.1694e-01],
          [ 4.8486e-01, -1.4082e+00, -1.1240e+00,  ..., -1.3311e+00,
           -1.5190e-02,  9.2239e-03]],

         [[ 4.6272e-03, -4.8309e-02,  1.0818e-02,  ...,  5.0323e-02,
            8.2703e-02, -8.1787e-01],
          [ 1.0186e+00,  3.0640e-01,  3.3301e-01,  ..., -3.3984e-01,
            2.8107e-02,  8.3594e-01],
          [ 1.4297e+00, -6.3184e-01, -1.0431e-01,  ..., -9.3945e-01,
            5.3516e-01,  7.8076e-01],
          ...,
          [ 9.2407e-02, -5.4004e-01,  7.3547e-02,  ..., -1.7324e+00,
           -1.7646e+00,  9.7107e-02],
          [ 6.4502e-01, -3.2593e-01,  2.3572e-01,  ..., -8.0713e-01,
           -1.0400e+00, -1.2207e+00],
          [ 4.6875e-01, -1.3457e+00,  3.2227e-01,  ..., -6.5552e-02,
           -1.2734e+00, -3.9990e-01]],

         [[ 3.0591e-01, -2.3773e-02,  3.4229e-01,  ..., -3.5327e-01,
           -2.2559e-01, -2.0098e+00],
          [ 3.3252e-01, -7.7100e-01,  1.1566e-01,  ..., -3.0542e-01,
            1.0429e-02,  3.5723e+00],
          [ 1.1133e+00, -3.6792e-01,  1.2119e+00,  ...,  4.1333e-01,
           -9.3408e-01,  4.4531e+00],
          ...,
          [ 4.7534e-01,  8.4473e-01,  1.2451e-01,  ..., -6.5137e-01,
           -1.7871e+00,  5.8789e+00],
          [ 3.0933e-01,  5.1074e-01, -2.3499e-01,  ..., -1.0803e-01,
           -9.9951e-01,  5.9141e+00],
          [ 6.1768e-01,  1.0273e+00, -1.0068e+00,  ..., -1.2920e+00,
           -2.8223e-01,  6.0195e+00]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>), tensor([[[[ 3.4155e-01, -3.2257e-02,  1.0107e-01,  ...,  1.2054e-01,
           -1.5088e-01, -6.2927e-02],
          [-8.0029e-01, -5.8411e-02, -2.6719e+00,  ...,  2.3633e+00,
            2.7695e+00,  9.7803e-01],
          [-1.0674e+00,  3.0859e-01, -6.8994e-01,  ...,  1.3984e+00,
            2.8164e+00, -1.0869e+00],
          ...,
          [-4.3008e+00, -2.6719e+00, -2.4500e-01,  ...,  4.3677e-01,
            3.0312e+00, -4.2969e+00],
          [-2.3672e+00, -1.9346e+00,  6.5869e-01,  ...,  1.8154e+00,
            2.4082e+00, -2.3301e+00],
          [-6.4307e-01, -3.9961e+00,  9.9365e-01,  ..., -2.1992e+00,
            3.0688e-01, -3.6182e-03]],

         [[ 3.3321e-03, -1.7908e-01,  1.6260e-01,  ...,  3.2749e-03,
            1.3208e-01, -2.5806e-01],
          [ 5.8398e-01, -1.2573e-01, -1.0246e-02,  ..., -1.1924e+00,
           -2.8574e+00, -2.6113e+00],
          [-2.3555e+00,  1.0547e+00,  3.0371e+00,  ..., -4.1914e+00,
           -6.3516e+00, -6.9727e-01],
          ...,
          [-3.9199e+00,  1.7627e+00,  3.2559e+00,  ...,  4.2031e+00,
           -3.7129e+00,  3.9764e-02],
          [-2.6328e+00, -2.9907e-01,  1.1846e+00,  ...,  3.2852e+00,
           -2.1016e+00, -3.4888e-01],
          [-1.5586e+00,  3.0713e-01,  4.0039e+00,  ...,  1.2900e+00,
           -3.2363e+00,  1.3794e-01]],

         [[-1.3977e-01,  2.4866e-01, -2.9785e-01,  ..., -8.3679e-02,
           -3.2007e-01, -4.9255e-02],
          [ 2.8281e+00,  6.5137e-01,  4.1650e-01,  ...,  7.7441e-01,
           -9.4531e-01,  9.4678e-01],
          [ 1.6875e+00,  2.4570e+00, -1.8105e+00,  ...,  2.8926e+00,
           -4.8706e-01, -1.0361e+00],
          ...,
          [ 1.6211e+00, -9.5605e-01, -2.6172e-01,  ...,  4.6055e+00,
            5.0439e-01,  1.3916e+00],
          [ 2.6992e+00, -2.2656e+00, -1.9995e-01,  ...,  4.2266e+00,
           -8.2959e-01, -3.0713e-01],
          [ 1.8535e+00, -3.8008e+00, -1.0068e+00,  ...,  1.9248e+00,
           -1.3037e+00, -9.5361e-01]],

         ...,

         [[ 1.0510e-01,  3.9581e-02,  1.4941e-01,  ..., -2.4231e-02,
           -7.3730e-02,  5.2246e-02],
          [ 1.8096e+00, -2.0645e+00,  3.0566e-01,  ..., -1.0010e+00,
            6.8115e-01,  2.5801e+00],
          [ 1.4336e+00, -1.3336e-02,  1.2129e+00,  ..., -4.5977e+00,
            1.9619e+00, -1.6416e+00],
          ...,
          [-5.4961e+00, -2.6055e+00,  2.1152e+00,  ..., -1.5195e+00,
            3.6646e-01,  5.2197e-01],
          [-5.5420e-01, -5.1660e-01,  1.3350e+00,  ..., -2.9121e+00,
            1.8271e+00, -2.3008e+00],
          [ 3.0215e+00,  8.7061e-01,  2.8281e+00,  ...,  2.7246e+00,
            1.3535e+00,  1.6768e+00]],

         [[ 3.6072e-02,  4.3750e-01, -3.1421e-01,  ..., -2.6221e-01,
           -1.0185e-02,  1.7603e-01],
          [-1.5420e+00, -2.0645e+00, -3.5547e-01,  ...,  4.0991e-01,
            1.0773e-01,  2.6816e+00],
          [ 1.1729e+00, -3.7090e+00,  2.5537e-01,  ..., -4.8281e+00,
           -9.3994e-02,  8.0469e-01],
          ...,
          [-1.1904e+00,  8.1543e-02, -8.7402e-02,  ...,  1.7979e+00,
           -8.7793e-01,  3.0918e+00],
          [-1.7783e+00,  9.9268e-01, -6.9141e-01,  ...,  2.2148e+00,
           -7.9199e-01,  1.3555e+00],
          [-8.8916e-01, -9.4287e-01, -2.0645e+00,  ...,  1.2168e+00,
           -9.6875e-01, -6.6016e-01]],

         [[-2.0581e-01, -3.8232e-01,  2.8271e-01,  ..., -2.1399e-01,
           -1.6089e-01, -1.0828e-01],
          [-1.1163e-01, -3.0591e-01,  2.3730e+00,  ..., -1.9951e+00,
           -1.3252e+00,  8.7207e-01],
          [ 5.7764e-01, -8.8477e-01,  2.6016e+00,  ..., -1.2646e+00,
           -2.2598e+00,  3.9600e-01],
          ...,
          [-1.9326e+00,  4.7241e-01, -1.5566e+00,  ...,  6.4331e-02,
            1.9043e+00, -3.9154e-02],
          [-1.4844e+00,  9.1943e-01, -2.2656e+00,  ..., -1.5615e+00,
            1.3076e+00,  1.3416e-01],
          [ 2.2012e+00,  2.8535e+00, -2.5635e-01,  ..., -1.0713e+00,
            1.7910e+00,  1.3359e+00]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>), tensor([[[[ 1.9690e-01, -1.1074e+00, -1.8652e+00,  ..., -2.3750e+00,
           -5.6610e-02,  5.0928e-01],
          [-4.8169e-01, -3.5859e+00,  9.6375e-02,  ..., -2.7207e+00,
            6.0693e-01, -2.2324e+00],
          [-1.1152e+00, -1.2842e+00, -9.3613e-03,  ..., -2.5547e+00,
           -7.7930e-01, -2.4561e-01],
          ...,
          [ 3.1519e-01,  4.0137e-01,  5.3174e-01,  ...,  1.3203e+00,
           -1.4590e+00, -1.4172e-01],
          [ 3.7598e+00,  1.2031e+00, -3.8910e-02,  ..., -5.2051e-01,
           -3.0176e-01,  5.0781e-01],
          [-1.1240e+00,  1.5117e+00,  1.0193e-01,  ..., -3.6328e-01,
           -7.1143e-01, -8.8330e-01]],

         [[-2.9517e-01,  1.6035e+00, -3.3789e-01,  ..., -2.1934e+00,
            7.5195e-01, -3.0176e+00],
          [-5.4047e-02,  2.6172e+00, -3.4727e+00,  ..., -1.3398e+00,
           -3.7256e-01, -1.5010e+00],
          [-3.0136e-02,  1.4189e+00,  3.0212e-02,  ..., -1.6074e+00,
           -7.6758e-01, -1.9482e+00],
          ...,
          [ 1.5908e+00,  2.0801e+00,  1.5762e+00,  ..., -1.7617e+00,
            1.4685e-01, -2.4062e+00],
          [ 2.6641e+00,  1.3994e+00, -7.7051e-01,  ..., -5.8685e-02,
            2.1562e+00, -5.1514e-01],
          [ 4.8755e-01, -5.0508e+00, -5.1025e-01,  ..., -3.2764e-01,
           -8.4570e-01,  1.1992e+00]],

         [[-2.9434e+00, -2.1035e+00, -1.0723e+00,  ...,  9.2090e-01,
            5.4492e-01, -1.3457e+00],
          [-3.6895e+00, -2.1309e+00, -1.5601e-01,  ...,  1.4209e-01,
           -6.0034e-04,  2.0957e+00],
          [-3.3203e+00, -1.5400e+00, -1.1592e+00,  ...,  3.3276e-01,
           -9.7852e-01,  1.0762e+00],
          ...,
          [-3.6094e+00, -1.9902e+00, -3.5718e-01,  ...,  1.6387e+00,
           -1.3633e+00, -3.9502e-01],
          [-2.2676e+00, -6.7822e-01,  5.2148e-01,  ...,  9.9707e-01,
           -8.8196e-02, -8.5596e-01],
          [ 1.9375e+00,  7.8076e-01, -3.9478e-01,  ..., -9.9072e-01,
            1.5234e+00, -1.4424e+00]],

         ...,

         [[ 3.8818e-01,  8.5449e-01, -1.6865e+00,  ...,  4.3774e-01,
           -1.5498e+00,  3.7476e-01],
          [ 3.1348e-01,  1.0938e+00, -2.8867e+00,  ..., -8.9697e-01,
           -1.2627e+00,  2.5723e+00],
          [ 4.3750e-01,  1.1445e+00, -3.7915e-01,  ..., -2.7490e-01,
           -5.1367e-01,  1.8047e+00],
          ...,
          [ 7.1680e-01, -1.3848e+00, -8.7402e-02,  ...,  1.1328e+00,
            2.3438e+00, -7.5012e-02],
          [-2.7070e+00, -3.8281e-01,  1.0977e+00,  ..., -6.0547e-01,
            6.0352e-01, -2.0581e-01],
          [ 1.5439e+00,  2.7271e-01,  6.4844e-01,  ..., -4.0137e-01,
            4.1309e-01, -2.0469e+00]],

         [[ 3.5034e-01,  1.6133e+00,  3.6316e-02,  ..., -2.3059e-01,
           -7.2510e-01,  2.2559e+00],
          [ 1.1982e+00,  2.8613e+00, -2.7598e+00,  ...,  1.9697e+00,
            1.5225e+00,  2.3379e+00],
          [ 3.3862e-01,  1.7773e+00, -1.5830e+00,  ..., -7.0312e-01,
           -3.5913e-01,  3.5801e+00],
          ...,
          [-3.0078e-01,  4.4946e-01,  1.2646e+00,  ...,  6.9434e-01,
           -4.0845e-01,  1.0713e+00],
          [-2.2812e+00,  2.0508e+00, -1.7852e+00,  ...,  7.3682e-01,
           -1.3643e+00,  2.3027e+00],
          [ 4.1138e-01, -5.4004e-01,  1.7969e+00,  ...,  1.3848e+00,
           -3.9453e-01,  8.5547e-01]],

         [[-7.2266e-01,  2.6328e+00, -2.0215e-01,  ...,  4.9048e-01,
            2.4629e+00, -2.6172e+00],
          [-8.8037e-01,  2.0156e+00, -1.0735e-02,  ...,  1.4453e+00,
            4.1172e+00, -2.5254e+00],
          [-1.0537e+00,  1.4033e+00, -2.9495e-02,  ...,  2.4719e-01,
            2.1543e+00, -1.7090e+00],
          ...,
          [ 1.9043e-01, -9.8926e-01, -1.4307e-01,  ...,  7.6025e-01,
            2.5039e+00, -1.5234e+00],
          [ 3.1465e+00, -1.1689e+00, -1.3320e+00,  ...,  2.6641e+00,
            3.2007e-01, -1.7773e+00],
          [-3.2544e-01, -1.3438e+00, -2.0723e+00,  ...,  8.3679e-02,
           -5.7852e+00,  1.9131e+00]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>), tensor([[[[-3.5605e+00,  3.4473e+00, -9.9219e-01,  ..., -2.7598e+00,
           -7.3096e-01, -1.3701e+00],
          [-6.2988e-01, -1.8877e+00, -2.4883e+00,  ...,  2.1602e+00,
            2.8320e+00, -3.7061e-01],
          [-8.6572e-01,  2.4878e-01, -1.5117e+00,  ...,  5.1465e-01,
            4.5117e+00, -1.2305e-01],
          ...,
          [-1.5869e+00,  7.5195e-01, -1.6760e-01,  ..., -2.6123e-02,
            2.3262e+00,  2.1289e+00],
          [-1.8330e+00,  8.1482e-02,  1.3770e+00,  ..., -4.3869e-03,
            3.0103e-01,  8.6426e-01],
          [-2.8760e-01, -4.4849e-01, -1.0358e-01,  ..., -9.0625e-01,
           -3.1586e-02,  7.1960e-02]],

         [[ 2.8125e+00, -2.7461e+00, -2.6699e+00,  ..., -3.8535e+00,
            1.3013e-01,  3.1699e+00],
          [ 2.0176e+00,  8.4668e-01,  1.7549e+00,  ..., -1.1924e+00,
            5.2686e-01, -3.8184e+00],
          [-1.2090e+00, -2.4963e-01, -4.8926e-01,  ..., -2.5254e+00,
            4.4580e-01, -2.8633e+00],
          ...,
          [-9.4922e-01, -3.2013e-02, -4.0039e+00,  ..., -2.5762e+00,
            2.2012e+00,  1.5566e+00],
          [ 4.0352e+00,  1.2803e+00, -2.1934e+00,  ..., -4.0747e-01,
            5.5977e+00,  1.7212e-01],
          [-4.4525e-02, -2.3145e-01, -1.5466e-01,  ...,  6.8896e-01,
           -4.6655e-01, -2.6733e-01]],

         [[ 2.5463e-04,  1.0713e+00,  1.6787e+00,  ..., -1.0400e+00,
           -2.4102e+00,  3.4844e+00],
          [-1.1992e+00, -2.7930e+00, -6.1523e-01,  ...,  5.4297e+00,
           -9.0723e-01,  1.8164e+00],
          [-3.9014e-01, -1.6455e+00,  4.0967e-01,  ...,  3.0039e+00,
           -1.7354e+00,  1.6328e+00],
          ...,
          [ 2.3594e+00,  1.9512e+00,  4.6641e+00,  ..., -1.4331e-01,
           -3.6172e+00,  8.5547e-01],
          [-3.3633e+00,  1.5635e+00,  2.0312e+00,  ...,  3.2207e+00,
           -2.1953e+00,  1.1221e+00],
          [-5.3809e-01,  6.9824e-01,  9.8896e-04,  ...,  2.0691e-01,
           -1.2705e+00, -3.9087e-01]],

         ...,

         [[-1.5264e+00, -2.2500e+00,  9.7168e-01,  ...,  3.1201e-01,
            4.6094e+00,  2.0137e+00],
          [-4.9707e-01, -2.5410e+00, -1.9214e-01,  ...,  2.0352e+00,
            1.6348e+00,  4.6655e-01],
          [-3.9668e+00, -3.3066e+00,  2.8540e-01,  ...,  1.9307e+00,
            1.5000e+00, -1.8936e+00],
          ...,
          [-1.4863e+00, -6.8701e-01,  1.5918e+00,  ...,  7.3242e-01,
            1.8857e+00, -3.1665e-01],
          [ 2.8086e+00, -1.7969e+00,  3.3613e+00,  ...,  1.7568e+00,
            2.9609e+00,  3.6875e+00],
          [ 2.7374e-02,  7.4170e-01,  1.6345e-01,  ...,  1.2054e-01,
           -1.2000e-01, -2.1851e-01]],

         [[ 3.1885e-01,  2.3457e+00,  1.7744e+00,  ...,  8.4668e-01,
           -4.4609e+00,  4.9878e-01],
          [-6.7031e+00,  3.6055e+00,  3.1992e+00,  ..., -3.6875e+00,
           -2.8906e+00, -3.5840e+00],
          [-2.7539e+00, -1.2812e+00, -1.4668e+00,  ..., -4.5349e-02,
           -2.8652e+00, -1.6797e+00],
          ...,
          [-9.2920e-01, -9.9487e-02, -2.8203e+00,  ..., -3.8086e-02,
           -1.5297e-02, -3.0059e+00],
          [-3.0840e+00,  2.3359e+00,  1.9202e-01,  ..., -2.5488e-01,
           -3.1738e+00, -3.5977e+00],
          [-4.0710e-02, -6.5381e-01, -5.4102e-01,  ..., -5.2295e-01,
            2.4463e-01, -4.5581e-01]],

         [[ 8.4326e-01,  2.2793e+00, -1.1055e+00,  ..., -1.3687e-02,
           -4.7632e-01, -1.2568e+00],
          [-2.3164e+00, -1.4150e+00,  1.9541e+00,  ..., -1.2715e+00,
            8.9340e-03, -1.0615e+00],
          [-1.4229e+00,  1.8613e+00, -4.7412e-01,  ..., -1.8457e+00,
           -1.1182e+00, -3.3438e+00],
          ...,
          [ 3.3960e-01, -6.5332e-01, -1.2832e+00,  ..., -5.1221e-01,
           -2.6602e+00, -3.5645e+00],
          [-1.0762e+00, -2.2793e+00,  9.3311e-01,  ...,  1.4355e+00,
           -4.2578e+00, -7.4463e-02],
          [-1.1652e-01,  1.1887e-02,  1.0931e-01,  ..., -6.0547e-01,
           -3.9124e-02,  6.5186e-01]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>)), (tensor([[[[-1.2054e-01,  1.3389e+00,  2.0532e-01,  ...,  1.3281e-01,
           -1.0132e-01,  1.9617e-01],
          [ 4.7241e-01, -2.6562e+00,  5.2588e-01,  ...,  1.7441e+00,
            3.7769e-01, -9.4434e-01],
          [-2.3035e-01, -1.9629e+00, -1.1572e+00,  ..., -5.1514e-01,
            1.0830e+00, -6.3428e-01],
          ...,
          [-8.7793e-01, -2.3594e+00, -5.1074e-01,  ...,  2.4438e-01,
            1.0020e+00, -7.8369e-01],
          [ 1.0078e+00, -2.2305e+00,  4.5654e-01,  ...,  6.2842e-01,
           -4.3018e-01, -1.0283e+00],
          [ 1.6035e+00, -2.1094e+00, -2.0020e+00,  ...,  7.0557e-01,
           -1.0879e+00, -1.6328e+00]],

         [[-1.7539e+00,  1.3403e-01,  1.5759e-01,  ...,  2.6587e-01,
           -3.4497e-01,  3.6102e-02],
          [ 3.4609e+00,  3.4180e-01,  1.9031e-01,  ...,  1.5479e-01,
           -4.2261e-01,  1.4443e+00],
          [ 3.0723e+00,  1.6174e-01,  2.2583e-03,  ..., -2.0190e-01,
           -4.6167e-01,  6.4453e-01],
          ...,
          [ 2.3887e+00,  1.8867e+00,  1.6748e-01,  ...,  5.9863e-01,
            5.0830e-01,  1.4102e+00],
          [ 3.3379e+00,  1.3008e+00, -6.1475e-01,  ...,  9.7119e-01,
            1.2373e+00,  1.3164e+00],
          [ 2.0664e+00,  2.0605e+00,  1.1650e+00,  ..., -5.9204e-02,
            4.0625e-01,  1.1709e+00]],

         [[-4.0234e-01,  4.3945e-01,  2.6016e-02,  ...,  7.9224e-02,
           -3.7866e-01,  2.5220e-01],
          [-1.9775e-01, -1.2188e+00, -5.9540e-02,  ...,  8.4229e-02,
            1.8103e-01, -7.0068e-01],
          [ 1.3779e+00, -5.3906e-01, -8.0908e-01,  ..., -6.8652e-01,
            1.3555e+00, -2.8638e-01],
          ...,
          [ 1.4639e+00, -4.6313e-01,  1.4033e+00,  ...,  1.2900e+00,
           -5.1172e-01, -1.0576e+00],
          [ 7.1191e-01, -9.1357e-01,  8.0225e-01,  ...,  1.3047e+00,
            5.7487e-03, -1.4248e+00],
          [ 1.4111e+00, -2.2156e-01,  9.9414e-01,  ...,  1.8018e+00,
           -1.1357e+00, -5.5225e-01]],

         ...,

         [[ 6.5723e-01, -2.3279e-01,  3.8794e-01,  ..., -4.8926e-01,
           -1.2195e-01,  6.5491e-02],
          [-1.6572e+00, -1.0820e+00,  2.2539e+00,  ...,  1.7529e+00,
            5.4932e-01, -1.4609e+00],
          [-8.5254e-01, -8.8525e-01,  1.5173e-01,  ...,  4.6826e-01,
           -1.2832e+00,  4.4214e-01],
          ...,
          [-1.8184e+00, -4.7290e-01,  2.5854e-01,  ...,  5.6592e-01,
            8.4595e-02,  1.6494e+00],
          [-1.2383e+00, -7.2510e-01,  1.7461e+00,  ...,  1.5869e+00,
           -1.1316e-01,  4.9780e-01],
          [-4.0332e-01,  2.1887e-01, -2.1055e+00,  ...,  3.5083e-01,
           -1.9932e+00,  5.1318e-01]],

         [[-7.4158e-02, -8.9844e-02,  7.6538e-02,  ...,  1.5955e-01,
           -1.2354e+00,  3.0713e-01],
          [ 6.4404e-01,  9.4092e-01,  7.2144e-02,  ..., -5.3125e-01,
            2.3965e+00,  4.6289e-01],
          [ 8.1689e-01,  6.7480e-01,  7.3584e-01,  ..., -2.6807e-01,
            2.4980e+00, -5.7129e-01],
          ...,
          [ 1.1055e+00,  1.2441e+00,  5.8203e-01,  ..., -6.8896e-01,
            2.2988e+00, -1.3904e-01],
          [ 1.1045e+00,  9.5508e-01, -5.0720e-02,  ..., -4.7803e-01,
            2.9863e+00,  7.3975e-01],
          [ 1.0557e+00, -3.0365e-02, -6.8506e-01,  ..., -6.5430e-01,
            2.6621e+00, -8.0139e-02]],

         [[-4.0192e-02,  8.4229e-01, -1.9153e-01,  ...,  1.3877e+00,
            5.0323e-02, -1.7920e-01],
          [ 3.6841e-01,  1.1719e+00,  7.9883e-01,  ..., -5.0586e-01,
           -1.7715e+00,  1.3418e+00],
          [-3.8379e-01,  1.5664e+00, -4.6704e-01,  ..., -1.3682e+00,
           -7.3047e-01,  1.2344e+00],
          ...,
          [ 7.2461e-01,  5.2881e-01,  2.8400e-03,  ...,  7.9407e-02,
            9.4287e-01, -8.9453e-01],
          [ 4.3188e-01,  1.3318e-01,  8.7842e-01,  ..., -5.7892e-02,
            1.5625e+00,  1.5015e-01],
          [ 6.9629e-01,  8.0615e-01,  7.4414e-01,  ..., -4.8730e-01,
            1.1426e+00, -1.6865e+00]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>), tensor([[[[ 1.5259e-02, -3.1909e-01, -2.0264e-01,  ...,  1.1310e-01,
            1.0687e-01,  1.6980e-01],
          [ 1.9775e-02,  3.5469e+00, -1.8867e+00,  ..., -8.9014e-01,
           -2.8540e-01, -1.0156e+00],
          [ 6.2305e-01,  7.3340e-01, -1.6426e+00,  ..., -1.6973e+00,
            2.6172e+00, -2.0215e+00],
          ...,
          [-2.5547e+00,  1.9375e+00, -3.3828e+00,  ..., -1.5938e+00,
           -5.1094e+00,  7.2119e-01],
          [-1.7520e+00,  2.0234e+00, -3.9160e+00,  ..., -2.8369e-01,
           -3.6836e+00,  3.7573e-01],
          [ 1.1572e+00,  2.7588e-01, -2.8867e+00,  ..., -2.5508e+00,
           -1.6035e+00, -1.5166e+00]],

         [[-5.5481e-02, -9.7534e-02, -1.1023e-01,  ...,  3.3472e-01,
           -5.4541e-01, -1.7957e-01],
          [-2.5918e+00, -5.1074e-01,  2.3750e+00,  ...,  1.9248e+00,
            2.7634e-02,  1.0449e+00],
          [-7.8076e-01,  3.9727e+00,  5.0625e+00,  ..., -1.7549e+00,
            7.2705e-01,  6.6699e-01],
          ...,
          [ 1.5020e+00, -2.5527e+00, -1.5596e+00,  ..., -4.7852e+00,
           -3.3722e-02,  2.2637e+00],
          [-8.4131e-01, -2.2148e+00, -4.7705e-01,  ..., -2.1973e+00,
            4.3066e-01,  2.8164e+00],
          [ 3.2544e-01, -1.1786e-01, -3.4814e-01,  ..., -1.3867e+00,
           -2.4316e+00, -1.7090e-01]],

         [[-2.7490e-01,  1.6748e-01,  3.2373e-01,  ..., -1.9751e-01,
            1.4087e-01, -1.4478e-01],
          [ 2.2715e+00,  1.6006e+00, -1.4229e+00,  ..., -2.2988e+00,
            6.5381e-01, -7.9395e-01],
          [ 3.0884e-01, -1.4180e+00,  1.5176e+00,  ..., -1.2100e+00,
           -3.0371e-01,  1.6084e+00],
          ...,
          [-1.1611e+00, -2.3496e+00,  3.8403e-01,  ..., -8.1875e+00,
            7.9395e-01,  1.1562e+00],
          [-3.6230e-01, -9.7998e-01, -6.5771e-01,  ..., -5.3047e+00,
           -1.7725e-01, -3.1152e-01],
          [-2.3750e+00, -2.4231e-02,  1.1660e+00,  ..., -4.9727e+00,
            4.3945e+00, -8.6816e-01]],

         ...,

         [[-2.3499e-02,  2.7930e-01,  3.1543e-01,  ..., -9.4116e-02,
           -3.6475e-01, -3.0762e-01],
          [ 3.0005e-01,  1.7148e+00, -5.3027e-01,  ..., -3.2251e-01,
           -4.6523e+00,  4.9492e+00],
          [-1.6279e+00, -7.7734e-01,  1.7061e+00,  ...,  1.0244e+00,
           -6.0977e+00, -2.2422e+00],
          ...,
          [ 1.3398e+00,  3.6270e+00,  2.4023e+00,  ...,  1.8564e+00,
           -1.8828e+00, -3.5034e-01],
          [ 2.9150e-01,  4.9512e-01,  1.0283e+00,  ...,  1.5889e+00,
           -1.5098e+00,  4.8945e+00],
          [ 2.1348e+00, -3.5938e-01,  3.1299e-01,  ...,  2.5156e+00,
           -4.5039e+00,  3.1274e-01]],

         [[-4.4092e-01, -7.2803e-01, -2.9346e-01,  ...,  5.2344e-01,
           -9.6533e-01, -8.6279e-01],
          [-6.8213e-01, -1.3643e+00,  2.2129e+00,  ...,  5.0293e-01,
           -1.5928e+00, -3.5205e-01],
          [-9.7266e-01,  6.1914e-01, -1.4551e+00,  ..., -7.3682e-01,
           -8.9258e-01,  1.1250e+00],
          ...,
          [ 3.1348e+00,  9.1016e-01,  1.8936e+00,  ..., -4.1055e+00,
           -4.8477e+00,  5.7422e-01],
          [ 3.8477e+00,  4.4238e-01,  3.0059e+00,  ..., -3.7559e+00,
           -4.0977e+00, -5.9668e-01],
          [ 1.3398e+00,  1.7559e+00,  1.2480e+00,  ..., -1.7197e+00,
           -2.8672e+00, -3.1758e+00]],

         [[-1.6882e-01,  5.4016e-02, -3.1445e-01,  ...,  2.2302e-01,
            1.3574e-01,  6.8512e-03],
          [-1.7715e+00,  1.3916e+00, -1.4307e-01,  ...,  1.0215e+00,
            2.5039e+00, -3.1582e+00],
          [-8.9600e-01,  9.0918e-01, -3.8496e+00,  ...,  3.7656e+00,
           -2.4805e+00, -2.3008e+00],
          ...,
          [ 2.0935e-01, -2.5610e-01, -1.9854e+00,  ...,  3.9624e-01,
            6.6846e-01,  6.3965e-01],
          [-4.1719e+00, -3.6816e-01, -2.8833e-01,  ...,  2.9321e-01,
            2.5449e+00, -1.3018e+00],
          [-4.4609e+00, -4.1797e+00, -2.2832e+00,  ...,  2.1543e+00,
            1.5049e+00, -1.0303e+00]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>), tensor([[[[-0.0540, -1.8711,  0.6265,  ..., -0.8076,  2.4062,  3.5742],
          [ 0.4551, -1.0840, -0.1476,  ..., -0.3481, -1.2275,  2.1602],
          [-1.4922,  0.9946,  0.5669,  ...,  0.1284, -0.0962,  3.3770],
          ...,
          [ 0.6665, -1.6416,  0.0339,  ...,  0.7329,  0.0464,  1.7812],
          [ 0.4031, -0.8223,  0.2551,  ..., -0.5620, -2.0547,  0.7456],
          [-2.2402,  2.0293, -2.5625,  ..., -0.9189,  0.2512,  1.1299]],

         [[-0.9805,  2.7891, -4.2773,  ...,  0.3022,  2.8047, -1.8203],
          [-2.0820,  2.9570, -0.3076,  ...,  0.6260,  2.1133, -0.4956],
          [-0.3257,  2.5371, -2.5957,  ...,  0.7974,  3.8008, -1.6914],
          ...,
          [ 1.5791,  1.7324, -1.5049,  ..., -1.1465,  0.7788, -1.0957],
          [ 1.9502,  0.7324, -1.4385,  ..., -1.4902,  1.4121,  1.4189],
          [-0.3462, -0.3428,  3.2051,  ...,  0.6758, -2.1855,  1.0820]],

         [[-1.7930, -0.9604,  2.7637,  ..., -1.6533, -0.5464,  0.6143],
          [-1.0371, -1.1094,  0.7441,  ..., -3.3164,  0.4226,  0.6860],
          [-1.1387,  1.2246,  1.2842,  ..., -1.8457,  1.1289,  0.8442],
          ...,
          [ 0.9346,  1.6729,  2.0547,  ..., -1.5137, -0.4387,  0.2046],
          [ 0.2856,  1.0762,  0.2223,  ..., -1.0752,  1.3750,  1.9648],
          [ 0.7100, -0.7378, -1.8018,  ...,  1.7793,  3.0723,  0.8389]],

         ...,

         [[-1.8076,  0.5610,  0.5107,  ...,  4.0156, -1.8594,  4.5898],
          [-3.0059,  0.1836,  0.8911,  ...,  2.1211, -1.4756,  3.0703],
          [-1.1338,  0.2222,  0.4917,  ...,  0.8555,  1.7520,  3.0469],
          ...,
          [-1.3906, -1.1533,  0.2979,  ..., -1.1377, -0.1909,  2.8438],
          [ 1.9092,  0.1267,  2.2324,  ...,  0.7036,  1.1182,  0.1656],
          [ 1.6875, -0.7700,  0.7095,  ..., -1.7002,  3.5215, -2.5996]],

         [[ 0.2258,  1.0693, -0.3352,  ..., -1.0557,  0.4521,  0.3662],
          [-2.3887,  2.3164,  0.1083,  ..., -0.0229,  0.4043,  3.5508],
          [-1.3291,  0.8623,  1.8096,  ...,  0.6030,  1.1846,  0.6729],
          ...,
          [ 1.6377, -0.7183, -0.0745,  ..., -1.4062,  1.3496, -1.3906],
          [ 0.7383, -2.8047, -0.2822,  ..., -0.8940,  1.6953, -1.4307],
          [-0.2676, -5.8516,  0.2375,  ...,  0.0740,  1.5947, -1.5059]],

         [[-0.0463, -1.0127,  0.4944,  ...,  0.8418,  2.2344, -1.6064],
          [-0.1186, -0.2271,  2.0898,  ..., -1.0078,  0.5933, -1.4492],
          [-0.2639, -1.1016,  0.3352,  ..., -1.7148,  1.2598, -3.0762],
          ...,
          [-0.3521,  0.3049, -0.3462,  ...,  1.2422,  3.0469,  0.4548],
          [-1.0576, -0.8555, -0.3293,  ...,  0.8667, -0.4832, -0.6938],
          [-2.3574,  2.4375, -5.8555,  ..., -1.8477, -4.0859, -1.1191]]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<TransposeBackward0>), tensor([[[[-0.5874,  2.0996, -1.5527,  ...,  0.0086,  2.0762, -2.5352],
          [ 0.1873,  2.0078, -0.8379,  ...,  0.6992, -0.2164, -2.8750],
          [-0.2271,  3.1797,  1.5576,  ..., -0.2103,  0.9780, -1.3301],
          ...,
          [ 0.6870,  1.5801,  2.4688,  ...,  0.3914, -0.6904, -0.0920],
          [-4.2148,  3.6191, -5.4180,  ..., -7.2930, -2.4434,  3.8477],
          [-0.0076,  0.6499, -0.5840,  ..., -0.1918, -0.6626, -0.7861]],

         [[-3.3633,  1.9531,  2.0039,  ..., -1.7412, -4.1172,  4.3555],
          [-3.2285,  5.6172, -6.4023,  ..., -0.0266, -2.8242,  6.3594],
          [-2.6152,  0.8643, -1.6689,  ..., -0.1609, -0.5000, -1.9854],
          ...,
          [-5.3164,  3.7070,  0.3621,  ..., -0.5801,  1.1025,  2.8945],
          [-0.1620,  2.4766,  1.3916,  ..., -4.3945,  2.8945,  3.8438],
          [ 0.1417, -0.2408,  0.1039,  ...,  0.7031, -1.2227, -0.4983]],

         [[ 2.5566, -0.7246,  4.1133,  ...,  1.5986,  3.5098,  0.3621],
          [-0.6611, -2.6113,  1.6631,  ...,  0.6025,  5.9180,  1.8535],
          [-2.0410, -0.6226,  0.1133,  ..., -2.7012,  2.6953,  1.2168],
          ...,
          [ 0.2993, -3.4297, -4.2383,  ..., -0.0792, -0.5659, -0.5913],
          [ 1.7119, -3.7676,  2.1992,  ...,  3.8535,  2.6504,  1.0742],
          [ 0.5630,  0.4055,  0.1964,  ..., -0.0403,  0.6333,  1.5977]],

         ...,

         [[-2.2246,  5.8711,  2.9531,  ..., -2.8535,  2.8848, -3.8027],
          [-2.7461,  1.0615,  0.9038,  ..., -1.0215,  1.6094,  4.0039],
          [ 0.9307,  1.0146, -3.8555,  ...,  0.4592, -2.5957,  1.1338],
          ...,
          [-0.0605,  2.0078, -2.4922,  ..., -6.0078, -1.0879, -0.7168],
          [-1.5576, -3.9141, -0.7202,  ..., -7.1875,  3.6250,  2.8574],
          [-0.0266, -1.3105, -0.2617,  ...,  0.5688,  0.0214,  0.3269]],

         [[-4.8789,  1.2578, -2.2461,  ..., -2.2168, -1.1299, -1.9229],
          [-2.7051, -3.1992,  3.4531,  ...,  0.5083, -1.8984, -2.0488],
          [-0.4285, -0.3975, -0.9829,  ...,  0.5400,  0.0812, -4.0469],
          ...,
          [-1.2402, -0.5464, -1.4053,  ..., -1.2852, -0.4963, -5.0859],
          [-0.1720, -2.3047, -2.7930,  ..., -3.4922,  6.2266,  2.9375],
          [ 1.2832,  0.7339, -1.2305,  ..., -0.5049,  1.4482,  0.4497]],

         [[-0.8271, -0.8721,  6.5742,  ...,  4.2734,  2.1406,  0.2390],
          [ 5.0352, -4.8477,  2.5859,  ...,  4.1055,  6.3164, -0.4260],
          [ 2.2188, -0.4143,  3.5938,  ...,  3.9961,  2.0410,  2.5020],
          ...,
          [ 0.5674, -0.5581,  2.9082,  ...,  2.5781,  2.2422,  1.9355],
          [-3.5000,  1.6084,  4.1055,  ...,  0.7573,  2.0566, -0.9648],
          [ 0.1530, -0.1307, -0.6484,  ..., -1.2119, -0.3694, -0.8262]]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<TransposeBackward0>)), (tensor([[[[-3.1738e-02, -8.7305e-01,  4.9805e-01,  ...,  6.8481e-02,
            1.4795e-01, -1.5698e-01],
          [ 5.0293e-01, -1.9941e+00,  1.1895e+00,  ...,  7.7148e-02,
           -2.7319e-01,  7.0166e-01],
          [-1.7236e-01, -1.4395e+00,  3.5586e+00,  ..., -3.5547e-01,
           -5.4297e-01, -2.4512e-01],
          ...,
          [-1.0527e+00, -1.0977e+00,  2.1367e+00,  ..., -2.2335e-03,
            9.8389e-01,  4.5557e-01],
          [-9.7949e-01, -1.1553e+00,  1.1777e+00,  ..., -4.9976e-01,
           -6.6455e-01,  9.4287e-01],
          [ 4.6167e-01, -1.2324e+00,  1.2324e+00,  ...,  2.4194e-01,
           -5.4102e-01,  3.4497e-01]],

         [[-1.3647e-01,  7.5928e-01,  1.6223e-01,  ..., -5.4639e-01,
           -5.4004e-01, -2.0593e-01],
          [-1.1562e+00,  4.0991e-01, -1.0950e-01,  ..., -8.4473e-01,
           -5.4395e-01, -2.9614e-01],
          [-9.4336e-01,  1.3135e+00,  1.1497e-02,  ..., -7.5537e-01,
           -1.7979e+00,  5.1758e-01],
          ...,
          [ 5.4785e-01, -4.8633e-01, -3.2861e-01,  ..., -1.6484e+00,
           -4.0991e-01, -9.8438e-01],
          [-9.5020e-01,  1.7563e-02,  4.4531e-01,  ..., -9.5264e-01,
           -1.7285e-01, -1.0596e+00],
          [-8.7207e-01,  1.3037e+00, -1.7419e-01,  ..., -6.3916e-01,
           -1.1494e+00,  3.9722e-01]],

         [[ 1.8335e-01,  4.1943e-01, -5.0146e-01,  ..., -1.0162e-01,
           -1.2207e+00,  1.6577e-01],
          [ 2.7197e-01,  1.8633e+00,  1.0010e+00,  ..., -1.1836e+00,
           -1.9902e+00,  1.8662e+00],
          [-2.9517e-01,  8.5107e-01,  2.4390e-01,  ..., -4.3164e-01,
           -9.0576e-01,  1.5986e+00],
          ...,
          [ 2.4634e-01,  1.2070e+00,  5.0507e-02,  ...,  1.5049e+00,
           -2.2522e-01,  5.3955e-01],
          [ 7.6172e-01,  1.8369e+00,  4.9951e-01,  ..., -1.8701e-01,
           -1.6221e+00,  1.0020e+00],
          [ 6.7139e-01,  7.2070e-01,  6.2402e-01,  ...,  1.0479e+00,
           -9.4580e-01, -6.7969e-01]],

         ...,

         [[-2.6099e-01,  1.6357e-01,  2.6855e-01,  ..., -1.0248e-01,
           -6.0840e-01, -2.0068e-01],
          [-1.2201e-01,  1.8809e+00,  5.8899e-02,  ..., -4.0344e-02,
           -4.3872e-01, -1.0333e-01],
          [ 3.9453e-01,  4.5435e-01,  7.1350e-02,  ..., -6.0059e-01,
           -6.6113e-01, -1.0654e+00],
          ...,
          [-7.0605e-01,  4.7241e-01, -5.6982e-01,  ..., -1.4185e-01,
            6.8506e-01, -5.4395e-01],
          [-3.3569e-01,  1.4512e+00, -2.1997e-01,  ...,  8.0273e-01,
            2.3828e-01, -2.4060e-01],
          [-1.0925e-02,  1.0947e+00, -1.0820e+00,  ...,  1.3098e-01,
           -2.3975e-01, -3.7329e-01]],

         [[ 3.5205e-01,  2.7905e-01, -3.2012e+00,  ...,  4.5312e-01,
           -2.4829e-01,  6.0840e-01],
          [ 6.3232e-01, -3.7305e-01,  3.3750e+00,  ...,  1.7480e+00,
           -2.7295e-01,  1.5498e+00],
          [ 1.6973e+00, -1.1924e+00,  3.2559e+00,  ...,  1.8789e+00,
           -6.7200e-02, -1.6074e+00],
          ...,
          [-4.1122e-03, -7.9834e-02,  3.3457e+00,  ...,  1.3330e+00,
           -2.5513e-01, -1.0820e+00],
          [-9.9072e-01, -3.4546e-01,  4.2461e+00,  ...,  5.5786e-02,
           -6.3330e-01, -1.2482e-01],
          [ 3.6133e-01, -1.0342e+00,  3.3223e+00,  ...,  4.5044e-01,
            5.3857e-01, -1.3447e+00]],

         [[-6.0333e-02,  1.3855e-01,  2.5195e-01,  ...,  1.0278e-01,
           -3.6133e-01,  1.2024e-01],
          [-1.0498e+00,  6.4355e-01,  4.4495e-02,  ...,  2.3792e-01,
           -1.6152e+00,  8.6865e-01],
          [-3.6597e-01,  7.5879e-01,  2.4255e-01,  ...,  7.1191e-01,
           -1.2070e+00,  8.4277e-01],
          ...,
          [-8.6328e-01, -4.8242e-01,  3.6670e-01,  ...,  2.5610e-01,
           -1.9053e+00,  2.0117e-01],
          [-1.1113e+00,  4.4702e-01,  3.8379e-01,  ...,  7.6270e-01,
           -2.2559e+00,  6.9189e-01],
          [ 5.5420e-01,  3.0444e-01,  1.3318e-01,  ..., -5.1367e-01,
           -2.2083e-01,  3.3350e-01]]]], device='cuda:0', dtype=torch.float16,
       grad_fn=<TransposeBackward0>), tensor([[[[ 0.2817, -0.3193, -0.3984,  ..., -0.2469,  0.4316, -0.1116],
          [-3.5703, -0.1162,  1.7461,  ..., -0.3723,  1.2139,  2.2422],
          [-4.8125, -0.8364, -0.2649,  ...,  0.7393,  2.5898,  0.3484],
          ...,
          [-0.9272, -0.7666, -0.5986,  ...,  0.6494, -0.5479,  2.4922],
          [-2.3848, -0.1543, -0.1897,  ...,  0.9419,  0.8774,  3.1816],
          [ 0.0830, -1.7900, -1.1416,  ...,  1.0771, -0.7935, -0.1252]],

         [[-0.3438, -0.0462, -0.1394,  ..., -0.1763, -0.1191,  0.6436],
          [-3.8887, -0.5024, -1.2891,  ..., -1.5791, -2.3438,  1.0957],
          [-1.2715, -0.0699, -2.1543,  ...,  0.5464, -4.2344,  2.0078],
          ...,
          [-0.9878, -1.0342, -1.9229,  ..., -1.9434, -1.0635,  1.7637],
          [-0.6069, -0.6597, -2.3438,  ..., -2.4023, -2.0215,  1.6748],
          [ 0.2118, -1.3105, -0.1615,  ..., -2.2871,  0.0131,  3.6914]],

         [[ 0.3013, -0.0963, -0.0994,  ..., -0.3049,  0.0124, -0.3577],
          [-0.3037, -1.7168,  2.5332,  ..., -1.3584, -1.2588, -1.3330],
          [ 1.2334, -1.8848,  1.0635,  ...,  0.0122, -0.2930, -1.1338],
          ...,
          [ 0.2986, -0.7725,  1.9297,  ..., -2.3047,  0.6270,  0.4570],
          [-1.2100, -1.1201,  2.8164,  ..., -0.4768,  0.2656,  1.4463],
          [ 0.7124,  0.1157,  1.5059,  ..., -0.2651,  3.1699, -0.6475]],

         ...,

         [[-0.1440,  0.7202, -0.1798,  ..., -0.1998,  0.6045, -0.2360],
          [-1.1172,  1.2764,  1.0215,  ..., -0.9683, -0.4624, -0.1959],
          [-0.9746,  0.5166,  0.4465,  ...,  1.0400,  1.9375,  3.2793],
          ...,
          [-0.0619, -1.6992,  2.9277,  ...,  0.8203,  1.2783, -2.5645],
          [-0.3882, -0.8818,  1.7373,  ..., -0.0762,  0.5938, -1.4814],
          [-0.5908, -2.7754,  3.0059,  ..., -0.6113,  1.0020, -0.8521]],

         [[ 0.1703, -0.1033,  0.2852,  ..., -0.2671, -0.1200,  0.0051],
          [ 0.7578,  4.8711,  2.9590,  ...,  1.4277,  0.4333,  1.3350],
          [-0.4993,  1.8320,  1.7520,  ...,  0.1506, -1.1768, -1.1240],
          ...,
          [ 1.4053, -0.3030, -1.2314,  ..., -1.9746, -0.7632,  0.5884],
          [ 0.3816,  1.6416,  1.9277,  ..., -0.1864, -1.9668,  3.0098],
          [-0.9321,  1.2080, -0.7251,  ..., -1.2812, -0.1736, -0.7505]],

         [[ 0.0158, -0.2544, -0.9004,  ..., -0.5835, -0.8960,  0.2510],
          [ 0.3525,  1.6318, -2.9492,  ...,  1.0703,  0.7017,  1.3359],
          [-0.6929, -0.4409, -1.1758,  ..., -0.7764,  1.7939, -0.3489],
          ...,
          [ 0.4424, -0.9790, -0.7354,  ...,  2.3652,  4.2930,  0.8682],
          [ 0.3452,  0.8003, -2.8184,  ...,  1.0605,  1.9238,  0.1571],
          [-0.5767,  1.4238, -0.9648,  ...,  0.3159,  0.9141, -0.8506]]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<TransposeBackward0>), tensor([[[[ 1.0400,  3.3359, -0.2166,  ..., -0.2639,  0.7866,  3.4199],
          [ 1.3711,  2.6836, -0.2603,  ...,  0.5723, -1.9209,  2.1387],
          [ 1.4238,  0.3916, -1.0430,  ..., -0.7236,  0.0707,  0.9727],
          ...,
          [ 1.5527,  0.1443, -0.6938,  ...,  0.4385,  0.3315,  2.0195],
          [ 0.6523, -0.2166,  1.6768,  ..., -0.2025,  0.7065,  0.9780],
          [-1.3262, -0.1544,  0.4727,  ..., -1.2031,  1.1553, -6.2461]],

         [[ 1.9580,  1.0234,  0.1316,  ...,  1.8828, -0.2744, -0.0076],
          [ 1.9854,  0.6499,  1.8555,  ...,  1.7393,  2.0332, -0.9023],
          [ 1.6201,  1.9463, -0.9414,  ...,  1.4004,  1.1826, -0.6812],
          ...,
          [ 1.9355,  0.3467, -1.5908,  ...,  2.2754,  1.2734,  0.0717],
          [-0.0312,  2.1094, -3.3242,  ..., -1.1338,  0.2246,  1.1348],
          [-3.1152,  1.5430,  0.1290,  ..., -4.2227, -0.6064,  0.8389]],

         [[-0.9585, -2.3320,  1.9893,  ...,  1.5898,  1.5342,  1.8682],
          [ 0.8369, -1.4961,  0.4426,  ..., -3.2344,  1.6250,  1.0010],
          [ 1.2617,  0.7505,  0.9404,  ...,  0.1199,  0.8838, -0.3428],
          ...,
          [-0.7510, -0.8496, -0.5098,  ...,  3.5117,  1.5059,  0.2742],
          [-1.5547,  0.6606, -0.2720,  ...,  0.3105,  2.0039, -1.2969],
          [ 1.2949,  5.1680, -0.0150,  ..., -0.2839,  0.1150, -0.8516]],

         ...,

         [[ 1.1465,  0.8081, -0.2075,  ..., -0.7671, -1.6152,  0.2866],
          [ 1.6553,  0.3711, -1.0264,  ...,  2.3438, -0.7266, -1.9209],
          [ 0.9434,  0.1923, -1.0273,  ..., -0.0226,  0.8887, -1.0947],
          ...,
          [ 0.6094,  2.5996, -1.2490,  ..., -1.0811, -3.0215,  1.0605],
          [ 0.1768, -0.1145, -0.5664,  ..., -1.5977,  0.6279,  0.5576],
          [-3.5820, -1.1670, -0.0498,  ..., -0.0974,  6.2656, -0.5405]],

         [[ 0.5835,  0.5317,  2.3867,  ..., -0.0746,  2.0703,  1.6357],
          [-0.1578,  0.9946, -0.5942,  ..., -0.0463,  0.8481,  1.8457],
          [ 1.1338,  0.6055,  1.2949,  ..., -0.5366,  1.4326,  0.9790],
          ...,
          [ 0.4241,  0.6064,  1.0234,  ..., -1.1797,  2.0957,  1.0625],
          [ 1.1562, -0.9189,  0.3228,  ...,  0.9722, -1.5850,  0.2233],
          [ 0.7407,  1.2119,  1.5059,  ..., -0.0069, -1.0088,  0.0207]],

         [[ 0.7949,  0.4363, -0.6348,  ...,  1.2158, -0.6084, -0.5659],
          [ 0.1158, -0.4092, -2.3926,  ...,  1.0195, -1.6445, -2.0527],
          [ 1.8340,  0.0143, -0.7056,  ...,  0.6431, -0.8276,  1.3984],
          ...,
          [ 0.5669, -0.7432, -0.4238,  ..., -2.2109, -0.7568,  0.1208],
          [ 1.7891,  0.0417,  0.7065,  ..., -0.7959, -0.7129, -1.6670],
          [ 5.0664,  3.6758,  4.2773,  ..., -1.7207,  2.2617,  1.1543]]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<TransposeBackward0>), tensor([[[[ -0.6191,  -0.4194,  -1.0625,  ...,  -0.2134,   1.4893,  -1.5967],
          [ -8.3828,   0.8115,   2.3125,  ...,   2.2422,  -0.8442,  -0.7720],
          [ -1.3086,   0.1060,  -0.0717,  ...,   0.9097,  -0.7334,   1.7266],
          ...,
          [  3.0176,   1.1406,  -0.5903,  ...,  -0.2925,   1.5088,  -2.3379],
          [ -2.2344,  -0.5112,   5.4883,  ...,   7.5078,   0.6191,  -4.6016],
          [ -0.1120,  -0.4348,  -0.4263,  ...,  -0.4692,  -1.0664,   0.1718]],

         [[  0.6450,   2.9902,  -0.2639,  ...,  -1.2969,   0.2517,   2.5664],
          [ -0.5649,  -0.2939,  -3.2266,  ...,   1.3574,   3.8555,  -0.7163],
          [ -1.8105,  -1.2832,   0.4434,  ...,   2.4043,  -0.5991,  -1.4727],
          ...,
          [ -7.4375,  -2.2734,   2.6914,  ...,   1.0684,  -0.9541,   0.6680],
          [ -6.3359,  -4.0312,   0.0894,  ...,   5.6523,  -3.3535,   6.5859],
          [  1.2002,  -0.6992,  -0.2671,  ...,   0.7773,   0.9292,  -0.7178]],

         [[ -1.4688,   1.4756,   2.2793,  ...,  -3.0625,   0.0271,   0.5474],
          [  0.2585,   1.5537,  -3.3477,  ...,  -3.3516,  -4.0625,   0.8999],
          [ -2.9941,   1.0088,   3.4590,  ...,  -2.6641,   2.7539,   3.7578],
          ...,
          [  0.5308,   3.1348,  -3.0879,  ...,  -3.9082,  -4.2656,  -0.4182],
          [ -2.1680,  -2.7695,  -2.5488,  ...,  -0.2354,   6.7148,  -3.0195],
          [ -0.9780,  -0.4824,  -0.4658,  ...,   0.2849,   0.5112,  -0.2568]],

         ...,

         [[  1.4277,  -3.0273,   0.6426,  ...,   2.9668,   0.8213,  -7.2070],
          [  4.5664,  -1.5068,  -1.0381,  ...,  -0.3911,   2.7910,  -6.2617],
          [  0.5391,   1.2900,   2.5254,  ...,   1.0342,   1.2080,  -5.9180],
          ...,
          [ -0.6187,  -4.8477,  -1.0420,  ...,  -1.0225,   3.9102,  -6.2109],
          [ -1.4141,  -3.9434,  -5.3438,  ...,  -7.0586,  -4.1445,  -4.7266],
          [ -0.4192,   0.4409,  -0.8389,  ...,   0.6772,  -0.0625,   1.8643]],

         [[ -2.1113,  -5.8008,   3.2246,  ...,  -7.9922,  -6.5742,  -2.1055],
          [ -1.5615,  -1.1162,  -2.4062,  ...,  -8.1484,   3.0273,  -0.7227],
          [ -1.7988,  -5.0039,  -2.4902,  ...,  -4.9336,  -0.1837,  -1.2861],
          ...,
          [ -2.5156,  -5.2773,   0.7598,  ..., -12.0156,  -0.4070,  -0.2949],
          [  3.4453,  -7.0977,  -0.8730,  ...,  -0.1105,   8.4609,  -2.4707],
          [  0.3706,  -0.1637,  -0.4805,  ...,   1.3564,   0.4751,   0.2122]],

         [[  0.4043,   1.8232,  -1.1445,  ...,   1.7061,   2.9922,  -2.8652],
          [ -0.1475,   5.0781,   4.2227,  ...,   0.7642,   2.1719,  -2.8906],
          [ -1.0410,   0.4634,   1.3242,  ...,  -0.5229,   1.2900,  -1.5186],
          ...,
          [ -2.8613,   3.4531,  -0.8828,  ...,  -2.2207,   0.4666,  -1.0117],
          [  1.8301,  -7.5430,  -1.6738,  ...,   4.1094,  -2.6484,   4.5391],
          [ -0.2935,  -0.8452,  -0.8091,  ...,  -0.8276,   0.1887,  -0.4875]]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<TransposeBackward0>))), decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[ 0.0785,  0.1023, -0.0710,  ..., -0.0031, -0.1344,  0.1379],
         [ 0.1234,  0.1624,  0.0044,  ...,  0.1060, -0.0430,  0.0049],
         [-0.0797, -0.0555, -0.0983,  ...,  0.0984,  0.0357,  0.1094],
         ...,
         [-0.1034,  0.1049, -0.1344,  ..., -0.0906,  0.1537,  0.0607],
         [-0.1669, -0.0124, -0.1302,  ..., -0.1242,  0.2024, -0.1549],
         [ 0.0210,  0.0206,  0.0559,  ...,  0.0013, -0.0229,  0.0071]]],
       device='cuda:0', dtype=torch.float16, grad_fn=<MulBackward0>), encoder_hidden_states=None, encoder_attentions=None)
Warmup (for Torch) done!
Compilation (for Triton) done!
Triton time: 35.866287344999996ms
Torch time: 35.86606342ms
 Triton and Torch match!
a dog is a good pet
a dog is a good pet
